{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Video\n",
    "\n",
    "ML experimenet: The process of building an ML model\n",
    "Experiment run: Each trial in an ML experiment\n",
    "Run Artifact: Any file that is associated with an ML run\n",
    "Experiment metadata\n",
    "\n",
    "What is experiment tracking?\n",
    "Experiment tracking is the process of keeping track of all the relevant information from an ML experiment, which includes:\n",
    "- Source code\n",
    "- Environment\n",
    "- Data\n",
    "- Model\n",
    "- Hyperparameters\n",
    "- Metrics\n",
    "...\n",
    "\n",
    "Why is experiment tracking so important?\n",
    "In general, because of these 3 main reasons:\n",
    "- Reproducibility\n",
    "- Organization\n",
    "- Optimization\n",
    "\n",
    "Trakcing Experiments in spreedsheets:\n",
    "Why is not enough?\n",
    "- Error Prone\n",
    "- No standard format\n",
    "- Visibility & Collaboration\n",
    "\n",
    "MLflow\n",
    "Definition: \"An open source platform for the machine learning lifecyle\"\n",
    "\n",
    "In practice, it's just a Python package that can be installed with pip, and it contains four main modules:\n",
    "- Tracking\n",
    "- Models\n",
    "- Model Registry\n",
    "- Projects\n",
    "\n",
    "Tracking experiments with MLflow\n",
    "The Mlflow Tracking module allows you to organize your experiments into runs, and to keep track of:\n",
    "- Parameters: Hyperparameters / any other parameters that you think will have an effect on the metric of the model example: path to the training dataset, cause later you can change it. Hence it will be reflected into the run and can be tracked. Others, can be pre-processing techniques used.\n",
    "- Metrics: Any evaluation metric\n",
    "- Metadata: eg. tags\n",
    "- Artifacts: Any file, model trained, visualizations, datasets as well -> but does scale very well.\n",
    "- Models: Save the model\n",
    "\n",
    "Along with this information, MLflow automatically logs extra information about the run:\n",
    "- Source code\n",
    "- Version of the code (git commit)\n",
    "- Start and end time\n",
    "- Author\n",
    "\n",
    "mlflow demo\n",
    "To launch the mlflow ui: `mlflow ui`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Video\n",
    "\n",
    "Check out the requirements.txt\n",
    "\n",
    "mlflow ui --backend-store-uri sqlite:///mlflow.db\n",
    "\n",
    "Create environment: conda create -p venv python=3.9 -y \n",
    "Activate: conda activate venv/\n",
    "Install requirements.txt: pip install -r requirements.txt\n",
    "\n",
    "```\n",
    ".\n",
    "├── duration_prediction.ipynb\n",
    "├── models\n",
    "├── requirements.txt\n",
    "└── venv\n",
    "```\n",
    "\n",
    "Checkout the duration_predictions.ipybn file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Video\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Video\n",
    "\n",
    "Model Management:\n",
    "- Error prone\n",
    "- No versioning\n",
    "- No model lineage\n",
    "\n",
    "directory name as -> final_model, model_final_final, ...\n",
    "\n",
    "\n",
    "1. Log the model as artifact:\n",
    "```\n",
    "mlflow.log_artifact(local_path=\"models/lin_reg.bin\", artifact_path=\"models_pickle\")\n",
    "```\n",
    "\n",
    "2. Log model using the method `log_model`:\n",
    "```\n",
    "mlflow.xgboost.log_model(model_as_ip, artifact_path=\"models_mlflow\")\n",
    "mlflow.<framework>.log_model(model_as_ip, artifact_path=\"models_mlflow\")\n",
    "```\n",
    "\n",
    "Now let's save the pre-processing step as well\n",
    "```\n",
    "with open(\"models/preprocessor.b\", \"wb\") as handle:\n",
    "    pickle.dump(dv, handle) # dv = dictvectorizer\n",
    "\n",
    "mlfow.log_artifact(\"models/preprocessor.b\", artifact_path=\"preprocessor\")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Video\n",
    "\n",
    "Model Registry\n",
    "All the models that are ready for production should be stored in model registry. \n",
    "It helps in communication between the person building the model and the person that is in charge of deploying the model\n",
    "\n",
    "Model registry has multiple stages:\n",
    "- Staging\n",
    "- Production\n",
    "- Archive\n",
    "\n",
    "Data Scientist only decides what are the models that are ready for production, once the model is registered in the model registry the deployment engineer can take a look and check what are the parameters that were used, what is the size of the model, the performance, and based on that decide to move this model between the different stages.\n",
    "\n",
    "Model registry is not deploying any model, it is only a place to list what are the models that are production-ready and the stages are just labels. Complement model registry with some CI/CD pipeline for deployment.\n",
    "```\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "MLFLOW_TRACKING_URI = \"sqlite:///mlflow.db\"\n",
    "clinet = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)\n",
    "\n",
    "client.list_experiments()\n",
    "\n",
    "client.create_experiment(name = \"my-cool-experiment\")\n",
    "\n",
    "from mlflow.entities import ViewType\n",
    "\n",
    "runs = client.search_runs (\n",
    "    experiment_ids='1',\n",
    "    filter_string='',\n",
    "    run_view_type=ViewType.ACTIVE_ONLY,\n",
    "    max_results=5,\n",
    "    order_by=[\"metrics.rmse ASC\"]\n",
    ")\n",
    "```\n",
    "**Promote some of the models to model registry**\n",
    "\n",
    "```\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "mlflow.register_model(model_uri=model_uri, name=\"nyc-taxi-regressor\")\n",
    "\n",
    "client.list_registered_models()\n",
    "\n",
    "model_name = \"nyc-taxi-regressor\"\n",
    "latest_versions = client.get_latest_versions(name=model_name)\n",
    "\n",
    "model_version = 4\n",
    "new_stage = \"Staging\"\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=model_version,\n",
    "    stage=new_stage,\n",
    "    archive_existing_versions=False\n",
    ")\n",
    "\n",
    "from datetime import datetime\n",
    "date = datetime.today().date()\n",
    "client.update_model_version(\n",
    "    name=model_name,\n",
    "    version=model_version,\n",
    "    description = f\"The model version {model_version} was transitioned to {new_stage} on {date}\" \n",
    ")\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_dataframe(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n",
    "    df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "\n",
    "    df['duration'] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n",
    "    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess(df, dv):\n",
    "    df['PU_DO'] = df['PULocationID'] + '_' + df['DOLocationID']\n",
    "    categorical = ['PU_DO']\n",
    "    numerical = ['trip_distance']\n",
    "    train_dicts = df[categorical + numerical].to_dict(orient='records')\n",
    "    return dv.transform(train_dicts)\n",
    "\n",
    "\n",
    "def test_model(name, stage, X_test, y_test):\n",
    "    model = mlflow.pyfunc.load_model(f\"models:/{name}/{stage}\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    return {\"rmse\": mean_squared_error(y_test, y_pred, squared=False)}\n",
    "\n",
    "df = read_dataframe(\"data/green_tripdata_2021-03.csv\")\n",
    "\n",
    "client.download_artifacts(run_id=run_id, path='preprocessor', dst_path='.')\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"preprocessor/preprocessor.b\", \"rb\") as f_in:\n",
    "    dv = pickle.load(f_in)\n",
    "\n",
    "X_test = preprocess(df, dv)\n",
    "\n",
    "target = \"duration\"\n",
    "y_test = df[target].values\n",
    "\n",
    "%time test_model(name=model_name, stage=\"Production\", X_test=X_test, y_test=y_test)\n",
    "\n",
    "%time test_model(name=model_name, stage=\"Staging\", X_test=X_test, y_test=y_test)\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=4,\n",
    "    stage=\"Production\",\n",
    "    archive_existing_versions=True\n",
    ")\n",
    "```\n",
    "Model management in MLflow\n",
    "The model registry component is a centralized model store, set of APIs, and a UI, to collaboratively manage the full lifecycle of an MLflow Model.\n",
    "\n",
    "It provides:\n",
    "- Model lineage\n",
    "- Model versioning\n",
    "- Stage transitions, and \n",
    "- Annotations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Video\n",
    "\n",
    "MLflow in Practice\n",
    "Let's consider these three scenarios:\n",
    "\n",
    "- A single data scientist participating in an ML competition\n",
    "    - A remote tracking server will be a over-kill\n",
    "- A cross-functional team with one data scientist working on an ML model\n",
    "    - Sharing information is required but running a tracking server remotely is also not required. Locally is enough.\n",
    "    - Model registry could be a good idea to manage the life cycle of the models but not clear whether to run it locally or remotely.\n",
    "- Multiple data scientists working on multiple ML models\n",
    "    - Sharing the information is very important.\n",
    "    - Remote tracking is important.\n",
    "    - Model registry is also important.\n",
    "\n",
    "Configuring MLflow\n",
    "- Backend Store\n",
    "    - local filesystem\n",
    "    - SQLAlchemy compatible DB (eg. SQLite)\n",
    "- Artifacts Store\n",
    "    - local filesystem\n",
    "    - remote (eg. S3 bucket)\n",
    "- Tracking Server\n",
    "    - No tracking server\n",
    "    - local host\n",
    "    - remote\n",
    "\n",
    "Checkout the dirctory at: https://github.com/DataTalksClub/mlops-zoomcamp/tree/main/02-experiment-tracking/running-mlflow-examples\n",
    "\n",
    "Remote tracking server:\n",
    "The tracking server can be easily deployed to the cloud.\n",
    "Some benefits:\n",
    "- Share experiments with other data scientists.\n",
    "- Collaborate with others to build and deploy models\n",
    "- Give more visibility of the data science efforts.\n",
    "\n",
    "Issues with running a remote (shared) MLflow server\n",
    "\n",
    "- Security\n",
    "    - Restrict access to the server (eg. access through VPN)\n",
    "- Scalability\n",
    "    - Check Deploy MLflow on AWS Fargate\n",
    "    - Check MLflow at Company Scale by Jean-Denis Lesage\n",
    "- Isolation\n",
    "    - Deine standard for naming experiments, models, and a set of default tags\n",
    "    - Restrict access to artifact (eg. use S3 buckets living in different AWS accounts)\n",
    "\n",
    "MLflow limitations (and when not to use it)\n",
    "- Authentication & Users: The open source version of MLflow doesn't provide any sort of authentication\n",
    "- Data versioning: to ensure full reproducibility we need to version the data used to train the model. MLflow doesn't provide a built-in solution for that but there are a few ways to deal with this limitation. (log Params for data path)\n",
    "- Model/Data Monitoring & Alerting: This is outside of the scope of Mlflow and currently there are more suitable tools for doing this.\n",
    "\n",
    "Alternatives:\n",
    "- Neptune\n",
    "- Comet\n",
    "- Weights & Biases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 2\n",
    "\n",
    "1. 2.3.2\n",
    "2. 154 kb\n",
    "3. max_depth = 10\n",
    "4. mlflow server --backend-store-uri sqlite:///backend.db --default-artifact-root ./artifacts\n",
    "    RMSE: 2.45\n",
    "5. 2.185 (I got 2.285)\n",
    "6. Model version, Source Experiment, Model Signature -> All of the above"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
