{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Prefect: An Workflow Orchestration Tool\"\n",
    "description: \"Prefect is an open-source workflow orchestration tool that helps you automate and manage the flow of work across your data stack.\"\n",
    "author: \"Sagar Thacker\"\n",
    "date: \"2023-06-05\"\n",
    "categories: [MLOps, Prefect]\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prefect is an open-source workflow orchestration tool that helps you automate and manage the flow of work across your data stack. \n",
    "\n",
    "Prefect is built on Python and uses a modular architecture that makes it easy to build and deploy complex workflows.  Prefect also includes a rich set of features for monitoring, debugging, and managing your workflows.\n",
    "\n",
    "Before we dive into Prefect, let's first understand what is workflow orchestration, why do we need it, and where does Prefect come into play.\n",
    "\n",
    "## What is Workflow Orchestration?\n",
    "\n",
    "Building a ML system has a lot of moving parts. We have to deal with data collection, data preprocessing, model training, model serving, etc. Each of these steps can be further broken down into sub-steps. For example, data preprocessing can be broken down into feature engineering, feature selection, etc.\n",
    "\n",
    "Workflow Orchestration is the process of automating and managing the flow of work across these steps. It helps us build complex workflows by combining multiple steps together. It also helps us manage the dependencies between these steps.\n",
    "\n",
    "![](../images/introduction-workflow-orchestration/workflow.png)\n",
    "\n",
    "## Negative Engineering\n",
    "\n",
    "Inspite of building a robust system, there are chances that something might go wrong. For example, the data might not be available, the model might fail to train, etc. \n",
    "\n",
    "![](../images/introduction-workflow-orchestration/workflow_failed.png)\n",
    "\n",
    "Often time while developing any application, we find ourselves spending a lot of time fixing bugs and making sure that everything works as expected. We spend time writing code to handle edge cases and make sure that our application is robust enough to handle any unexpected situation. Exmaples of such situations are:\n",
    "\n",
    "- Writing Retries logic when APIs go down\n",
    "- Build Notifications for when a job fails\n",
    "- Record logs for observability and debugging\n",
    "- Write conditional logic to handle edge cases\n",
    "- Handle cases when requests timeouts or fails\n",
    "\n",
    "Negative Engineering refers to ideology that we spend most of our time writing code to handle edge cases rather than writing code that actually solves the problem. This is where Workflow Orchestration comes into play.\n",
    "\n",
    "Worflow Orchestration tools provide set of features off the shelf that aim to eliminate the need for negative engineering. These tools provide features like retries, notifications, logging, lineage tracking, etc. out of the box.\n",
    "\n",
    "## Prefect\n",
    "\n",
    "Prefect aims to eliminate the need for negative engineering by providing a rich set of features out of the box.\n",
    "\n",
    "### Setup\n",
    "\n",
    "It is a good practice to create a virtual environment for each project. This helps us keep our dependencies separate and avoid any version conflicts.\n",
    "\n",
    "```{.bash filename=\"requirements.txt\"}\n",
    "fastparquet==2023.4.0\n",
    "mlflow==2.3.1\n",
    "pandas==2.0.1\n",
    "prefect==2.10.8\n",
    "scikit_learn==1.2.2\n",
    "xgboost==1.7.5\n",
    "psycopg2-binary==2.9.5\n",
    "```\n",
    "Let's create a virtual environment named `venv` and install all the dependencies.\n",
    "\n",
    "```{.bash}\n",
    "# Create a virtual environment\n",
    "conda create -p venv python=3.10 -y\n",
    "\n",
    "# Activate the virtual environment\n",
    "conda activate venv/\n",
    "\n",
    "# Install all the dependencies\n",
    "pip install -r requirements.txt --no-cache-dir\n",
    "```\n",
    "\n",
    "### Prefect Flow\n",
    "\n",
    "We will be using the [NYC Taxi Dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) for this tutorial. The dataset contains information about green taxi trips in New York City. The dataset is available in parquet format.\n",
    "\n",
    "Steps that we'll cover:\n",
    "\n",
    "1. We'll download the dataset from the NY Taxi website and load it into a pandas dataframe.\n",
    "2. Preprocess the data.\n",
    "3. Train a model on the preprocessed data.\n",
    "4. Log the model and the metrics to MLflow.\n",
    "\n",
    "You'll wonder where do we use Prefect in this? Answer is in each step.\n",
    "\n",
    "We'll go through each step in detail. At the end you'll find the complete code for the workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Import Libraries\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pathlib\n",
    "import scipy\n",
    "import mlflow\n",
    "import sklearn\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from prefect import flow, task\n",
    "from prefect.tasks import task_input_hash\n",
    "from datetime import timedelta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two most important building blocks in Prefect are `Task` and `Flow`. We'll start by importing these two. More on these later.\n",
    "\n",
    "```{.python}\n",
    "from prefect import task, flow\n",
    "```\n",
    "\n",
    "### Download the dataset\n",
    "\n",
    "We have created a python function named `fetch` what will download the data from the NY Taxi website and save it to the `data` directory.\n",
    "\n",
    "```{.python}\n",
    "@task(name=\"Fetch Data\", log_prints=True, retries=3, cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1))\n",
    "def fetch(year: int, month: int, color: str) -> None:\n",
    "    \"\"\"Fetches data from the NYC Taxi dataset and saves it locally\"\"\"\n",
    "\n",
    "    # Download the data from the NYC Taxi dataset\n",
    "\n",
    "    url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{color}_tripdata_{year}-{month:0>2}.parquet\"\n",
    "    file_name = f\"{color}_tripdata_{year}-{month:0>2}.parquet\"\n",
    "\n",
    "    pathlib.Path(\"data\").mkdir(exist_ok=True)\n",
    "\n",
    "    os.system(f\"wget {url} -O ./data/{file_name}\")\n",
    "```\n",
    "\n",
    "Let's understand what is happening here.\n",
    "\n",
    "The function is defined in the same way as any other python function. The only difference is that we have added a decorator `@task` to the function. This decorator converts the python function into a Prefect Task. The decorator takes a few arguments:\n",
    "\n",
    "- `@task` is a decorator that converts a python function into a Prefect Task.\n",
    "- `name` is the name of the task. This is used to identify the task in the Prefect UI.\n",
    "- `log_prints` is a boolean flag that tells Prefect to log the output of the task.\n",
    "- `retries` is the number of times the task should be retried in case of failure.\n",
    "- `cache_key_fn` is a function that returns a unique key for the task. This is used to cache the output of the task.\n",
    "- `cache_expiration` is the time after which the cache should expire.\n",
    "\n",
    "::: {.callout-note}\n",
    "## Terminology Alert: Task\n",
    "\n",
    "A task is a unit of work that needs to be done. It can be anything from downloading a file to training a model. Prefect provides a decorator `@task` that converts a python function into a Prefect Task.\n",
    "\n",
    "Imagine you are preparing a sandwich. To prepare a delicious sandwich you need to perform a few tasks like: get the bread, apply butter, add cheese, cut down the veggies, etc. Each of these tasks is a unit of work that needs to be done to prepare the sandwich. In Prefect, each of these tasks can be represented as a Prefect Task.\n",
    ":::\n",
    "\n",
    "::: {.callout-note}\n",
    "To enable caching we specified a `cache_key_fn` which is a function that returns a cache key on our task. We cachec our task based on its input by using `task_input_hash` which is a function that returns a unique hash for the input of the task.\n",
    "\n",
    "It hashes all inputs to the task and returns a unique hash. If the task inputs do not change, the hash will remain the same and the cachec results are used instead of running the task again until cache expires.\n",
    "\n",
    "We also specified a `cache_expiration` of 1 day. This means that the cache will expire after 1 day.\n",
    ":::\n",
    "\n",
    "This is a good practice while working with large datasets. It helps us avoid downloading the same dataset again and again.\n",
    "\n",
    "Next up, we have function named `read_data` that takes a filename and load the data into a pandas dataframe.\n",
    "\n",
    "```{.python}\n",
    "@task(name=\"Read a Parquet file\")\n",
    "def read_data(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"Read data into DataFrame\"\"\"\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n",
    "    df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "\n",
    "    df[\"duration\"] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n",
    "    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical = [\"PULocationID\", \"DOLocationID\"]\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "\n",
    "    return df\n",
    "```\n",
    "\n",
    "This function is also decorated with `@task` decorator. Nothing new here.\n",
    "\n",
    "### Preprocess the data\n",
    "\n",
    "Now that we have the data, we need to preprocess it before we can train a model on it. We'll create a function named `add_features` that combines `PULocationID` and `DOLocationID`. Also, we'll represent our features using the DictVectorizer. We'll perform these steps for both training and validation data.\n",
    "\n",
    "Again, we'll decorate the function with `@task` decorator.\n",
    "\n",
    "```{.python}\n",
    "@task(name=\"Add Features\")\n",
    "def add_features(df_train: pd.DataFrame, df_val: pd.DataFrame) -> tuple([\n",
    "        scipy.sparse._csr.csr_matrix,\n",
    "        scipy.sparse._csr.csr_matrix,\n",
    "        np.ndarray,\n",
    "        np.ndarray,\n",
    "        sklearn.feature_extraction.DictVectorizer,\n",
    "    ]):\n",
    "    \"\"\"Add features to the model\"\"\"\n",
    "    df_train[\"PU_DO\"] = df_train[\"PULocationID\"] + \"_\" + df_train[\"DOLocationID\"]\n",
    "    df_val[\"PU_DO\"] = df_val[\"PULocationID\"] + \"_\" + df_val[\"DOLocationID\"]\n",
    "\n",
    "    categorical = [\"PU_DO\"]  #['PULocationID', 'DOLocationID']\n",
    "    numerical = [\"trip_distance\"]\n",
    "\n",
    "    dv = DictVectorizer()\n",
    "\n",
    "    train_dicts = df_train[categorical + numerical].to_dict(orient=\"records\")\n",
    "    X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "    val_dicts = df_val[categorical + numerical].to_dict(orient=\"records\")\n",
    "    X_val = dv.transform(val_dicts)\n",
    "\n",
    "    y_train = df_train[\"duration\"].values\n",
    "    y_val = df_val[\"duration\"].values\n",
    "    return X_train, X_val, y_train, y_val, dv\n",
    "```\n",
    "\n",
    "### Train the model\n",
    "\n",
    "We have an XGBoost model that we want to train. We'll create a function named `train_best_model` that takes the training and validation data along with the DictVectorizer and trains the model. We'll use the best hyperparameters that we found seperately.\n",
    "\n",
    "One awesome thing you'll observe is that we are also using mlflow to log the model and the hyperparameters. This is a great way to track the model performance and the hyperparameters that were used to train the model.\n",
    "\n",
    "Due to the `log_prints` flag, Prefect will log the output of the task including the output logs from mlflow. This is a great way to visualize the logs in the Prefect UI. \n",
    "\n",
    "```{.python}\n",
    "@task(name=\"Train Model\", log_prints=True)\n",
    "def train_best_model(\n",
    "    X_train: scipy.sparse._csr.csr_matrix,\n",
    "    X_val: scipy.sparse._csr.csr_matrix,\n",
    "    y_train: np.ndarray,\n",
    "    y_val: np.ndarray,\n",
    "    dv: sklearn.feature_extraction.DictVectorizer,\n",
    ") -> None:\n",
    "    \"\"\"train a model with best hyperparams and write everything out\"\"\"\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        train = xgb.DMatrix(X_train, label=y_train)\n",
    "        valid = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "        best_params = {\n",
    "            \"learning_rate\": 0.09585355369315604,\n",
    "            \"max_depth\": 30,\n",
    "            \"min_child_weight\": 1.060597050922164,\n",
    "            \"objective\": \"reg:linear\",\n",
    "            \"reg_alpha\": 0.018060244040060163,\n",
    "            \"reg_lambda\": 0.011658731377413597,\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "\n",
    "        mlflow.log_params(best_params)\n",
    "\n",
    "        booster = xgb.train(\n",
    "            params=best_params,\n",
    "            dtrain=train,\n",
    "            num_boost_round=100,\n",
    "            evals=[(valid, \"validation\")],\n",
    "            early_stopping_rounds=20,\n",
    "        )\n",
    "\n",
    "        y_pred = booster.predict(valid)\n",
    "        rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "        pathlib.Path(\"models\").mkdir(exist_ok=True)\n",
    "        with open(\"models/preprocessor.b\", \"wb\") as f_out:\n",
    "            pickle.dump(dv, f_out)\n",
    "        mlflow.log_artifact(\"models/preprocessor.b\", artifact_path=\"preprocessor\")\n",
    "\n",
    "        mlflow.xgboost.log_model(booster, artifact_path=\"models_mlflow\")\n",
    "    return None\n",
    "```\n",
    "\n",
    "Now, we have our indenpendent tasks ready. Let's create a flow that will run these tasks in the order we want.\n",
    "\n",
    "### Create a flow\n",
    "\n",
    "We'll create a flow named `main_flow` that will run the tasks in the order we want. We'll also set the `name` of the flow to \"Main Flow\". This is a great way to identify the flow in the Prefect UI.\n",
    "\n",
    "Observe that yet again the flow is simply a function decorated with `@flow` decorator. We'll also set the `params` argument to the flow. These paramarater also get recorded in the Prefect UI which can be very useful to track the flow runs.\n",
    "\n",
    "::: {.callout-note}\n",
    "## Terminology Alert: Flow\n",
    "\n",
    "A flow is a collection of tasks that are executed in a particular order. A flow is a function decorated with `@flow` decorator. The flow function can take arguments and return values. \n",
    "\n",
    "Imagine flow is a container that holds all the tasks and the order in which they are executed. Making a delicious sandwich is a flow and the actions you take to make the sandwich are the tasks.\n",
    ":::\n",
    "\n",
    "```{.python}\n",
    "@flow(name=\"Main Flow\")\n",
    "def main_flow(params):\n",
    "    \"\"\"Main flow of the program\"\"\"\n",
    "    \n",
    "    # MLflow settings\n",
    "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "    mlflow.set_experiment(\"nyc-taxi-experiment\")\n",
    "\n",
    "    # Download and read data\n",
    "    df_train, df_val = download_and_read(params.years, params.months, params.color)\n",
    "\n",
    "    # Transform\n",
    "    X_train, X_val, y_train, y_val, dv = add_features(df_train, df_val)\n",
    "\n",
    "    # Train\n",
    "    train_best_model(X_train, X_val, y_train, y_val, dv)\n",
    "```\n",
    "\n",
    "If you observe closely, you'll notice that we have a `download_and_read` function but we didn't define that anywhere to avoid confusion until now. We'll define that function next but that function will be a `Subflow` in our main flow.\n",
    "\n",
    "### Create a subflow\n",
    "\n",
    "Image a subflow as a flow within a flow. A subflow is a function decorated with the same `@flow` decorator but it is called from within another flow. A subflow can also take arguments and return values.\n",
    "\n",
    "We'll create a subflow named `download_and_read` that will call the previously defined functions (tasks) to download the data and read it into a pandas dataframe. We'll also set the `name` of the subflow to \"Download and Read\". This is a great way to identify the subflow in the Prefect UI.\n",
    "\n",
    "```{.python}\n",
    "@flow(name=\"Subflow - Download and Read Data\", log_prints=True)\n",
    "def download_and_read(years: list, months: list, color: str):\n",
    "    # Download the data from the NYC Taxi dataset\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            print(f\"Download: Year-Month: {year}-{month} ({color})\")\n",
    "            fetch(year, month, color)\n",
    "\n",
    "    # Read the data into a DataFrame\n",
    "    track_data =[[], []]\n",
    "    df_train = pd.DataFrame()\n",
    "    for month in months[:-1]:\n",
    "        print(f\"Read: Year-Month: {year}-{month:0>2} ({color})\")\n",
    "        df = read_data(f\"./data/{color}_tripdata_{year}-{month:0>2}.parquet\")\n",
    "        df_train = pd.concat([df_train, df], ignore_index=True)\n",
    "        track_data[0].append(month)\n",
    "\n",
    "    print(f\"Read: Year-Month: {year}-{months[-1]:0>2} ({color})\")\n",
    "    df_val = read_data(f\"./data/{color}_tripdata_{year}-{months[-1]:0>2}.parquet\")\n",
    "    track_data[1].append(months[-1])\n",
    "\n",
    "    print(f\"Training data consists of months: {track_data[0]}\")\n",
    "    print(f\"Validation data consists of months: {track_data[1]}\")\n",
    "\n",
    "    return df_train, df_val\n",
    "```\n",
    "\n",
    "::: {.callout-note}\n",
    "## Terminology Alert: Subflow\n",
    "\n",
    "A subflow is a flow within a flow. A subflow is a function decorated with `@flow` decorator. The subflow function can take arguments and return values.\n",
    "\n",
    "Imagine you want to make a spicy sauce to have with your sandwich. To make a sauce you need to follow a recipe with multiple steps. Consider this as a subflow within the flow of making a sandwich.\n",
    ":::\n",
    "\n",
    "### Run the flow\n",
    "\n",
    "We'll run the script by calling the `main_flow` with the necessary arguments.\n",
    "\n",
    "```{.python}\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Ingest CSV data to Postgres')\n",
    "\n",
    "    parser.add_argument(\"--years\", nargs=\"+\", required=True, help=\"Data from year\")\n",
    "    parser.add_argument(\"--months\", nargs=\"+\", required=True, help=\"Data from months\")\n",
    "    parser.add_argument(\"--color\", required=True, help=\"Taxi color\", default=\"green\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main_flow(args)\n",
    "```\n",
    "\n",
    "Example, to run the flow for the months of January, February and March of 2021 for green taxis, we'll run the following command:\n",
    "\n",
    "```bash\n",
    "python orchestration.py --years 2021 --months 1 2 3 --color green\n",
    "```\n",
    "\n",
    "Illustration of the flow of execution of the tasks in the flow.\n",
    "\n",
    "```\n",
    "main_flow                       (Flow)\n",
    "    |\n",
    "    |---- download_and_read     (Subflow)\n",
    "    |           |\n",
    "    |           |---- fetch     (Task)\n",
    "    |           |---- read_data (Task)\n",
    "    |---- add_features          (Task)\n",
    "    |---- train_best_model      (Task)\n",
    "```\n",
    "\n",
    "![](../images/introduction-workflow-orchestration/pipeline.png)\n",
    "\n",
    "This will run the flow and log all the tasks in the Prefect UI."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you'll find some screenshots of the Prefect UI, sample output on terminal, and MLflow UI.\n",
    "\n",
    "Sample output on terminal:\n",
    "\n",
    "![](../images/introduction-workflow-orchestration/sample_output.png)\n",
    "\n",
    "Prefect UI:\n",
    "\n",
    "You can start the Prefect server using the command `prefect server start` and then navigate to `http://127.0.0.1:4200` to view the Prefect UI.\n",
    "\n",
    "![](../images/introduction-workflow-orchestration/prefect_ui.png)\n",
    "\n",
    "You'll find both the main flow and the subflow on the page as it records all the flows that are run. You can click on the flow to view the details of the flow. Main flow:\n",
    "\n",
    "![](../images/introduction-workflow-orchestration/main_flow.png)\n",
    "\n",
    "Subflow:\n",
    "\n",
    "![](../images/introduction-workflow-orchestration/subflow.png)\n",
    "\n",
    "Lastly you'll find your experiment in the MLflow UI:\n",
    "\n",
    "Run the mlflow UI using the command `mlflow server --backend-store-uri sqlite:///mlflow.db --host 0.0.0.0 --port 8080` and then navigate to `http://127.0.0.1:8080` to view the MLflow UI. Note: MLflow runs on port 5000 by default but we are running it on port 8080.\n",
    "\n",
    "![](../images/introduction-workflow-orchestration/mlflow_ui.png)\n",
    "\n",
    "Congratulations on running your first Prefect flow! ðŸŽ‰"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the whole code below:\n",
    "\n",
    "```{.python filename=\"orchestration.py\"}\n",
    "import os\n",
    "import pickle\n",
    "import pathlib\n",
    "import scipy\n",
    "import mlflow\n",
    "import sklearn\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from prefect import flow, task\n",
    "from prefect.tasks import task_input_hash\n",
    "from datetime import timedelta\n",
    "\n",
    "@task(name=\"Fetch Data\", log_prints=True, retries=3, cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1))\n",
    "def fetch(year: int, month: int, color: str) -> None:\n",
    "    \"\"\"Fetches data from the NYC Taxi dataset and saves it locally\"\"\"\n",
    "\n",
    "    # Download the data from the NYC Taxi dataset\n",
    "\n",
    "    url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{color}_tripdata_{year}-{month:0>2}.parquet\"\n",
    "    file_name = f\"{color}_tripdata_{year}-{month:0>2}.parquet\"\n",
    "\n",
    "    pathlib.Path(\"data\").mkdir(exist_ok=True)\n",
    "\n",
    "    os.system(f\"wget {url} -O ./data/{file_name}\")\n",
    "\n",
    "@task(name=\"Read a Parquet file\")\n",
    "def read_data(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"Read data into DataFrame\"\"\"\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n",
    "    df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "\n",
    "    df[\"duration\"] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n",
    "    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical = [\"PULocationID\", \"DOLocationID\"]\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "@task(name=\"Add Features\")\n",
    "def add_features(df_train: pd.DataFrame, df_val: pd.DataFrame) -> tuple([\n",
    "        scipy.sparse._csr.csr_matrix,\n",
    "        scipy.sparse._csr.csr_matrix,\n",
    "        np.ndarray,\n",
    "        np.ndarray,\n",
    "        sklearn.feature_extraction.DictVectorizer,\n",
    "    ]):\n",
    "    \"\"\"Add features to the model\"\"\"\n",
    "    df_train[\"PU_DO\"] = df_train[\"PULocationID\"] + \"_\" + df_train[\"DOLocationID\"]\n",
    "    df_val[\"PU_DO\"] = df_val[\"PULocationID\"] + \"_\" + df_val[\"DOLocationID\"]\n",
    "\n",
    "    categorical = [\"PU_DO\"]  #'PULocationID', 'DOLocationID']\n",
    "    numerical = [\"trip_distance\"]\n",
    "\n",
    "    dv = DictVectorizer()\n",
    "\n",
    "    train_dicts = df_train[categorical + numerical].to_dict(orient=\"records\")\n",
    "    X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "    val_dicts = df_val[categorical + numerical].to_dict(orient=\"records\")\n",
    "    X_val = dv.transform(val_dicts)\n",
    "\n",
    "    y_train = df_train[\"duration\"].values\n",
    "    y_val = df_val[\"duration\"].values\n",
    "    return X_train, X_val, y_train, y_val, dv\n",
    "\n",
    "\n",
    "@flow(name=\"Subflow - Download and Read Data\", log_prints=True)\n",
    "def download_and_read(years: list, months: list, color: str):\n",
    "    # Download the data from the NYC Taxi dataset\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            print(f\"Download: Year-Month: {year}-{month} ({color})\")\n",
    "            fetch(year, month, color)\n",
    "\n",
    "    # Read the data into a DataFrame\n",
    "    track_data =[[], []]\n",
    "    df_train = pd.DataFrame()\n",
    "    for month in months[:-1]:\n",
    "        print(f\"Read: Year-Month: {year}-{month:0>2} ({color})\")\n",
    "        df = read_data(f\"./data/{color}_tripdata_{year}-{month:0>2}.parquet\")\n",
    "        df_train = pd.concat([df_train, df], ignore_index=True)\n",
    "        track_data[0].append(month)\n",
    "\n",
    "    print(f\"Read: Year-Month: {year}-{months[-1]:0>2} ({color})\")\n",
    "    df_val = read_data(f\"./data/{color}_tripdata_{year}-{months[-1]:0>2}.parquet\")\n",
    "    track_data[1].append(months[-1])\n",
    "\n",
    "    print(f\"Training data consists of months: {track_data[0]}\")\n",
    "    print(f\"Validation data consists of months: {track_data[1]}\")\n",
    "\n",
    "    return df_train, df_val\n",
    "\n",
    "@task(name=\"Train Model\", log_prints=True)\n",
    "def train_best_model(\n",
    "    X_train: scipy.sparse._csr.csr_matrix,\n",
    "    X_val: scipy.sparse._csr.csr_matrix,\n",
    "    y_train: np.ndarray,\n",
    "    y_val: np.ndarray,\n",
    "    dv: sklearn.feature_extraction.DictVectorizer,\n",
    ") -> None:\n",
    "    \"\"\"train a model with best hyperparams and write everything out\"\"\"\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        train = xgb.DMatrix(X_train, label=y_train)\n",
    "        valid = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "        best_params = {\n",
    "            \"learning_rate\": 0.09585355369315604,\n",
    "            \"max_depth\": 30,\n",
    "            \"min_child_weight\": 1.060597050922164,\n",
    "            \"objective\": \"reg:linear\",\n",
    "            \"reg_alpha\": 0.018060244040060163,\n",
    "            \"reg_lambda\": 0.011658731377413597,\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "\n",
    "        mlflow.log_params(best_params)\n",
    "\n",
    "        booster = xgb.train(\n",
    "            params=best_params,\n",
    "            dtrain=train,\n",
    "            num_boost_round=100,\n",
    "            evals=[(valid, \"validation\")],\n",
    "            early_stopping_rounds=20,\n",
    "        )\n",
    "\n",
    "        y_pred = booster.predict(valid)\n",
    "        rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "        pathlib.Path(\"models\").mkdir(exist_ok=True)\n",
    "        with open(\"models/preprocessor.b\", \"wb\") as f_out:\n",
    "            pickle.dump(dv, f_out)\n",
    "        mlflow.log_artifact(\"models/preprocessor.b\", artifact_path=\"preprocessor\")\n",
    "\n",
    "        mlflow.xgboost.log_model(booster, artifact_path=\"models_mlflow\")\n",
    "    return None\n",
    "\n",
    "@flow(name=\"Main Flow\")\n",
    "def main_flow(params):\n",
    "    \"\"\"Main flow of the program\"\"\"\n",
    "    \n",
    "    # MLflow settings\n",
    "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "    mlflow.set_experiment(\"nyc-taxi-experiment\")\n",
    "\n",
    "    # Download and read data\n",
    "    df_train, df_val = download_and_read(params.years, params.months, params.color)\n",
    "\n",
    "    # Transform\n",
    "    X_train, X_val, y_train, y_val, dv = add_features(df_train, df_val)\n",
    "\n",
    "    # Train\n",
    "    train_best_model(X_train, X_val, y_train, y_val, dv)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Ingest CSV data to Postgres')\n",
    "\n",
    "    parser.add_argument(\"--years\", nargs=\"+\", required=True, help=\"Data from year\")\n",
    "    parser.add_argument(\"--months\", nargs=\"+\", required=True, help=\"Data from months\")\n",
    "    parser.add_argument(\"--color\", required=True, help=\"Taxi color\", default=\"green\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main_flow(args)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "1. [Prefect Flows](https://docs.prefect.io/2.10.12/concepts/flows/)\n",
    "2. [Prefect Tasks](https://docs.prefect.io/2.10.12/concepts/tasks/)\n",
    "3. [Prefect Tutorials](https://docs.prefect.io/2.10.12/tutorial/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next post, we will see what are prefect blocks and how to use them to build a more complex flow. Also, how to deploy our flows to Prefect Cloud and run them on a schedule.\n",
    "\n",
    "Thank you for reading and I hope you found this notebook helpful. Upvote if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
