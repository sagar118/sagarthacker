{
  "hash": "1a624157f111f292e114d7d51f8b5140",
  "result": {
    "markdown": "---\ntitle: Deployment of Dockerized ML Models with AWS Kinesis and Lambda\ndescription: 'A comprehensive guide to deploying ml model with AWS Lambda, Kenisis and ECR'\nauthor: Sagar Thacker\ndate: '2023-06-21'\nimage: ./images/lambda-kinesis/aws-deployment.png\ncategories:\n  - MLOps\n  - AWS Lambda\n  - AWS Kinesis\n  - AWS ECR\n  - Docker\n  - Deployment\nexecute:\n  echo: false\nformat:\n  html:\n    toc: true\n    code-summary: Show the code\n    code-line-numbers: true\n    code-block-background: true\n---\n\nThis post explains the deployment of a Dockerized ML model using AWS Kinesis as the event stream and AWS Lambda as the consumer. The Dockerized ML model will be registered on AWS ECR.\n\nThe workflow involves triggering an AWS Lambda function upon receiving a request on AWS Kinesis, which will then invoke the ML model and push the results to another AWS Kinesis stream.\n\n<p align=\"center\">\n<img src=\"./images/lambda-kinesis/aws-deployment.png\" alt=\"AWS deployment - lambda, kinesis, and ecr\"></img>\n</p>\n\nThe interaction with AWS services and resources will be performed from a local machine using the AWS CLI.\n\n## Prerequisites\n\nTo follow along with this post, you'll need the [AWS Command Line Interface (AWS CLI) version 2](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html). To check if you have AWS CLI installed, run the following command:\n\n```bash\naws --version\n```\n\n::: {.callout-note}\n## Important\n\nUse the `SAME region` across all the services and resources.\n:::\n\n## Create an IAM role\n\nCreate an IAM role that you can use to access AWS services and resources. Follow these steps:\n\n1. Sign in to the AWS Management Console and open the [roles page](https://console.aws.amazon.com/iam/home#/roles) in the IAM console.\n2. Click on **Create role**.\n3. Give the role the following properties:\n    - **Trusted entity**: AWS service\n    - **Use case**: Lambda\n    - **Permissions**: AWSLambdaKinesisExecutionRole\n    - **Role name**: `lamdba-kinesis-role`\n\nThe **AWSLambdaKinesisExecutionRole** policy has the permissions that the function needs to read items from Kinesis and write logs to CloudWatch Logs.\n\n## Create a Lambda function\n\nWe'll create a Lambda function that reads records from a Kinesis stream and writes them to CloudWatch Logs. Follow these steps:\n\n1. Open the [Lambda console](https://console.aws.amazon.com/lambda/) in the AWS Management Console.\n2. Choose **Create function**.\n3. Give the function the following properties:\n    - Select **Author from scratch**.\n    - **Function name**: `ride-duration-prediction-test`\n    - **Runtime**: Python 3.9\n    - **Permissions**: Toggle the `Change default execution role` and select `Use an existing role`. Choose the `lamdba-kinesis-role` role that you created earlier.\n4. Choose **Create function**.\n\nOnce the function is created, you'll see code editor with a file name `lambda_function.py` created for you.\n\nInside the file you might have noticed the `lambda_handler` function. The handler function is the entry point for Lambda functions. It's the python function that get's executed when the Lambda function is invoked. \n\nThe handler function always takes two arguments: `event` and `context`. \n\n### Event\n\nThe `event` argument is the data that's passed to the function when it's invoked. For example, if the lambda function is invoked by an HTTP request, the `event` argument will contain information about the HTTP request. \n\nThink of the `event` argument as the input to the function, the function then processes the input based on some logic and may or may not return an output. The functions can be triggered by different events such as an HTTP request, a message in a queue, a file upload to S3, etc.\n\n### Context\n\nThe `context` argument provides information about the function and the execution environment. For example, the `context` argument contains the name of the function, the function version, the execution time, etc.\n\n`context` is a Python object that implements methods and has properties that you can use to get information about the function and the execution environment. For example, you can use the `context.get_remaining_time_in_millis()` method to get the remaining execution time for the function in milliseconds.\n\nLet's add some code to the `lambda_handler` function to log the `event` and `context` arguments:\n\n```python\nimport json\n\ndef lambda_handler(event, context):\n    print(\"Received event: \" + json.dumps(event))\n    print(\"Remaining time (ms): \" + str(context.get_remaining_time_in_millis()))\n    return {\n        'statusCode': 200,\n        'body': json.dumps('Hello from Lambda!')\n    }\n```\n\n::: {.callout-note}\nEverytime you make changes to the Lambda function, you need to deploy the changes. You can deploy the changes by clicking on the **Deploy** button in the top right corner of the Lambda console.\n:::\n\nTo test the function, click on the **Test** button and create a new test event. Give the test event a name, for example `test-event`.\n\nYou'll notice there is a drop down for the event template. For now we'll use the default `hello-world` template, however I encourage you to explore the other templates.\n\nClick on the **Save** button and then click on the **Test** button again.\n\n## Create a Kinesis stream\n\nCreate a Kinesis stream that you can use to send data to the Lambda function. Follow these steps:\n\n1. Open the [Kinesis console](https://console.aws.amazon.com/kinesis) in the AWS Management Console.\n2. Choose **Kinesis Data Streams** and then choose **Create data stream**.\n3. Give the stream the following properties:\n    - **Stream name**: `ride-event`\n    - **Capacity mode**: Provisioned\n    - **Provisioned shards**: 1\n4. Choose **Create data stream**.\n\n## Add a trigger to the Lambda function\n\nA trigger is a configurable resource that enables an AWS service to invoke your function in response to specific events or conditions.\nYour function can be associated with multiple triggers, allowing it to respond to various event-driven scenarios.\n\nAdd a trigger to the Lambda function that we created earlier. Follow these steps:\n\n1. Navigate back to the `ride-duration-prediction-test` Lambda function in the Lambda console.\n2. In the **Function overview** section, choose **Add trigger** button.\n3. Choose **Kinesis** from the list of triggers.\n4. Give the trigger the following properties:\n    - **Kinesis stream**: `ride-event`\n5. Choose **Add**.\n\n## Create a IAM user and policy\n\nTo interact with AWS services and resources from your local machine we need to create an IAM user and policy.\n\nThe IAM user represents the human user or workload who uses the IAM user to interact with AWS. A user in AWS consists of a name and credentials.\n\nThe credentials are used to authenticate the user when they interact with AWS. \n\nThe IAM policy defines the permissions that the user has to access AWS services and resources. We attach the policy to the user to grant the user permissions to access AWS services and resources.\n\nFollow these steps to create an IAM user and policy:\n\n### Policy\n\n1. Sign in to the AWS Management Console and open the [policies page](https://console.aws.amazon.com/iam/home#/policies) in the IAM console.\n2. Choose **Create policy**.\n3. Search for `kinesis` and click on it.\n4. Under the **Access level** section, for **Write** select the **PutRecord** and **PutRecords** actions.\n5. Under the **Resources** section, select **Specific** and then **Add ARN**. \n6. Choose the **Text** tab and Copy-Paste the ARN of the Kinesis stream that you created earlier. You can find the ARN of the Kinesis stream in the Kinesis console under the **Data stream summary** section.\n7. Choose **Add ARNs** and then **Next**.\n8. Give the policy the following properties:\n    - **Name**: `kinesis-write-policy`\n    - **Description**: `Allows write access to the Kinesis stream`\n9. Choose **Create policy**.\n\n### User\n\n1. Sign in to the AWS Management Console and open the [users page](https://console.aws.amazon.com/iam/home#/users) in the IAM console.\n2. Choose **Add users**.\n3. Name the user `kinesis-user`.\n4. Select **Add user to group** and then **Create group**.\n5. Name the group `kinesis-write-group`.\n6. Search for the policy that you created earlier and select it. In this case it's `kinesis-write-policy`.\n7. Choose **Create user group**.\n8. Select the user group that you created earlier and choose **Next**.\n9. Choose **Create user**.\n\n### Create access keys for the IAM user\n\nSelect the IAM user that you created earlier.\n\n1. Choose **Security credentials**. \n2. Under the **Access keys** section, choose **Create access key**.\n3. Choose **Other** and then **Next**.\n5. Choose **Create access key**.\n\n::: {.callout-note}\nMake sure you download the access key file. You will not be able to access the secret access key again after you close the dialog box.\n:::\n\nNext we'll configure the AWS CLI to use the access keys that we just created.\n\n::: {.callout-note}\nMake sure you have the [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) installed.\n:::\n\nRun the following command to configure the AWS CLI:\n\n```bash\naws configure\n```\n\nEnter the access key ID and secret access key that you downloaded earlier. For the default region name, enter the region that you created the Kinesis stream in. \n\n## Send data to the Kinesis stream\n\nNow that we have a Lambda function and a Kinesis stream, let's send some data to the Kinesis stream. Follow these steps:\n\nOpen terminal or command prompt and run the following command to send data to the Kinesis stream:\n\n```bash\nKINESIS_STREAM_NAME=ride-event\naws kinesis put-record \\\n    --stream-name ${KINESIS_STREAM_NAME} \\\n    --partition-key 1 \\\n    --data \"Hello, this is a test.\" \\\n    --cli-binary-format raw-in-base64-out\n```\n\nYou should see the following output:\n\n```bash\n{\n    \"ShardId\": \"shardId-000000000000\",\n    \"SequenceNumber\": \"49605507193692568746561138951989197221575295936257281026\"\n}\n```\n\nPress `q` to exit the output.\n\n### View the logs\n\nNow that we have sent some data to the Kinesis stream, let's view the logs in CloudWatch Logs. Follow these steps:\n\n1. Navigate back to the `ride-duration-prediction-test` Lambda function in the Lambda console.\n2. Choose the **Monitor** tab and then choose **Logs**.\n3. Choose `View CloudWatch Logs` to view the logs in CloudWatch Logs.\n4. Under the **Log streams** tab, choose the latest log stream.\n\nYou'll see the log messages. Find the log message that says `Received event:` and you'll see the request body that we sent to the Kinesis stream. It should look something like this:\n\n```{.bash}\nReceived event: {\n    \"Records\": [\n        {\n            \"kinesis\": {\n                \"kinesisSchemaVersion\": \"1.0\",\n                \"partitionKey\": \"1\",\n                \"sequenceNumber\": \"49641836346418405924036776538604986498724303454435540994\",\n                \"data\": \"SGVsbG8sIFRoaXMgaXMgYSB0ZXN0IG1lc3NhZ2U=\",\n                \"approximateArrivalTimestamp\": 1687115491.219\n            },\n            \"eventSource\": \"aws:kinesis\",\n            \"eventVersion\": \"1.0\",\n            \"eventID\": \"shardId-000000000000:49641836346418405924036776538604986498724303454435540994\",\n            \"eventName\": \"aws:kinesis:record\",\n            \"invokeIdentityArn\": \"arn:aws:iam::247894370182:role/lamdba-kinesis-role\",\n            \"awsRegion\": \"us-east-1\",\n            \"eventSourceARN\": \"arn:aws:kinesis:us-east-1:247894370182:stream/ride-event\"\n        }\n    ]\n}\n```\n\nYou'll notice that the data is base64 encoded. Let's decode the data and print it to the logs. Add the following code to the `lambda_handler` function:\n\n```python\n# Add the following code to the lambda_handler function\nimport base64\n\nrecord = event['Records'][0]\nprint(\"Decoded message: \" + base64.b64decode(record['kinesis']['data']).decode('utf-8'))\n```\n\nDeploy the changes and send some data to the Kinesis stream again. You should see the decoded message in the logs.\n\n:::{.callout-note}\nEverytime you make changes and deploy the Lambda function, there will be a **new entry** in the **Log streams** tab. Make sure you choose the latest log stream.\n:::\n\nUsually after the model prediction is made, we would want to send the prediction to another service. In the next section, we'll send the prediction to another Kinesis stream.\n\n## Send the prediction to another Kinesis stream\n\nIn this section, we'll send the prediction to another Kinesis stream. Follow these steps:\n\n1. Open the [Kinesis console](https://console.aws.amazon.com/kinesis) in the AWS Management Console.\n2. Choose **Kinesis Data Streams** and then choose **Create data stream**.\n3. Give the stream the following properties:\n    - **Stream name**: `ride-prediction`\n    - **Capacity mode**: Provisioned\n    - **Provisioned shards**: 1\n4. Choose **Create data stream**.\n\nNow, that we have created the Kinesis stream, we want the lambda function to send the prediction to the Kinesis stream. \n\nHowever, lambda function doesn't have the permission to write to the Kinesis stream. Let's add the permission to the `lambda-kinesis-role` role. \n\nFollow these steps to create a policy and attach it to the role:\n\n### Create a policy\n\n1. Sign in to the AWS Management Console and open the [policies page](https://console.aws.amazon.com/iam/home#/policies) in the IAM console.\n2. Choose **Create policy**.\n3. Search for `kinesis` and click on it.\n4. Under the **Access level** section, for **Write** select the **PutRecord** and **PutRecords** actions.\n5. Under the **Resources** section, select **Specific** and then **Add ARN**. \n6. Choose the **Text** tab and Copy-Paste the ARN of the Kinesis stream for `ride-prediction`. You can find the ARN of the Kinesis stream in the Kinesis console under the **Data stream summary** section.\n7. Choose **Add ARNs** and then **Next**.\n8. Give the policy the following properties:\n    - **Name**: `kinesis-write-policy-prediction`\n    - **Description**: `Allows write access to the Prediction Kinesis stream`\n9. Choose **Create policy**.\n\n### Attach the policy to the role\n1. Sign in to the AWS Management Console and open the [roles page](https://console.aws.amazon.com/iam/home#/roles) in the IAM console.\n2. Search for `lambda-kinesis-role` and click on it.\n3. Under the **Add Permissions** dropdown, click on **Attach policies**.\n4. Search for `kinesis-write-policy-prediction` and click on it. Chooes **Add Permissions**.\n\nBefore moving forward to the next section, let's review what we have done so far.\n\n## Review what we have done so far [Optional]\n\nI'll divide the review into three parts:\n\n### Part 1: Create a Lambda function and a Kinesis stream\n\n- We created a Lambda function and explained how it works.\n- We also tried to modify the Lambda function and tested it.\n- We created a Kinesis stream that will receive the data.\n- We added a Kinesis trigger to the Lambda function, so that the Lambda function is invoked when data is sent to the Kinesis stream.\n- However, the Lambda function doesn't have the permission to read from the Kinesis stream. So, we added the `AWSLambdaKinesisExecutionRole` permission to the `lambda-kinesis-role` role.\n\n### Part 2: Send data to the Kinesis stream\n\n- To test what we have done in part 1, we sent some data to the Kinesis stream.\n- Since, we are performing this action from our local machine, we need to have the AWS CLI installed and configured.\n- For a individual (user) to interact with the AWS resources, we created an IAM user. However, what are the permissions that we gave to the IAM user? We gave the permissions to send data to the Kinesis stream. We did this by attaching the `kinesis-write-policy` policy to the IAM user, allowing the user to write on `ride-event` Kinesis stream.\n- Similar to how we use API key to authenticate ourselves to a service, we used the `access key` and `secret key` of the IAM user to authenticate ourselves to AWS.\n- Lastly, we used the AWS CLI to send data to the Kinesis stream and view the logs in CloudWatch Logs.\n\n### Part 3: Send the prediction to another Kinesis stream\n\n- We created another Kinesis stream called `ride-prediction` which will receive the prediction from the Lambda function.\n- However, does the Lambda function have the permission to write to the Kinesis stream? No, it doesn't. So, we created a policy called `kinesis-write-policy-prediction` that allows the Lambda function to write to the Kinesis stream. \n- We attached this policy to the `lambda-kinesis-role` role.\n- We'll now update the Lambda function to send the prediction to the Kinesis stream.\n\n## Update the Lambda function to send the prediction to the Kinesis stream\n\nNow that we have created the policy and attached it to the role, let's update the lambda function to send the prediction to the Kinesis stream.\n\n```python\nimport os\nimport json\nimport boto3\nimport base64\n\n# Create a Kinesis client\nkinesis_client = boto3.client('kinesis')\n\n# Get the name of the Kinesis stream from the environment variable\nPREDICTIONS_STREAM_NAME = os.getenv('PREDICTIONS_STREAM_NAME')\n\ndef lambda_handler(event, context):\n    print(\"Received event: \" + json.dumps(event))\n    record = event['Records'][0]\n    decoded_data = base64.b64decode(record['kinesis']['data']).decode('utf-8')\n    print(\"Decoded message: \" + decoded_data)\n    \n    ride_event = json.loads(decoded_data)\n    ride_id = ride_event['ride_id']\n    \n    # Create a dummy prediction\n    prediction = {\n        'model': 'ride_duration_prediction_model',\n        'version': '12345',\n        'prediction': {\n            'ride_id': ride_id,\n            'ride_duration': 10.0\n        }\n    }\n\n    # Write the prediction on the prediction-kinesis-stream\n    kinesis_client.put_record(\n        StreamName=PREDICTIONS_STREAM_NAME,\n        Data=json.dumps(prediction),\n        PartitionKey=str(ride_id)\n    )\n\n    return {\n        'statusCode': 200,\n        'body': json.dumps('Hello from Lambda!')\n    }\n\n```\n\nWe used `boto3` to create a Kinesis client. We then used the `put_record` method to write the prediction to the Kinesis stream. We used the `PREDICTIONS_STREAM_NAME` environment variable to get the name of the Kinesis stream.\n\n`Deploy` the Lambda function.\n\nSetup the environment variable by following these steps:\n\n1. Go to the Lambda console and click on the **Configuration** tab.\n2. Under the **Environment variables** section, click on **Edit**.\n3. Choose **Add environment variable**.\n4. Give the environment variable the following properties:\n    - **Key**: `PREDICTIONS_STREAM_NAME`\n    - **Value**: `ride-prediction`\n5. Click on **Save**.\n\nNow, that we have updated the Lambda function, let's test it.\n\n### Test the Lambda function\n\nUse the following code to test the Lambda function:\n\n```bash\nKINESIS_STREAM_NAME=ride-event\naws kinesis put-record \\\n    --stream-name ${KINESIS_STREAM_NAME} \\\n    --partition-key 1 \\\n    --data '{\"ride_id\": 1}' \\\n    --cli-binary-format raw-in-base64-out\n```\n\nLike before you can view the logs in CloudWatch Logs.\n\n### View the prediction in the Kinesis stream\nFor the user to view the prediction, we need to give the user the permission to read from the Kinesis stream. We'll do this by attaching the `kinesis-read-policy-prediction` policy to the `kinesis-user`.\n\nCreate the `kinesis-read-policy-prediction` policy by following these steps:\n\n1. Sign in to the AWS Management Console and open the [policies page](https://console.aws.amazon.com/iam/home#/policies) in the IAM console.\n2. Choose **Create policy**.\n3. Search for `kinesis` and click on it.\n4. Under the **Access level** section, for **Read** select the **GetRecords** and **GetShardIterator** actions.\n5. Under the **Resources** section, select **Specific** and then **Add ARN**. \n6. Choose the **Text** tab and Copy-Paste the ARN of the Kinesis stream for `ride-prediction`. You can find the ARN of the Kinesis stream in the Kinesis console under the **Data stream summary** section.\n7. Choose **Add ARNs** and then **Next**.\n8. Give the policy the following properties:\n    - **Name**: `kinesis-read-policy-prediction`\n    - **Description**: `Allows read access to the Prediction Kinesis stream`\n9. Choose **Create policy**.\n\nAttach the `kinesis-read-policy-prediction` policy to the `kinesis-user` by following these steps:\n\n1. Sign in to the AWS Management Console and open the [users page](https://console.aws.amazon.com/iam/home#/users) in the IAM console.\n2. Choose **kinesis-user**.\n3. Choose the **Add Permissions** tab and then **Add permissions**.\n4. Choose **Attach existing policies directly**.\n5. Search for `kinesis-read-policy-prediction` and select it. Choose **Next**.\n6. Choose **Add permissions**.\n\nNow that we push the prediction to the prediction stream, we can view the prediction in the Kinesis stream. Use the following code in the terminal to view the prediction:\n\n```bash\nKINESIS_STREAM_OUTPUT='ride-prediction'\n# Get the shard id, since we only have one shard we can hardcode it\nSHARD='shardId-000000000000'\n\n# Get the shard iterator\nSHARD_ITERATOR=$(aws kinesis \\\n    get-shard-iterator \\\n        --shard-id ${SHARD} \\\n        --shard-iterator-type TRIM_HORIZON \\\n        --stream-name ${KINESIS_STREAM_OUTPUT} \\\n        --query 'ShardIterator' \\\n)\n\n# Get the records\nRESULT=$(aws kinesis get-records --shard-iterator $SHARD_ITERATOR)\n\n# Print the data\necho ${RESULT} \n\n# Print the data in a more readable format for a specific record\necho ${RESULT} | jq -r '.Records[0].Data' | base64 --decode\n```\nA shard iterator specifies the shard position from which to start reading data records sequentially. \n\nThere are many types of shard iterators. We are using the `TRIM_HORIZON` shard iterator type. This type causes the shard iterator to point to the last untrimmed record in the shard in the system, which is the oldest data record in the shard. \n\nFor more information on shard iterators, see [here](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetShardIterator.html).\n\nWe get the data from the iterator and print it.\n\n::: {.callout-tip}\njq is a lightweight and flexible command-line JSON processor. If you don't have `jq` installed on your machine, you can install it by following these [instructions](https://jqlang.github.io/jq/download/)\n:::\n\n## Dockerize the prediction service\n\nUntil now, we have been using a dummy script to generate the prediction. In this section, we will upload model to S3 and then use it to generate the prediction. We will use Docker to containerize the prediction service.\n\n### Upload the model to S3\n\nWe will use the `lin_reg.bin` file that I had created for you. You can find it [here](https://drive.google.com/file/d/1XbjfjLkk55LSLrgjMWnvsURK981hATjF/view?usp=sharing). Download the file and upload it to the `s3://ride-prediction-model` bucket. Follow these steps to upload the file:\n\n1. Go to the [S3 console](https://console.aws.amazon.com/s3) and click on the `Create bucket` button.\n2. Give the bucket the following properties:\n    - **Bucket name**: `ride-prediction-model`\n3. Click on **Create Bucket**.\n4. Click on the bucket name.\n5. Click on the **Upload** button.\n6. Click on **Add files** and select the `lin_reg.bin` file.\n7. Click on **Upload**.\n\n### Update the lambda function\n\nLocally create a `lambda_function.py` file with the following code:\n\n```python\nimport os\nimport json\nimport s3fs\nimport boto3\nimport base64\nimport pickle\n\ns3 = s3fs.S3FileSystem(anon=False)\nkinesis_client = boto3.client('kinesis')\nPREDICTIONS_STREAM_NAME = os.getenv('PREDICTIONS_STREAM_NAME')\n\nfile = 's3://ride-prediction-model/lin_reg.bin'\nwith s3.open(file, 'rb') as handle:\n    dv, model = pickle.load(handle)\n\nTEST_RUN = os.getenv('TEST_RUN', 'False') == 'True'\n\ndef prepare_features(ride):\n    features = {}\n    features['PU_DO'] = '%s_%s' % (ride['PULocationID'], ride['DOLocationID'])\n    features['trip_distance'] = ride['trip_distance']\n    return features\n\ndef predict(features):\n    X = dv.transform(features)\n    pred = model.predict(X)\n    return float(pred[0])\n\ndef lambda_handler(event, context):  \n    predictions_events = []\n    \n    for record in event['Records']:\n        encoded_data = record['kinesis']['data']\n        decoded_data = base64.b64decode(encoded_data).decode('utf-8')\n        ride_event = json.loads(decoded_data)\n\n        ride = ride_event['ride']\n        ride_id = ride_event['ride_id']\n    \n        features = prepare_features(ride)\n        prediction = predict(features)\n    \n        prediction_event = {\n            'model': 'ride_duration_prediction_model',\n            'version': '123',\n            'prediction': {\n                'ride_duration': prediction,\n                'ride_id': ride_id   \n            }\n        }\n\n        # Comment the below if statement if you want to test the lambda function locally\n        if not TEST_RUN:\n            kinesis_client.put_record(\n                StreamName=PREDICTIONS_STREAM_NAME,\n                Data=json.dumps(prediction_event),\n                PartitionKey=str(ride_id)\n            )\n        \n        predictions_events.append(prediction_event)\n\n    return {\n        'predictions': predictions_events\n    }\n```\n\n### Create a virtual environment\n\nCreate a virtual environment and install the required packages:\n\n```bash\npipenv install boto3 scikit-learn s3fs --python 3.9\n```\n\n### Create a Dockerfile\n\nCreate a `Dockerfile` with the following content:\n\n```dockerfile\nFROM public.ecr.aws/lambda/python:3.9\n\nRUN pip install -U pip\nRUN pip install pipenv\n\nCOPY [\"Pipfile\", \"Pipfile.lock\", \"./\"]\n\nRUN pipenv install --system --deploy\n\nCOPY [\"lambda_function.py\", \"./\"]\n\nCMD [\"lambda_function.lambda_handler\"]\n```\n\n### Build the Docker image\n\nBuild the Docker image:\n\n```bash\n# Windows/Linux users\ndocker build -t stream-model-duration:v1 .\n\n# Mac users\ndocker build -t stream-model-duration:v1 . --platform=linux/amd64\n```\n\n### Give lambda access to s3\n\nWe need to give the lambda function access to the `ride-prediction-model` bucket. Follow these steps to give the lambda function access to the bucket:\n\nCreate the `s3-list-read` policy by following these steps:\n\n1. Sign in to the AWS Management Console and open the [policies page](https://console.aws.amazon.com/iam/home#/policies) in the IAM console.\n2. Choose **Create policy**.\n3. Search for `s3` and click on it.\n4. Under the **Access level** section, for **Read** select the **GetObject** action and **List** select the **ListBucket** action.\n5. Under the **Resources** section, select **Specific** and then **Add ARN**. You'll need to add the ARNs for s3 bucket and `lin_reg.bin` object in s3. See the image below for an example.\n\n![](./images/lambda-kinesis/s3-arn.png)\n\n6. Choose the **Text** tab and Copy-Paste the ARN of the S3 bucket and object.\n7. Choose **Add ARNs** and then **Next**.\n8. Give the policy the following properties:\n    - **Name**: `s3-list-read`\n    - **Description**: `Allows lambda to list and read objects from s3`\n9. Choose **Create policy**.\n\nAttach the policy to the role:\n\n1. Sign in to the AWS Management Console and open the [roles page](https://console.aws.amazon.com/iam/home#/roles) in the IAM console.\n2. Search for `lambda-kinesis-role` and click on it.\n3. Under the **Add Permissions** dropdown, click on **Attach policies**.\n4. Search for `s3-list-read` and click on it. Chooes **Add Permissions**.\n\n### Create ECR Repository\nWe will use ECR to store the Docker image. Follow these steps to create a repository in ECR:\n\n1. Go to the [ECR console](https://console.aws.amazon.com/ecr).\n2. Click on **Create repository**.\n3. Give the repository the following properties:\n    - **Repository name**: `duration-model`\n4. Click on **Create repository**.\n\n### Attach permissions to the user that will push the Docker image to ECR\n\nCreate the `ecr-read-write` policy by following these steps:\n\n1. Sign in to the AWS Management Console and open the [policies page](https://console.aws.amazon.com/iam/home#/policies) in the IAM console.\n2. Choose **Create policy**.\n3. Search for `ecr` and click on it.\n4. Under the **Access level** section,\n    - **Read** select **BatchCheckLayerAvailability** and **GetAuthorizationToken** \n    - **Write** select **CompleteLayerUpload**, **InitiateLayerUpload**, **PutImage**, and **UploadLayerPart**.\n5. Under the **Resources** section, select **Specific** and then **Add ARN**. You'll need to add the ARNs for ecr repository.\n6. Choose the **Text** tab and Copy-Paste the ARN of the ecr repo.\n7. Choose **Add ARNs** and then **Next**.\n8. Give the policy the following properties:\n    - **Name**: `ecr-read-write`\n    - **Description**: `Allows user to read and write to ecr repo`\n9. Choose **Create policy**.\n\nAttach the policy to the role:\n\n1. Sign in to the AWS Management Console and open the [roles page](https://console.aws.amazon.com/iam/home#/roles) in the IAM console.\n2. Search for `lambda-kinesis-role` and click on it.\n3. Under the **Add Permissions** dropdown, click on **Attach policies**.\n4. Search for `ecr-read-write` and click on it. Chooes **Add Permissions**.\n\n### Upload the Docker image to ECR\n\nAuthenticate Docker to an Amazon ECR registry with `get-login-password`:\n\n```bash\n# Authenticate Docker to an Amazon ECR registry with get-login-password\naws ecr get-login-password --region <REGION> | docker login --username AWS --password-stdin <AWS_ACCOUNT_ID>.dkr.ecr.us-east-1.amazonaws.com\n```\n\nReplace the `<REGION>` and `<AWS_ACCOUNT_ID>` with the region and account id of your AWS account.\n\nTag the Docker image and push it to ECR:\n\n```bash\n# Store the URI of the ECR repository\nREMOTE_URI=\"<AWS_ACCOUNT_ID>.dkr.ecr.us-east-2.amazonaws.com/duration-model\"\nREMOTE_TAG=\"v1\"\nREMOTE_IMAGE=${REMOTE_URI}:${REMOTE_TAG}\n\n# Tag the local image with the remote image URI\nLOCAL_IMAGE=\"stream-model-duration:v1\"\ndocker tag ${LOCAL_IMAGE} ${REMOTE_IMAGE}\n\n# Push the image to ECR\ndocker push ${REMOTE_IMAGE}\n```\n\n### Create a lambda function\nWe'll create a lambda function using the docker image that we created. Follow these steps to create the lambda function:\n\n1. Go to the [lambda console](https://console.aws.amazon.com/lambda).\n2. Click on **Create function**.\n3. Choose **Container image** option.\n4. Give the function the following properties:\n    - **Function name**: `ride-duration-prediction`\n    - **Container image URI**: Browse and select the image that we pushed to ECR.\n5. Toggle the `Change default execution role`.\n6. Choose the `Use an existing role` option.\n7. Select the `lambda-kinesis-role` role.\n8. Click on **Create function**.\n\nSetup the environment variable by following these steps:\n\n1. Go to the Lambda console and click on the **Configuration** tab.\n2. Under the **Environment variables** section, click on **Edit**.\n3. Choose **Add environment variable**.\n4. Give the environment variable the following properties:\n    - **Key**: `PREDICTIONS_STREAM_NAME`\n    - **Value**: `ride-prediction`\n5. Click on **Save**.\n\nUpdate the timeout and memory settings by following these steps:\n\n1. Go to the Lambda console and click on the **Configuration** tab.\n2. Under the **General configuration** section, click on **Edit**.\n3. Increase the **Timeout** to `30 seconds` and **Memory** to `256 MB`.\n\n### Add a trigger to the lambda function\n\nWe'll add a trigger to the lambda function so that it gets invoked when a new object is created in the `ride-prediction-model` bucket. Follow these steps to add a trigger to the lambda function:\n\n1. Navigate back to the `ride-duration-prediction` Lambda function in the Lambda console.\n2. In the **Function overview** section, choose **Add trigger** button.\n3. Choose **Kinesis** from the list of triggers.\n4. Give the trigger the following properties:\n    - **Kinesis stream**: `ride-event`\n5. Choose **Add**.\n\n::: {.callout-tip}\nYou can delete the `ride-duration-prediction-test` lambda function that we created earlier. Since the trigger for both the lambda functions is the same, we don't want to trigger both the lambda functions.\n:::\n\n### Test the lambda function\n\nWe'll test the lambda function by invoking it with a sample event. Run the following command to invoke the lambda function:\n\n```bash\nKINESIS_STREAM_INPUT=ride-event\naws kinesis put-record \\\n    --stream-name ${KINESIS_STREAM_INPUT} \\\n    --partition-key 1 \\\n    --data '{\"ride\": {\"PULocationID\": 130,\"DOLocationID\": 205,\"trip_distance\": 3.66}, \"ride_id\": 156}' \\\n    --cli-binary-format raw-in-base64-out\n```\n\nNavigate to the [CloudWatch console](https://console.aws.amazon.com/cloudwatch) and click on **Logs**. You should see a log stream for the `ride-duration-prediction` lambda function. Click on the log stream and you should see the log messages from the lambda function.\n\nSimilar to how we read the output from the `ride-prediction` stream, you can follow the same steps to read the output from the `ride-prediction` stream.\n\n## Clean up your resources\n\nTo avoid incurring future charges, delete the resources you created unless you want to retain them for future use.\n\n1. Delete the `ride-prediction` stream.\n2. Delete the `ride-event` stream.\n3. Delete the `ride-duration-prediction` lambda function.\n4. Delete the `ride-prediction-model` bucket.\n5. Delete the `duration-model` repository from ECR.\n6. Delete all the policies, roles, and users that we created.\n\n## References\n\n1. [AWS event & context](https://aws-lambda-for-python-developers.readthedocs.io/en/latest/02_event_and_context/)\n2. [Tutorial: Using AWS Lambda with Amazon Kinesis](https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis-example.html)\n3. [AWS lambda/python docker images](https://gallery.ecr.aws/lambda/python)\n4. [AWS ECR Private Registry Authentication](https://docs.aws.amazon.com/AmazonECR/latest/userguide/registry_auth.html)\n5. [Pushing a Docker Image to AWS ECR](https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html)\n\n## Conclusion\n\nIn this article, you learned how to build a real-time streaming application using Kinesis Data Streams and Lambda. Also, dockerized the machine learning model, registered it with ECR, and used it in the lambda function.\n\nI hope you enjoyed this article. If you have any questions, feel free to reach out to me on Twitter or Email.\n\nüëè Upvote if you liked it, üí¨ comment if you loved it. Hope to see you guys in the next one. ‚úåÔ∏è Peace!\n\n",
    "supporting": [
      "aws-deployment-lambda-kinesis_files"
    ],
    "filters": [],
    "includes": {}
  }
}