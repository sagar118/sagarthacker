{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"MLflow on AWS\"\n",
    "description: \"Discover the implementation of MLflow on AWS, leveraging EC2 to host MLFlow Server, S3 for artifact storage and RDS-PostgreSQL for backend entity storager.\"\n",
    "author: \"Sagar Thacker\"\n",
    "date: \"2023-05-30\"\n",
    "# image: path_to_master_ml.jpg\n",
    "categories: [MLOps, MLflow, AWS]\n",
    "format:\n",
    "    html:\n",
    "        toc: true\n",
    "        code-summary: \"Show the code\"\n",
    "        code-line-numbers: true\n",
    "        code-block-background: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this blog post we explore how to set up MLflow on AWS, leveraging EC2 to host MLFlow Server, S3 for artifact storage and RDS-PostgreSQL for backend entity storager.\n",
    "\n",
    "If you're interested in learning about MLflow or need an introduction to its workings, I recommend checking out my previous blog post titled [\"Introduction to MLflow\"](https://sagarthacker.com/posts/mlops/mlflow.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AWS EC2 Instance\n",
    "\n",
    "1. Go to `https://aws.amazon.com` to Sign in / Create an AWS Account.\n",
    "2. To launch EC2 instance, click on to `services` on the left-top corner of the page. Select `Compute` and `EC2`.\n",
    "\n",
    "![](./images/find_ec2.png)\n",
    "\n",
    "3. To launch a new instance, click on `Launch Instance`.\n",
    "\n",
    "![](./images/launch_instance.png)\n",
    "\n",
    "4. Name our Instance\n",
    "\n",
    "![](./images/aws_mlflow/instance_name.png)\n",
    "\n",
    "5. Keep the default settings for `Application and OS Image`, and `Instance Type`.\n",
    "\n",
    "6. If you don't already have a `Key pair`, you can create a new key pair. You would be asked to download and save your key pair.\n",
    "\n",
    "::: {.callout-tip}\n",
    "Save your key pair at `~/.ssh/` folder.\n",
    ":::\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./images/setup_key_pair.png\"/>\n",
    "</p>\n",
    "\n",
    "7. Keep all the other settings as default and click on `Launch Instance`.\n",
    "\n",
    "## Configure Security Group\n",
    "\n",
    "1. After the instance is launched, click on `Security` Section on the Instance Summary Page.\n",
    "\n",
    "![](./images/aws_mlflow/security_section.png)\n",
    "\n",
    "2. Click on `Edit Inbound Rules` and add a new rule for `Custom TCP` with port `5000` and source. Save the changes.\n",
    "\n",
    "![](./images/aws_mlflow/edit_security_rules.png)\n",
    "\n",
    "![](./images/aws_mlflow/ipv4_5000_rule.png)\n",
    "\n",
    "## Create S3 Bucket\n",
    "\n",
    "1. Go to `services` on the left-top corner of the page. Select `Storage` and `S3`. Click on `Create Bucket`.\n",
    "\n",
    "![](./images/aws_mlflow/s3_bucket.png)\n",
    "\n",
    "2. Name your bucket and select the region. Keep all the other settings as default and click on `Create Bucket`.\n",
    "\n",
    "![](./images/aws_mlflow/s3_bucket_name.png)\n",
    "\n",
    "::: {.callout-note}\n",
    "## Important\n",
    "\n",
    "Please make note of the `bucket name` for later use.\n",
    ":::\n",
    "\n",
    "## Create RDS Database\n",
    "\n",
    "1. Go to `services` on the left-top corner of the page. Select `Database` and `RDS`. Click on `Create Database`.\n",
    "\n",
    "![](./images/aws_mlflow/rds.png)\n",
    "\n",
    "2. Choose `Standard create` and select `PostgreSQL` as the engine.\n",
    "\n",
    "3. Select `Free tier` in the Templates section.\n",
    "\n",
    "4. In the Settings section, name your database i.e., DB Instance Identifier (eg. `mlflow-database`). In the Credentials section, enter a username (eg. `mlflow`) and Tick the `Auto generate a password` checkbox.\n",
    "\n",
    "5. In the Additional configuration section, Set the `Initial database name` (eg. `mlflow_db`) under the Database options.\n",
    "\n",
    "::: {.callout-note}\n",
    "## Important\n",
    "\n",
    "Please make note of the `username` and the `database name` for later use.\n",
    ":::\n",
    "\n",
    "6. Keep all the other settings as default and click on `Create Database`.\n",
    "\n",
    "7. The database would take a few minutes to be created. To check the password, click on `View credential details`.\n",
    "\n",
    "::: {.callout-warning}\n",
    "You would need the password save the password for later use. This is the only time you would be able to view the password. However, you can always reset the password.\n",
    ":::\n",
    "\n",
    "![](./images/aws_mlflow/view_credentials.png)\n",
    "\n",
    "::: {.callout-note}\n",
    "## Important\n",
    "\n",
    "After the database is created, please save the endpoint and port for later use. You can find the endpoint and port on the database summary page. Marked in the image below with a green dotted box.\n",
    ":::\n",
    "\n",
    "8. Next, you would need to add an inbound rule to the security group of the database. To do so, click on the `Security` section on the database summary page. Click on `Edit Inbound Rules` and add a new rule for `PostgreSQL` with port `5432` and source. Save the changes. This allows the EC2 instance to connect to the database.\n",
    "\n",
    "![](./images/aws_mlflow/rds_security_group.png)\n",
    "\n",
    "![](./images/aws_mlflow/edit_inbound_rules.png)\n",
    "\n",
    "Select the security group that was created automatically when we launched the EC2 instance. \n",
    "\n",
    "![](./images/aws_mlflow/add_postgres_rule.png)\n",
    "\n",
    "## Install MLflow\n",
    "\n",
    "1. We'll utilize the easiest way to connect to the EC2 instance. Click on the `Connect` button on the EC2 instance summary page. In the EC2 Instance Connect section, click on `Connect`.\n",
    "\n",
    "![](./images/aws_mlflow/connect_to_ec2.png)\n",
    "\n",
    "![](./images/aws_mlflow/connect.png)\n",
    "\n",
    "This would open a terminal window in the browser.\n",
    "\n",
    "2. Run the following commands to install MLflow.\n",
    "\n",
    "```bash\n",
    "sudo yum update\n",
    "\n",
    "pip3 install mlflow boto3 psycopg2-binary\n",
    "```\n",
    "\n",
    "::: {.callout-note}\n",
    "If you get an error saying `No module named pip`, run the following command to install pip.\n",
    "\n",
    "```bash\n",
    "python3 -m ensurepip --upgrade\n",
    "```\n",
    ":::\n",
    "\n",
    "3. Next, we need to set up the MLflow Tracking Server. To do so, run the following command.\n",
    "\n",
    "```bash\n",
    "mlflow server -h 0.0.0.0 -p 5000 --backend-store-uri postgresql://DB_USER:DB_PASSWORD@DB_ENDPOINT:PORT/DB_NAME --default-artifact-root s3://S3_BUCKET_NAME\n",
    "\n",
    "# Example, Replace the following values\n",
    "mlflow server -h 0.0.0.0 -p 5000 --backend-store-uri postgresql://mlflow:WJgpP1lv4PQVnhzdq7T5@mlflow-database.c4rrlovvb5cx.us-east-1.rds.amazonaws.com:5432/mlflow_db --default-artifact-root s3://mlflow-artifact-remote-storage\n",
    "```\n",
    "\n",
    "::: {.callout-note}\n",
    "We made note of the `ENDPOINT` and `PORT` which can be replaced for `DB_ENDPOINT` and `PORT` respectively. We also saved the `PASSWORD` when creating the RDS-Postgresql database which can be replaced for `DB_PASSWORD` respectively.\n",
    "\n",
    "Similarly, we made note of the `USERNAME` and `DATABASE_NAME` which can be replaced for `DB_USER` and `DB_NAME` respectively. We also made note of the `BUCKET_NAME` which can be replaced for `S3_BUCKET_NAME`.\n",
    ":::\n",
    "\n",
    "4. To checkout the MLflow UI, open a new tab in the browser and enter the following URL.\n",
    "\n",
    "```bash\n",
    "http://EC2_INSTANCE_PUBLIC_IPv4_ADDRESS:5000\n",
    "\n",
    "# Example, Replace the following values\n",
    "http://52.91.235.206:5000/\n",
    "```\n",
    "\n",
    "![](./images/aws_mlflow/mlflow_ui.png)\n",
    "\n",
    "Voila! You have successfully set up MLflow on AWS EC2 instance.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup AWS CLI and Profile on local\n",
    "\n",
    "1. Install AWS CLI on your local machine. You can follow the instructions [here](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html).\n",
    "2. Go to IAM on AWS Console and click on Policies on the left panel. Click on `Create Policy`.\n",
    "\n",
    "![](./images/aws_mlflow/create_policy.png)\n",
    "\n",
    "3. Search for `S3` and click on s3.\n",
    "\n",
    "![](./images/aws_mlflow/permission.png)\n",
    "\n",
    "4. Select all the access level for `List`, and `Read`. Select all the marked options for `Write`.\n",
    "\n",
    "![](./images/aws_mlflow/read_list.png)\n",
    "\n",
    "![](./images/aws_mlflow/write.png)\n",
    "\n",
    "5. Mark all the checkboxes in the Resources section. Click on `Next`.\n",
    "\n",
    "![](./images/aws_mlflow/resources.png)\n",
    "\n",
    "6. Give the name for the Policy and click on `Create Policy`.\n",
    "\n",
    "![](./images/aws_mlflow/policy_name.png)\n",
    "\n",
    "7. Go to IAM and click on `UsersGroup` on the left panel. Click on `Create group`. Give a name for the group and select the policy that was created in the previous step. Click on `Create group`.\n",
    "\n",
    "![](./images/aws_mlflow/user_group.png)\n",
    "\n",
    "8. Go to IAM and click on `Users` on the left panel. Click on `Add user`. Give a name for the user and click on Next.\n",
    "\n",
    "![](./images/aws_mlflow/add_users.png)\n",
    "\n",
    "![](./images/aws_mlflow/create_user.png)\n",
    "\n",
    "9. Select `Add user to group` Permission option and select the group that was created in the previous step. Click on `Next`. Review the details and click on `Create user`.\n",
    "\n",
    "![](./images/aws_mlflow/user_to_group.png)\n",
    "\n",
    "10. Go to `Users` again and click on the user that was created in the previous step. Click on `Security credentials` tab. Click on `Create access key`.\n",
    "\n",
    "![](./images/aws_mlflow/create_access_key.png)\n",
    "\n",
    "11. Click on `Other` and click on `Next` Button. Then Click on `Create Access Key`.\n",
    "\n",
    "12. Click on `Download .csv file` and save the file. This file contains the `Access Key ID` and `Secret Access Key` which would be used to configure the AWS CLI.\n",
    "\n",
    "13. Open the terminal and run the following command.\n",
    "\n",
    "```bash\n",
    "aws configure\n",
    "```\n",
    "\n",
    "14. Enter the `Access Key ID` and `Secret Access Key` that was saved in the previous step. For other options like `Default region name` and `Default output format`, keep the default values by pressing enter.\n",
    "\n",
    "15. You can check if the AWS CLI is configured correctly by running the following command.\n",
    "\n",
    "```bash\n",
    "aws s3 ls\n",
    "```\n",
    "\n",
    "You should be able to see the list of buckets that are present in your AWS account.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow in Action\n",
    "\n",
    "I'll refer to the [Introduction to MLflow](http://localhost:4308/posts/mlops/mlflow.html) post for this section. I'll be using the same code and data for this section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Create a new virtual environment and install MLflow and other libraries using the following command:\n",
    "\n",
    "```{.bash filename=\"requirements.txt\"}\n",
    "mlflow\n",
    "jupyter\n",
    "scikit-learn\n",
    "pandas\n",
    "xgboost\n",
    "fastparquet\n",
    "hyperopt\n",
    "optuna\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Create conda environment\n",
    "conda create -p venv python=3.9 -y\n",
    "\n",
    "# Activate conda environment\n",
    "conda activate venv/\n",
    "\n",
    "# Install required libraries\n",
    "pip install -r requirements.txt --no-cache-dir\n",
    "```\n",
    "\n",
    "Download the dataset using the following command:\n",
    "\n",
    "```bash\n",
    "# Create data directory\n",
    "mkdir data\n",
    "\n",
    "# Move to data directory\n",
    "cd data\n",
    "\n",
    "# Download dataset\n",
    "wget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-01.parquet # January 2022\n",
    "wget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-02.parquet # February 2022\n",
    "wget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-03.parquet # March 2022\n",
    "```\n",
    "\n",
    "We'll also create a jupyter notebook named `mlflow.ipynb` to run our experiment. After following the above steps, you should have the following directory structure:\n",
    "\n",
    "```bash\n",
    ".\n",
    "â”œâ”€â”€ data\n",
    "â”‚   â”œâ”€â”€ green_tripdata_2022-01.parquet\n",
    "â”‚   â”œâ”€â”€ green_tripdata_2022-02.parquet\n",
    "â”‚   â””â”€â”€ green_tripdata_2022-03.parquet\n",
    "â”œâ”€â”€ mlflow.ipynb\n",
    "â”œâ”€â”€ requirements.txt\n",
    "â””â”€â”€ venv\n",
    "```\n",
    "\n",
    "The below hidden code block imports the required libraries, loads & transforms the dataset, and splits the dataset into train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: true\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import mlflow\n",
    "import optuna\n",
    "\n",
    "from mlflow.entities import ViewType\n",
    "from mlflow.tracking import MlflowClient\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def read_dataframe(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a Parquet file into a pandas DataFrame, performs data transformations, and returns the resulting DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): The path to the Parquet file to be read.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The processed DataFrame containing the data from the Parquet file.\n",
    "\n",
    "    Raises:\n",
    "        [Any exceptions raised by pandas.read_parquet()]\n",
    "\n",
    "    Notes:\n",
    "        - The function performs the following transformations on the DataFrame:\n",
    "            - Converts 'lpep_dropoff_datetime' and 'lpep_pickup_datetime' columns to pandas datetime objects.\n",
    "            - Computes the 'duration' column by subtracting 'lpep_pickup_datetime' from 'lpep_dropoff_datetime'\n",
    "              and converting the result to minutes.\n",
    "            - Filters the DataFrame to include rows where the 'duration' is between 1 and 60 minutes (inclusive).\n",
    "            - Converts 'PULocationID' and 'DOLocationID' columns to string type.\n",
    "\n",
    "    Example:\n",
    "        filename = 'data.parquet'\n",
    "        df = read_dataframe(filename)\n",
    "    \"\"\"\n",
    "    # Read the Parquet file into a DataFrame\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    # Convert 'lpep_dropoff_datetime' and 'lpep_pickup_datetime' columns to pandas datetime objects\n",
    "    df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n",
    "    df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "\n",
    "    # Compute the 'duration' column by subtracting 'lpep_pickup_datetime' from 'lpep_dropoff_datetime' and converting to minutes\n",
    "    df['duration'] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n",
    "    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "    # Filter the DataFrame to include rows where the 'duration' is between 1 and 60 minutes (inclusive)\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    # Convert 'PULocationID' and 'DOLocationID' columns to string type\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "    \n",
    "    # Return the processed DataFrame\n",
    "    return df\n",
    "\n",
    "# Read the Parquet file for training data into a DataFrame\n",
    "df_train = read_dataframe('./data/green_tripdata_2022-01.parquet')\n",
    "\n",
    "# Read the Parquet file for validation data into a DataFrame\n",
    "df_val = read_dataframe('./data/green_tripdata_2022-02.parquet')\n",
    "\n",
    "# Read the Parquet file for testing data into a DataFrame\n",
    "df_test = read_dataframe('./data/green_tripdata_2022-03.parquet')\n",
    "\n",
    "def preprocess(df: pd.DataFrame, dv: DictVectorizer, fit_dv: bool = False):\n",
    "    \"\"\"\n",
    "    Preprocesses a pandas DataFrame by creating new features, transforming categorical features into a numerical format,\n",
    "    and returning the transformed data along with the DictVectorizer.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): The input DataFrame to be preprocessed.\n",
    "        dv (sklearn.feature_extraction.DictVectorizer): The DictVectorizer instance to be used for transforming categorical features.\n",
    "        fit_dv (bool, optional): Indicates whether to fit the DictVectorizer on the data. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the transformed feature matrix and the DictVectorizer instance.\n",
    "\n",
    "    Notes:\n",
    "        - The function assumes that the DataFrame contains the columns 'PULocationID' and 'DOLocationID'.\n",
    "        - The function creates a new feature 'PU_DO' by concatenating 'PULocationID' and 'DOLocationID'.\n",
    "        - The categorical feature 'PU_DO' and numerical feature 'trip_distance' are selected for transformation.\n",
    "        - The function transforms the selected features into a dictionary representation and applies the DictVectorizer.\n",
    "        - If fit_dv is True, the DictVectorizer is fitted on the data. Otherwise, the existing fitted DictVectorizer is used.\n",
    "\n",
    "    Example:\n",
    "        df = read_dataframe('data.parquet')\n",
    "        dv = DictVectorizer()\n",
    "        X, dv = preprocess(df, dv, fit_dv=True)\n",
    "    \"\"\"\n",
    "    # Create a new feature 'PU_DO' by concatenating 'PULocationID' and 'DOLocationID'\n",
    "    df['PU_DO'] = df['PULocationID'] + '_' + df['DOLocationID']\n",
    "\n",
    "    # Select categorical and numerical features for transformation\n",
    "    categorical = ['PU_DO']\n",
    "    numerical = ['trip_distance']\n",
    "\n",
    "    # Convert the selected features into a dictionary representation\n",
    "    dicts = df[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "    # Apply DictVectorizer for transforming categorical features\n",
    "    if fit_dv:\n",
    "        # Fit the DictVectorizer on the data\n",
    "        X = dv.fit_transform(dicts)\n",
    "    else:\n",
    "        # Transform using the existing fitted DictVectorizer\n",
    "        X = dv.transform(dicts)\n",
    "\n",
    "    # Return the transformed feature matrix and DictVectorizer\n",
    "    return X, dv\n",
    "\n",
    "# Extract the target variable\n",
    "target = 'duration'\n",
    "\n",
    "# Extract the target variable from the training, validation and testing datasets\n",
    "y_train = df_train[target].values\n",
    "y_val = df_val[target].values\n",
    "y_test = df_test[target].values\n",
    "\n",
    "# Initialize a DictVectorizer for preprocessing\n",
    "dv = DictVectorizer()\n",
    "\n",
    "# Preprocess the training data\n",
    "X_train, dv = preprocess(df_train, dv, fit_dv=True)\n",
    "\n",
    "# Preprocess the validation data using the fitted DictVectorizer from the training data\n",
    "X_val, _ = preprocess(df_val, dv, fit_dv=False)\n",
    "\n",
    "# Preprocess the testing data using the fitted DictVectorizer from the training data\n",
    "X_test, _ = preprocess(df_test, dv, fit_dv=False)\n",
    "\n",
    "def dump_pickle(obj, filename: str):\n",
    "    \"\"\"\n",
    "    Pickles (serializes) an object and saves it to a file.\n",
    "\n",
    "    Parameters:\n",
    "        obj (Any): The object to be pickled.\n",
    "        filename (str): The path and filename to save the pickled object.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Notes:\n",
    "        - The function uses the 'pickle' module to serialize the object and save it to a file.\n",
    "        - The file is opened in binary mode for writing using the \"wb\" mode.\n",
    "    \"\"\"\n",
    "    with open(filename, \"wb\") as f_out:\n",
    "        return pickle.dump(obj, f_out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the MLflow Tracking Server URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACKING_SERVER_HOST = \"52.91.235.206\" # fill in with the public IPv4 of the EC2 instance\n",
    "mlflow.set_tracking_uri(f\"http://{TRACKING_SERVER_HOST}:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking URI: 'http://52.91.235.206:5000'\n"
     ]
    }
   ],
   "source": [
    "print(f\"tracking URI: '{mlflow.get_tracking_uri()}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/31 11:43:55 INFO mlflow.tracking.fluent: Experiment with name 'nyc-taxi-experiment' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://mlflow-artifact-remote-storage/1', creation_time=1685547835140, experiment_id='1', last_update_time=1685547835140, lifecycle_stage='active', name='nyc-taxi-experiment', tags={}>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new experiment\n",
    "mlflow.set_experiment(\"nyc-taxi-experiment\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "Train a simple Linear Regression model and log the model parameters, metrics, and artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the destination path for saving model files\n",
    "model_path = \"./outputs/models\"\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Set a tag for the developer\n",
    "    mlflow.set_tag(\"developer\", \"Sagar\")\n",
    "\n",
    "    # Initialize and train a LinearRegression model\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the validation data\n",
    "    yhat = lr.predict(X_val)\n",
    "\n",
    "    # Calculate the root mean squared error (RMSE)\n",
    "    rmse = mean_squared_error(y_val, yhat, squared=False)\n",
    "\n",
    "    # Log the RMSE metric to MLflow\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    # Create dest_path folder unless it already exists\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "    # Save the trained model as a pickle file\n",
    "    dump_pickle(lr, os.path.join(model_path, \"lin_reg.pkl\"))\n",
    "\n",
    "    # Log the trained model as an artifact to MLflow\n",
    "    mlflow.log_artifact(local_path=f\"{model_path}/lin_reg.pkl\", artifact_path=\"model\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see the MLflow UI with the experiment run.\n",
    "\n",
    "![](./images/aws_mlflow/lin_reg_exp.png)\n",
    "\n",
    "![](./images/aws_mlflow/lin_reg_exp_details.png)\n",
    "\n",
    "You can also see that the artifacts are stored in S3 bucket.\n",
    "\n",
    "![](./images/aws_mlflow/s3_artifacts.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "Next, we'll create a new experiment to perform hyperparameter tuning using Hyperopt and Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://mlflow-artifact-remote-storage/3', creation_time=1685548270366, experiment_id='3', last_update_time=1685548270366, lifecycle_stage='active', name='random-forest-hyperopt', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new experiment\n",
    "mlflow.set_experiment(\"random-forest-hyperopt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-31 11:57:34,678]\u001b[0m A new study created in memory with name: no-name-41ada3cf-dabd-4100-a384-dd6a2ee09103\u001b[0m\n",
      "\u001b[32m[I 2023-05-31 11:57:38,080]\u001b[0m Trial 0 finished with value: 6.012747224033297 and parameters: {'n_estimators': 25, 'max_depth': 20, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 0 with value: 6.012747224033297.\u001b[0m\n",
      "\u001b[32m[I 2023-05-31 11:57:38,649]\u001b[0m Trial 1 finished with value: 6.249433998787504 and parameters: {'n_estimators': 16, 'max_depth': 4, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 0 with value: 6.012747224033297.\u001b[0m\n",
      "\u001b[32m[I 2023-05-31 11:57:42,149]\u001b[0m Trial 2 finished with value: 6.039045655830305 and parameters: {'n_estimators': 34, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 0 with value: 6.012747224033297.\u001b[0m\n",
      "\u001b[32m[I 2023-05-31 11:57:43,620]\u001b[0m Trial 3 finished with value: 6.179387143797027 and parameters: {'n_estimators': 44, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 0 with value: 6.012747224033297.\u001b[0m\n",
      "\u001b[32m[I 2023-05-31 11:57:45,445]\u001b[0m Trial 4 finished with value: 6.075505898039151 and parameters: {'n_estimators': 22, 'max_depth': 11, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 0 with value: 6.012747224033297.\u001b[0m\n",
      "\u001b[32m[I 2023-05-31 11:57:46,134]\u001b[0m Trial 5 finished with value: 6.441117537172997 and parameters: {'n_estimators': 35, 'max_depth': 3, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 0 with value: 6.012747224033297.\u001b[0m\n",
      "\u001b[32m[I 2023-05-31 11:57:49,416]\u001b[0m Trial 6 finished with value: 6.0285791267371165 and parameters: {'n_estimators': 28, 'max_depth': 16, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 0 with value: 6.012747224033297.\u001b[0m\n",
      "\u001b[32m[I 2023-05-31 11:57:49,958]\u001b[0m Trial 7 finished with value: 7.881244954282265 and parameters: {'n_estimators': 34, 'max_depth': 1, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 0 with value: 6.012747224033297.\u001b[0m\n",
      "\u001b[32m[I 2023-05-31 11:57:51,295]\u001b[0m Trial 8 finished with value: 6.025608014492215 and parameters: {'n_estimators': 12, 'max_depth': 19, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 0 with value: 6.012747224033297.\u001b[0m\n",
      "\u001b[32m[I 2023-05-31 11:57:51,848]\u001b[0m Trial 9 finished with value: 7.071070856187059 and parameters: {'n_estimators': 22, 'max_depth': 2, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 0 with value: 6.012747224033297.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def run_optimization(num_trials: int = 10):\n",
    "    \"\"\"\n",
    "    Runs the optimization process using Optuna library to find the optimal hyperparameters for RandomForestRegressor.\n",
    "\n",
    "    Parameters:\n",
    "        num_trials (int): The number of optimization trials to perform. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Notes:\n",
    "        - The function defines an objective function for Optuna to minimize the root mean squared error (RMSE).\n",
    "        - The objective function samples hyperparameters, trains a RandomForestRegressor model with those hyperparameters,\n",
    "          evaluates the model on the validation data, and logs the RMSE metric to MLflow.\n",
    "        - Optuna performs the optimization process by searching for the set of hyperparameters that minimizes the RMSE.\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(trial):\n",
    "        \"\"\"\n",
    "        Objective function for Optuna optimization.\n",
    "\n",
    "        Parameters:\n",
    "            trial (optuna.Trial): A trial object representing a single optimization trial.\n",
    "\n",
    "        Returns:\n",
    "            float: The value of the objective function (RMSE).\n",
    "\n",
    "        Notes:\n",
    "            - The objective function samples hyperparameters from the defined search space.\n",
    "            - It initializes and trains a RandomForestRegressor model with the sampled hyperparameters.\n",
    "            - The model is evaluated on the validation data, and the RMSE is calculated.\n",
    "            - The RMSE and the sampled hyperparameters are logged to MLflow.\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 10, 50, 1),\n",
    "            'max_depth': trial.suggest_int('max_depth', 1, 20, 1),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 10, 1),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 4, 1),\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        # Start a new MLflow run for each trial\n",
    "        with mlflow.start_run():\n",
    "            # Set a tag for the model type\n",
    "            mlflow.set_tag(\"model\", \"RandomForestRegressor\")\n",
    "            \n",
    "            # Log the sampled hyperparameters to MLflow\n",
    "            mlflow.log_params(params)\n",
    "\n",
    "            # Initialize a RandomForestRegressor model with the sampled hyperparameters\n",
    "            rf = RandomForestRegressor(**params)\n",
    "\n",
    "            # Train the model on the training data\n",
    "            rf.fit(X_train, y_train)\n",
    "\n",
    "            # Make predictions on the validation data\n",
    "            y_pred = rf.predict(X_val)\n",
    "\n",
    "            # Calculate the root mean squared error (RMSE)\n",
    "            rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "\n",
    "            # Log the RMSE metric to MLflow\n",
    "            mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "        return rmse\n",
    "\n",
    "    # Use the Tree-structured Parzen Estimator (TPE) sampler for efficient hyperparameter search\n",
    "    sampler = TPESampler(seed=42)\n",
    "\n",
    "    # Create an Optuna study with the defined objective function and search direction\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "\n",
    "    # Run the optimization process with the specified number of trials\n",
    "    study.optimize(objective, n_trials=num_trials)\n",
    "\n",
    "run_optimization()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/aws_mlflow/hyperparameter_tuning.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Registry\n",
    "\n",
    "Finally, we'll take the top 5 models from the hyperparameter tuning experiment and run them on the test dataset. We'll log the metrics and artifacts for each model. We'll register the model with the best metrics to the model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://mlflow-artifact-remote-storage/2', creation_time=1685548138856, experiment_id='2', last_update_time=1685548138856, lifecycle_stage='active', name='random-forest-best-models', tags={}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameter optimization experiment name\n",
    "HPO_EXPERIMENT_NAME = \"random-forest-hyperopt\"\n",
    "\n",
    "# Best model experiment name\n",
    "EXPERIMENT_NAME = \"random-forest-best-models\"\n",
    "\n",
    "# Create a new experiment for the best models\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically log parameters and metrics\n",
    "mlflow.sklearn.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest Parameters\n",
    "RF_PARAMS = ['max_depth', 'n_estimators', 'min_samples_split',\n",
    "             'min_samples_leaf', 'random_state', 'n_jobs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_log_model(params):\n",
    "    \"\"\"\n",
    "    Trains a RandomForestRegressor model with the given hyperparameters and logs evaluation metrics to MLflow.\n",
    "\n",
    "    Parameters:\n",
    "        params (dict): Dictionary of hyperparameters for RandomForestRegressor.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Notes:\n",
    "        - The function starts an MLflow run to track the model training and evaluation process.\n",
    "        - It converts certain hyperparameters to integers.\n",
    "        - A RandomForestRegressor model is initialized with the provided hyperparameters.\n",
    "        - The model is trained on the training data.\n",
    "        - The trained model is evaluated on the validation and test sets, and the root mean squared error (RMSE) is calculated and logged to MLflow as evaluation metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        # Convert specific hyperparameters to integers\n",
    "        for param in RF_PARAMS:\n",
    "            params[param] = int(params[param])\n",
    "\n",
    "        # Initialize a RandomForestRegressor model with the given hyperparameters\n",
    "        rf = RandomForestRegressor(**params)\n",
    "\n",
    "        # Train the model on the training data\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the trained model on the validation set\n",
    "        val_rmse = mean_squared_error(y_val, rf.predict(X_val), squared=False)\n",
    "\n",
    "        # Log the validation RMSE metric to MLflow\n",
    "        mlflow.log_metric(\"val_rmse\", val_rmse)\n",
    "\n",
    "        # Evaluate the trained model on the test set\n",
    "        test_rmse = mean_squared_error(y_test, rf.predict(X_test), squared=False)\n",
    "\n",
    "        # Log the test RMSE metric to MLflow\n",
    "        mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "\n",
    "def run_register_model(top_n: int):\n",
    "    \"\"\"\n",
    "    Runs the process to register the best model based on the top_n model runs with the lowest test RMSE.\n",
    "\n",
    "    Parameters:\n",
    "        top_n (int): The number of top model runs to consider.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Notes:\n",
    "        - The function interacts with the MLflow tracking server to retrieve and register models.\n",
    "        - It retrieves the top_n model runs based on the lowest validation RMSE.\n",
    "        - For each run, it trains a model using the hyperparameters from the run and logs evaluation metrics to MLflow.\n",
    "        - After evaluating the models, it selects the one with the lowest test RMSE.\n",
    "        - The selected model is registered with a specified name in MLflow.\n",
    "    \"\"\"\n",
    "\n",
    "    # Connect to the MLflow tracking server\n",
    "    client = MlflowClient()\n",
    "\n",
    "    # Retrieve the top_n model runs and log the models\n",
    "    experiment = client.get_experiment_by_name(HPO_EXPERIMENT_NAME)\n",
    "\n",
    "    # Retrieve the top_n model runs based on the lowest validation RMSE\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=experiment.experiment_id,\n",
    "        run_view_type=ViewType.ACTIVE_ONLY,\n",
    "        max_results=top_n,\n",
    "        order_by=[\"metrics.rmse ASC\"]\n",
    "    )\n",
    "\n",
    "    # Train and log the model for each run\n",
    "    for run in runs:\n",
    "        # Train and log the model based on the hyperparameters from the run\n",
    "        train_and_log_model(params=run.data.params)\n",
    "\n",
    "    # Select the model with the lowest test RMSE\n",
    "    experiment = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\n",
    "    # Retrieve model runs based on the lowest test RMSE, and select the first run (with the lowest test RMSE)\n",
    "    best_run = client.search_runs(\n",
    "        experiment_ids=experiment.experiment_id,\n",
    "        run_view_type=ViewType.ACTIVE_ONLY,\n",
    "        order_by=[\"metrics.test_rmse ASC\"]\n",
    "    )[0]\n",
    "\n",
    "    # Register the best model\n",
    "    model_uri = f\"runs:/{best_run.info.run_id}/model\"\n",
    "\n",
    "    # Register the best model with a specified name\n",
    "    mlflow.register_model(\n",
    "        model_uri=model_uri,\n",
    "        name=\"random-forest-best-model\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/31 11:58:21 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/wizard/Astronaut/Dev/MLOps/week2/venv/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n",
      "Successfully registered model 'random-forest-best-model'.\n",
      "2023/05/31 11:58:49 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: random-forest-best-model, version 1\n",
      "Created version '1' of model 'random-forest-best-model'.\n"
     ]
    }
   ],
   "source": [
    "# The number of top model runs to consider\n",
    "top_n = 5\n",
    "\n",
    "run_register_model(top_n=top_n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/aws_mlflow/best_test_models.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! ðŸŽ‰ You have successfully set up MLflow on AWS EC2 instance and used it to track your machine learning experiments.\n",
    "\n",
    "::: {.callout-tip}\n",
    "## STOP EC2 instance\n",
    "\n",
    "Please remember to stop the EC2 instance after completing your work to avoid incurring any additional charges. \n",
    ":::\n",
    "\n",
    "Thank you for reading and I hope you found this post helpful. Upvote if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
