[
  {
    "objectID": "posts/2021-04-05/Path-to-become-a-Machine-Learning-Expert.html",
    "href": "posts/2021-04-05/Path-to-become-a-Machine-Learning-Expert.html",
    "title": "Path to become a Machine Learning Expert",
    "section": "",
    "text": "Path to becoming a Machine Learning (ML) Expert made easy. There are a lot of resources out there that can be overwhelming at the start. But don‚Äôt worry this learning path would provide structure and lay the foundational knowledge to begin a career in ML."
  },
  {
    "objectID": "posts/2021-04-05/Path-to-become-a-Machine-Learning-Expert.html#learn-the-basics-of-descriptive-statistics-inferential-statistics-and-math-used-in-machine-learning",
    "href": "posts/2021-04-05/Path-to-become-a-Machine-Learning-Expert.html#learn-the-basics-of-descriptive-statistics-inferential-statistics-and-math-used-in-machine-learning",
    "title": "Path to become a Machine Learning Expert",
    "section": "1. Learn the basics of Descriptive Statistics, Inferential Statistics and Math used in Machine Learning",
    "text": "1. Learn the basics of Descriptive Statistics, Inferential Statistics and Math used in Machine Learning\nUnderstanding the math used in ML can help in building the foundation strong. Udacity offers courses on descriptive statistics and inferential statistics. These courses are free and use excel to teach the concepts.\nAlong with statistics and probabilities, concepts on linear algebra, multivariate calculus, optimization functions and many more form the building blocks for ML. There is an awesome youtube channel that makes these concepts very easy to learn. 3Brown1Blue focuses on teaching mathematics using a distinct visual perspective.\nMore resources:\n\nComputational Linear Algebra for Coders\nProf.¬†Gilbert Strang‚Äôs Linear Algebra book/course\nMatrix Cookbook by Kaare Brandt Petersen & Michael Syskind Pedersen\nThink Stats (Exploratory Data Analysis in Python) by Allen Downey\nConvex Optimization by Stephen Boyd and Lieven Vandenberghe\nEssentials of Metaheuristics by Sean Luke"
  },
  {
    "objectID": "posts/2021-04-05/Path-to-become-a-Machine-Learning-Expert.html#learn-the-basics-of-python-and-its-packages",
    "href": "posts/2021-04-05/Path-to-become-a-Machine-Learning-Expert.html#learn-the-basics-of-python-and-its-packages",
    "title": "Path to become a Machine Learning Expert",
    "section": "2. Learn the basics of Python and it‚Äôs packages",
    "text": "2. Learn the basics of Python and it‚Äôs packages\nFirst, let‚Äôs install Python. The easiest way to do this is by installing Anaconda. All the packages that are required come along with Anaconda.\nYou can start from learning the basics of Python i.e.¬†data structures, functions, class, etc. and it‚Äôs libraries. I started learning about python in my college days, I read the book Learn Python the Hard Way. A very good book for beginners. Introduction to Python Programming by Udacity is a free course that covers the basics of Python. Introduction to Python is another free course by Analytics Vidhya. Another free course by Google is Google‚Äôs Python Class.\nNext, learn about how to use Regular Expression (also called regex) in Python. It will come in use for data cleaning, especially if you are working with text data. Learn regular expressions through Google class. A very good beginner tutorial for learning regular expression in python on Analytics Vidhya. Cheatsheet for Regex.\nNow comes the fun part of learning the various libraries in Python. Numpy, Pandas, Matplotlib, Seaborn, and Sklearn are the packages heavily used in ML.\n\nNumpy provides a high-performance multidimensional array and basic tools to compute with and manipulate these arrays. Numpy quickstart tutorial is a good place to start. This will form a good foundation for this to come. Practice numpy by solving 100 numpy exercises to solve.\nPandas is used for data manipulation and analysis. The most used package in Python is Pandas. Intro to pandas data structure provides a detailed tutorial on pandas. A short course by Kaggle on pandas.\nMatplotlib is a visualization library in python. In the matplotlib tutorial, you will learn the basics of Python data visualization, the anatomy of a Matplotlib plot, and much more. Official documentation of matplotlib is one of the best ways to learn the library.\nSeaborn is another visualization library built on top of matplotlib. Kaggle short course on data visualization provides a good start point to learn the library."
  },
  {
    "objectID": "posts/2021-04-05/Path-to-become-a-Machine-Learning-Expert.html#data-explorationcleaningpreparation",
    "href": "posts/2021-04-05/Path-to-become-a-Machine-Learning-Expert.html#data-explorationcleaningpreparation",
    "title": "Path to become a Machine Learning Expert",
    "section": "3. Data Exploration/Cleaning/Preparation",
    "text": "3. Data Exploration/Cleaning/Preparation\nReal-world data is unstructured, contains missing values, outliers, typos, etc. This step is one of the most important steps for a data analyst to perform because how good the model will perform will depend on the quality of the data.\nLearn different stages of data explorations:\n\nVariable Identification, Univariate and Multivariate analysis\nMissing values treatment\nOutlier treatment\nFeature Engineering\n\nAdditional resources:\n\nYou can also refer to the data exploration guide.\nBook on Python for Data Analysis by Wes McKinney"
  },
  {
    "objectID": "posts/2021-04-05/Path-to-become-a-Machine-Learning-Expert.html#introduction-to-machine-learning",
    "href": "posts/2021-04-05/Path-to-become-a-Machine-Learning-Expert.html#introduction-to-machine-learning",
    "title": "Path to become a Machine Learning Expert",
    "section": "4. Introduction to Machine Learning",
    "text": "4. Introduction to Machine Learning\nNow it‚Äôs time to enter the belly of the beast. There are various resources to learn ML and I would suggest the following courses:\n\nMachine Learning by Stanford (Coursera) The Machine Learning course by Andrew Ng is one of the best courses out there and covers all the basic algorithms. Also, it introduces all the advanced topics in a very simple manner which is easy to understand. However, this course is taught in Octave rather than the popular languages like R/Python. Also, this course is NOT free but you can apply for financial aid.\nMachine Learning A-Z‚Ñ¢: Hands-On Python & R In Data Science (Udemy) Good course for beginners. Explore complex topics such as natural language processing (NLP), reinforcement learning (RL), deep learning (DL) among many others. Tons of practice exercise and quizzes. This course is NOT free but comparatively not expensive.\nMachine Learning (edx) This is an advanced course that has the highest math prerequisite out of any other course in this list. You‚Äôll need a very firm grasp of Linear Algebra, Calculus, Probability, and programming. This course is free of cost but to acquire a certificate payment is required.\nComprehensive learning path for Data Science (Analytics Vidhya) This course covers every topic right from the beginning. Installing Python, data cleaning and preparation, Machine learning concepts, deep learning, and NLP. This course is free and does not come with any certification.\n\nBooks:\n\nThe Hundred Page Machine Learning Book\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n\nList of best books for machine learning.\nAfter learning about the various techniques in ML the next natural thing to do is apply those techniques. What better place than Kaggle. It is one of the most popular websites among data science enthusiasts. Below two problem statement can be a good starting problem statement to begin with.\n\nTitanic: Machine Learning from Disaster\nHouse Prices: Advanced Regression Techniques"
  },
  {
    "objectID": "posts/2021-04-05/Path-to-become-a-Machine-Learning-Expert.html#deep-learning",
    "href": "posts/2021-04-05/Path-to-become-a-Machine-Learning-Expert.html#deep-learning",
    "title": "Path to become a Machine Learning Expert",
    "section": "5. Deep Learning",
    "text": "5. Deep Learning\nUsing the idea to mimic a human brain has been around since the 1900s. There were various algorithms and techniques developed for the same but due to the lack of computing power, it was difficult to run those algorithms.\nDue to the improvements in the hardware and the introduction to using GPUs to compute caught the attention of people passionate about working on neural net-based models. Today, state of the art results can be obtained using deep neural networks.\nCourses from deeplearning.ai on Coursera are one of the most popular and fantastic courses on deep learning.\n\nNeural Networks and Deep Learning\nDeep Learning Specialization\n\nBoth the courses are paid but financial aid is available for both of them.\nAdditional Resources:\n\nDeep Learning Summer School, Montreal 2015\nDeep Learning for Perception, Virginia Tech, Electrical, and Computer Engineering\nCS231N 2017\nA blog that explains concepts on Convolutional Neural Nets (CNN)\n(Book) Deep Learning ‚Äì Methods and Applications\n(Youtube Channel) DeepLearning.TV\nDeep Learning book from MIT\nNeural Networks and Deep Learning online Book\nComprehensive resources on deeplearning.net"
  },
  {
    "objectID": "posts/2021-04-05/Path-to-become-a-Machine-Learning-Expert.html#natural-language-processing",
    "href": "posts/2021-04-05/Path-to-become-a-Machine-Learning-Expert.html#natural-language-processing",
    "title": "Path to become a Machine Learning Expert",
    "section": "6. Natural Language Processing",
    "text": "6. Natural Language Processing\nNatural language processing (NLP) is a branch of Artificial Intelligence which aims to bridge the gap between how a computer and human communicate with each other. The two major handles used for communication are speech and written i.e.¬†text. If you are unfamiliar with what NLP is, this blog could help in understanding what NLP is.\nCourses:\n\n(Youtube) Natural Language Processing by University of Michigan\nSpeech and Language Processing\nStanford CS224N: NLP with Deep Learning Winter 2019 ‚Äì Stanford\nLecture Collection on Natural Language Processing with Deep Learning (Winter 2017) ‚Äì Stanford\nCS224d: Deep Learning for Natural Language Processing ‚Äì Stanford\nNatural Language Processing Specialization offered by deeplearning.ai on Coursera (Intermediate level)\nNatural Language Processing offered by National Research University Higher School of Economics on Coursera (Advanced level course)\n\nMachine Learning in itself is a huge domain and the only way to master it is to explore and practice. I cannot stress more on practice because without practice is like trying to play the guitar without any strings.\nPopular blogs to follow:\n\nAnalytics Vidhya\nMachine Learning Mastery\nTowards Data Science\nKDnuggets\n\nAdditional Resources:\n\nA Complete Python Tutorial to Learn Data Science from Scratch\nA Comprehensive Learning Path for Deep Learning in 2019 on Analytics Vidhya\nLearning Path to Master Computer Vision in 2020 on Analytics Vidhya\nA Comprehensive Learning Path to Understand and Master NLP in 2020 on Analytics Vidhya\nA Comprehensive Guide to Understand and Implement Text Classification in Python on Analytics Vidhya\nCollection of datasets for NLP\nA comprehensive Learning path to becoming a data scientist in 2020 free course on Analytics Vidhya\n\nI wish you all the best on your journey to becoming a machine learning expert.\nShare if you like it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "posts/2021-04-19/Text-Preprocessing.html",
    "href": "posts/2021-04-19/Text-Preprocessing.html",
    "title": "Text Preprocessing",
    "section": "",
    "text": "In any machine learning task, cleaning and pre-processing of the data is a very important step. The better we can represent our data, the better the model training and prediction can be expected.\nSpecially in the domain of Natural Language Processing (NLP) the data is unstructured. It become crucial to clean and properly format it based on the task at hand. There are various pre-processing steps that can be performed but not necessary to perform all. These steps should be applied based on the problem statement.\nExample: Sentiment analysis on twitter data can required to remove hashtags, emoticons, etc. but this may not be the case if we are doing the same analysis on customer feedback data.\nHere we are using the twitter_sample dataset from the nltk library."
  },
  {
    "objectID": "posts/2021-04-19/Text-Preprocessing.html#lower-casing",
    "href": "posts/2021-04-19/Text-Preprocessing.html#lower-casing",
    "title": "Text Preprocessing",
    "section": "Lower Casing",
    "text": "Lower Casing\nLowercasing is a common text preprocessing technique. It helps to transform all the text in same case.  Examples ‚ÄòThe‚Äô, ‚Äòthe‚Äô, ‚ÄòThE‚Äô -> ‚Äòthe‚Äô\nThis is also useful to find all the duplicates since words in different cases are treated as separate words and becomes difficult for us to remove redundant words in all different case combination.\nThis may not be helpful when we do tasks like Part of Speech tagging (where proper casing gives some information about Nouns and so on) and Sentiment Analysis (where upper casing refers to anger and so on)\n\ndf.text = df.text.str.lower()\ndf.head(2)\n\n\n\n\n\n  \n    \n      \n      text\n      label\n    \n  \n  \n    \n      0\n      username changed! :d\n      1\n    \n    \n      1\n      @kimtaaeyeonss unnieeee!!!:)\n      1"
  },
  {
    "objectID": "posts/2021-04-19/Text-Preprocessing.html#remove-redundant-features",
    "href": "posts/2021-04-19/Text-Preprocessing.html#remove-redundant-features",
    "title": "Text Preprocessing",
    "section": "Remove Redundant Features",
    "text": "Remove Redundant Features\n\n\n\n\n\n\nNote\n\n\n\nNote: How you define redundant features varies based on the problem statement.\n\n\n\nURL‚Äôs\nURL stands for Uniform Resource Locator. If present in a text, it represents the location of another website.\nIf we are performing any websites backlink analysis, in that case URL‚Äôs are useful to keep. Otherwise, they don‚Äôt provide any information. So we can remove them from our text.\n\ndf.text = df.text.str.replace(r'https?://\\S+|www\\.\\S+', '', regex=True)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      text\n      label\n    \n  \n  \n    \n      0\n      username changed! :d\n      1\n    \n    \n      1\n      @kimtaaeyeonss unnieeee!!!:)\n      1\n    \n    \n      2\n      @amyewest thanks! i hope you've got a good book to keep you company. :-)\n      1\n    \n    \n      3\n      :) where are you situated? @hijay09\n      1\n    \n    \n      4\n      @egaroo you're welcome, i'm glad you liked it :)\n      1\n    \n  \n\n\n\n\n\n\nE-mail\nE-mail id‚Äôs are common in customer feedback data and they do not provide any useful information. So we remove them from the text.\nTwitter data that we are using does not contain any email id‚Äôs. Hence, please find the code snipper with an dummy example to remove e-mail id‚Äôs.\n\ntext = 'I have being trying to contact xyz via email to xyz@abc.co.in but there is no response.'\nre.sub(r'\\S+@\\S+', '', text)\n\n'I have being trying to contact xyz via email to  but there is no response.'\n\n\n\n\nDate\nDates can be represented in various formats and can be difficult at times to remove them. They are unlikely to contain any useful information for predicting the labels.\nBelow I have used dummy text to showcase the following task.\n\ntext = \"Today is 22/12/2020 and after two days on 24-12-2020 our vacation starts until 25th.09.2021\"\n\n# 1. Remove date formats like: dd/mm/yy(yy), dd-mm-yy(yy), dd(st|nd|rd).mm/yy(yy)\nre.sub(r'\\d{1,2}(st|nd|rd|th)?[-./]\\d{1,2}[-./]\\d{2,4}', '', text)\n\n'Today is  and after two days on  our vacation starts until '\n\n\n\ntext = \"Today is 11th of January, 2021 when I am writing this post. I hope to post this by February 15th or max to max by 20 may 21 or 20th-December-21\"\n\n# 2. Remove date formats like: 20 apr 21, April 15th, 11th of April, 2021\npattern = re.compile(r'(\\d{1,2})?(st|nd|rd|th)?[-./,]?\\s?(of)?\\s?([J|j]an(uary)?|[F|f]eb(ruary)?|[Mm]ar(ch)?|[Aa]pr(il)?|[Mm]ay|[Jj]un(e)?|[Jj]ul(y)?|[Aa]ug(ust)?|[Ss]ep(tember)?|[Oo]ct(ober)?|[Nn]ov(ember)?|[Dd]ec(ember)?)\\s?(\\d{1,2})?(st|nd|rd|th)?\\s?[-./,]?\\s?(\\d{2,4})?')\npattern.sub(r'', text)\n\n'Today is  when I am writing this post. I hope to post this byor max to max by or '\n\n\nThere are various formats in which dates are represented and the above regex can be customized in many ways. Above, ‚Äúbyor‚Äù got combined cause we are trying multiple format in single regex pattern. You can customize the above expression accordingly to your need.\n\n\nHTML Tags\nIf we are extracting data from various websites, it is possible that the data also contains HTML tags. These tags does not provide any information and should be removed. These tags can be removed using regex or by using BeautifulSoup library.\n\n# Dummy text\ntext = \"\"\"\n<title>Below is a dummy html code.</title>\n<body>\n    <p>All the html opening and closing brackets should be remove.</p>\n    <a href=\"https://www.abc.com\">Company Site</a>\n</body>\n\"\"\"\n\n\n# Using regex to remove html tags\npattern = re.compile('<.*?>')\npattern.sub('', text)\n\n'\\nBelow is a dummy html code.\\n\\n    All the html opening and closing brackets should be remove.\\n    Company Site\\n\\n'\n\n\n\n# Using Beautiful Soup\ndef remove_html(text):\n    clean_text = BeautifulSoup(text).get_text()\n    return clean_text\n\n\nremove_html(text)\n\n'\\nBelow is a dummy html code.\\n\\nAll the html opening and closing brackets should be remove.\\nCompany Site\\n\\n'\n\n\n\n\nEmojis\nAs more and more people have started using social media emoji‚Äôs play a very crucial role. Emoji‚Äôs are used to express emotions that are universally understood.\nIn some analysis such as sentiment analysis emoji‚Äôs can be useful. We can convert them to words or create some new features based on them. For some analysis we need to remove them. Find the below code snippet used to remove the emoji‚Äôs.\n\n\nCode\n# Reference: https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               u\"\\U0001f926-\\U0001f937\"\n                               u\"\\U00010000-\\U0010ffff\"\n                               u\"\\u2640-\\u2642\"\n                               u\"\\u2600-\\u2B55\"\n                               u\"\\u200d\"\n                               u\"\\u23cf\"\n                               u\"\\u23e9\"\n                               u\"\\u231a\"\n                               u\"\\ufe0f\"  # dingbats\n                               u\"\\u3030\"\n                               \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\n\ntext = \"game is on üî•üî•. HilariousüòÇ\"\nremove_emoji(text)\n\n'game is on . Hilarious'\n\n\n\n# Remove emoji's from text\ndf.text = df.text.apply(lambda x: remove_emoji(x))\n\n\n\nEmoticons\nEmoji‚Äôs and Emoticons are different. Yes!! Emoticons are used to express facial expressions using keyboard characters such as letters, numbers, and pucntuation marks. Where emjoi‚Äôs are small images.\nThanks to Neel Shah for curating a dictionary of emoticons and their description. We shall use this dictionary and remove the emoticons from our text.\n\n\nCode\nEMOTICONS = {\n    u\":‚Äë\\)\":\"Happy face or smiley\",\n    u\":\\)\":\"Happy face or smiley\",\n    u\":-\\]\":\"Happy face or smiley\",\n    u\":\\]\":\"Happy face or smiley\",\n    u\":-3\":\"Happy face smiley\",\n    u\":3\":\"Happy face smiley\",\n    u\":->\":\"Happy face smiley\",\n    u\":>\":\"Happy face smiley\",\n    u\"8-\\)\":\"Happy face smiley\",\n    u\":o\\)\":\"Happy face smiley\",\n    u\":-\\}\":\"Happy face smiley\",\n    u\":\\}\":\"Happy face smiley\",\n    u\":-\\)\":\"Happy face smiley\",\n    u\":c\\)\":\"Happy face smiley\",\n    u\":\\^\\)\":\"Happy face smiley\",\n    u\"=\\]\":\"Happy face smiley\",\n    u\"=\\)\":\"Happy face smiley\",\n    u\":‚ÄëD\":\"Laughing, big grin or laugh with glasses\",\n    u\":D\":\"Laughing, big grin or laugh with glasses\",\n    u\"8‚ÄëD\":\"Laughing, big grin or laugh with glasses\",\n    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n    u\"X‚ÄëD\":\"Laughing, big grin or laugh with glasses\",\n    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n    u\":-\\)\\)\":\"Very happy\",\n    u\":‚Äë\\(\":\"Frown, sad, andry or pouting\",\n    u\":-\\(\":\"Frown, sad, andry or pouting\",\n    u\":\\(\":\"Frown, sad, andry or pouting\",\n    u\":‚Äëc\":\"Frown, sad, andry or pouting\",\n    u\":c\":\"Frown, sad, andry or pouting\",\n    u\":‚Äë<\":\"Frown, sad, andry or pouting\",\n    u\":<\":\"Frown, sad, andry or pouting\",\n    u\":‚Äë\\[\":\"Frown, sad, andry or pouting\",\n    u\":\\[\":\"Frown, sad, andry or pouting\",\n    u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\n    u\">:\\[\":\"Frown, sad, andry or pouting\",\n    u\":\\{\":\"Frown, sad, andry or pouting\",\n    u\":@\":\"Frown, sad, andry or pouting\",\n    u\">:\\(\":\"Frown, sad, andry or pouting\",\n    u\":'‚Äë\\(\":\"Crying\",\n    u\":'\\(\":\"Crying\",\n    u\":'‚Äë\\)\":\"Tears of happiness\",\n    u\":'\\)\":\"Tears of happiness\",\n    u\"D‚Äë':\":\"Horror\",\n    u\"D:<\":\"Disgust\",\n    u\"D:\":\"Sadness\",\n    u\"D8\":\"Great dismay\",\n    u\"D;\":\"Great dismay\",\n    u\"D=\":\"Great dismay\",\n    u\"DX\":\"Great dismay\",\n    u\":‚ÄëO\":\"Surprise\",\n    u\":O\":\"Surprise\",\n    u\":‚Äëo\":\"Surprise\",\n    u\":o\":\"Surprise\",\n    u\":-0\":\"Shock\",\n    u\"8‚Äë0\":\"Yawn\",\n    u\">:O\":\"Yawn\",\n    u\":-\\*\":\"Kiss\",\n    u\":\\*\":\"Kiss\",\n    u\":X\":\"Kiss\",\n    u\";‚Äë\\)\":\"Wink or smirk\",\n    u\";\\)\":\"Wink or smirk\",\n    u\"\\*-\\)\":\"Wink or smirk\",\n    u\"\\*\\)\":\"Wink or smirk\",\n    u\";‚Äë\\]\":\"Wink or smirk\",\n    u\";\\]\":\"Wink or smirk\",\n    u\";\\^\\)\":\"Wink or smirk\",\n    u\":‚Äë,\":\"Wink or smirk\",\n    u\";D\":\"Wink or smirk\",\n    u\":‚ÄëP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"X‚ÄëP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":‚Äë√û\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":√û\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":‚Äë/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":‚Äë\\|\":\"Straight face\",\n    u\":\\|\":\"Straight face\",\n    u\":$\":\"Embarrassed or blushing\",\n    u\":‚Äëx\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":‚Äë#\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":‚Äë&\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\"O:‚Äë\\)\":\"Angel, saint or innocent\",\n    u\"O:\\)\":\"Angel, saint or innocent\",\n    u\"0:‚Äë3\":\"Angel, saint or innocent\",\n    u\"0:3\":\"Angel, saint or innocent\",\n    u\"0:‚Äë\\)\":\"Angel, saint or innocent\",\n    u\"0:\\)\":\"Angel, saint or innocent\",\n    u\":‚Äëb\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"0;\\^\\)\":\"Angel, saint or innocent\",\n    u\">:‚Äë\\)\":\"Evil or devilish\",\n    u\">:\\)\":\"Evil or devilish\",\n    u\"\\}:‚Äë\\)\":\"Evil or devilish\",\n    u\"\\}:\\)\":\"Evil or devilish\",\n    u\"3:‚Äë\\)\":\"Evil or devilish\",\n    u\"3:\\)\":\"Evil or devilish\",\n    u\">;\\)\":\"Evil or devilish\",\n    u\"\\|;‚Äë\\)\":\"Cool\",\n    u\"\\|‚ÄëO\":\"Bored\",\n    u\":‚ÄëJ\":\"Tongue-in-cheek\",\n    u\"#‚Äë\\)\":\"Party all night\",\n    u\"%‚Äë\\)\":\"Drunk or confused\",\n    u\"%\\)\":\"Drunk or confused\",\n    u\":-###..\":\"Being sick\",\n    u\":###..\":\"Being sick\",\n    u\"<:‚Äë\\|\":\"Dump\",\n    u\"\\(>_<\\)\":\"Troubled\",\n    u\"\\(>_<\\)>\":\"Troubled\",\n    u\"\\(';'\\)\":\"Baby\",\n    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(~_~;\\) \\(„Éª\\.„Éª;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-\\)zzz\":\"Sleeping\",\n    u\"\\(\\^_-\\)\":\"Wink\",\n    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n    u\"\\(\\+o\\+\\)\":\"Confused\",\n    u\"\\(o\\|o\\)\":\"Ultraman\",\n    u\"\\^_\\^\":\"Joyful\",\n    u\"\\(\\^_\\^\\)/\":\"Joyful\",\n    u\"\\(\\^O\\^\\)Ôºè\":\"Joyful\",\n    u\"\\(\\^o\\^\\)Ôºè\":\"Joyful\",\n    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"\\('_'\\)\":\"Sad or Crying\",\n    u\"\\(/_;\\)\":\"Sad or Crying\",\n    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n    u\"\\(;_;\":\"Sad of Crying\",\n    u\"\\(;_:\\)\":\"Sad or Crying\",\n    u\"\\(;O;\\)\":\"Sad or Crying\",\n    u\"\\(:_;\\)\":\"Sad or Crying\",\n    u\"\\(ToT\\)\":\"Sad or Crying\",\n    u\";_;\":\"Sad or Crying\",\n    u\";-;\":\"Sad or Crying\",\n    u\";n;\":\"Sad or Crying\",\n    u\";;\":\"Sad or Crying\",\n    u\"Q\\.Q\":\"Sad or Crying\",\n    u\"T\\.T\":\"Sad or Crying\",\n    u\"QQ\":\"Sad or Crying\",\n    u\"Q_Q\":\"Sad or Crying\",\n    u\"\\(-\\.-\\)\":\"Shame\",\n    u\"\\(-_-\\)\":\"Shame\",\n    u\"\\(‰∏Ä‰∏Ä\\)\":\"Shame\",\n    u\"\\(Ôºõ‰∏Ä_‰∏Ä\\)\":\"Shame\",\n    u\"\\(=_=\\)\":\"Tired\",\n    u\"\\(=\\^\\¬∑\\^=\\)\":\"cat\",\n    u\"\\(=\\^\\¬∑\\¬∑\\^=\\)\":\"cat\",\n    u\"=_\\^= \":\"cat\",\n    u\"\\(\\.\\.\\)\":\"Looking down\",\n    u\"\\(\\._\\.\\)\":\"Looking down\",\n    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n    u\"\\(\\„Éª\\„Éª?\":\"Confusion\",\n    u\"\\(?_?\\)\":\"Confusion\",\n    u\">\\^_\\^<\":\"Normal Laugh\",\n    u\"<\\^!\\^>\":\"Normal Laugh\",\n    u\"\\^/\\^\":\"Normal Laugh\",\n    u\"\\Ôºà\\*\\^_\\^\\*Ôºâ\" :\"Normal Laugh\",\n    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n    u\"\\(^\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n    u\"\\(\\^‚Äî\\^\\Ôºâ\":\"Normal Laugh\",\n    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n    u\"\\Ôºà\\^‚Äî\\^\\Ôºâ\":\"Waving\",\n    u\"\\(;_;\\)/~~~\":\"Waving\",\n    u\"\\(\\^\\.\\^\\)/~~~\":\"Waving\",\n    u\"\\(-_-\\)/~~~ \\($\\¬∑\\¬∑\\)/~~~\":\"Waving\",\n    u\"\\(T_T\\)/~~~\":\"Waving\",\n    u\"\\(ToT\\)/~~~\":\"Waving\",\n    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n    u\"\\(\\*_\\*\\)\":\"Amazed\",\n    u\"\\(\\*_\\*;\":\"Amazed\",\n    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n    u'\\(-\"-\\)':\"Worried\",\n    u\"\\(„Éº„Éº;\\)\":\"Worried\",\n    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n    u\"\\(\\ÔºæÔΩñ\\Ôºæ\\)\":\"Happy\",\n    u\"\\(\\ÔºæÔΩï\\Ôºæ\\)\":\"Happy\",\n    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n    u\"\\(\\^O\\^\\)\":\"Happy\",\n    u\"\\(\\^o\\^\\)\":\"Happy\",\n    u\"\\)\\^o\\^\\(\":\"Happy\",\n    u\":O o_O\":\"Surprised\",\n    u\"o_0\":\"Surprised\",\n    u\"o\\.O\":\"Surpised\",\n    u\"\\(o\\.o\\)\":\"Surprised\",\n    u\"oO\":\"Surprised\",\n    u\"\\(\\*Ôø£mÔø£\\)\":\"Dissatisfied\",\n    u\"\\(‚ÄòA`\\)\":\"Snubbed or Deflated\"\n}\n\n\n\ndef remove_emoticons(text):\n    emoticons_pattern = re.compile(u'(' + u'|'.join(emo for emo in EMOTICONS) + u')')\n    return emoticons_pattern.sub(r'', text)\n\n\nremove_emoticons(\"Hello :->\")\n\n'Hello '\n\n\n\n# Remove emoticons from text\ndf.text = df.text.apply(lambda x: remove_emoticons(x))\n\n\n\nHashtags and Mentions\nWe are habituated to use hashtags and mentions in our tweet either to indicate the context or bring attention to an individual. Hashtags can be used to extract features, to see what‚Äôs trending and in various other applications.\nSince, we don‚Äôt require them we‚Äôll remove them.\n\ndef remove_tags_mentions(text):\n    pattern = re.compile(r'(@\\S+|#\\S+)')\n    return pattern.sub('', text)\n\n\ntext = \"live @flippinginja on #younow - jonah and jareddddd\"\nremove_tags_mentions(text)\n\n'live  on  - jonah and jareddddd'\n\n\n\n# Remove hashtags and mentions\ndf.text = df.text.apply(lambda x: remove_tags_mentions(x))\n\n\n\nPunctuations\nPunctuations are character other than alphaters and digits. These include [!‚Äú#$%&'()*+,-./:;<=>?@\\^_`{|}~]\nIt is better remove or convert emoticons before removing the punctuations, since if we do the other we around we might loose the emoticons from the text. Another example, if the text contains $10.50 then we‚Äôll remove the .(dot) and the value will loose it‚Äôs meaning.\n\nPUNCTUATIONS = string.punctuation\n\ndef remove_punctuation(text):\n    return text.translate(str.maketrans('', '', PUNCTUATIONS))\n\n\ndf.text = df[\"text\"].apply(lambda text: remove_punctuation(text))\n\n\n\n\n\n  \n    \n      \n      text\n      label\n    \n  \n  \n    \n      0\n      username changed d\n      1\n    \n    \n      1\n      unnieeee\n      1\n    \n    \n      2\n      thanks i hope youve got a good book to keep you company\n      1\n    \n    \n      3\n      where are you situated\n      1\n    \n    \n      4\n      youre welcome im glad you liked it\n      1\n    \n  \n\n\n\n\n\n\nStopwords\nStopwords are commonly occuring words in any language. Such as, in english these words are ‚Äòthe‚Äô, ‚Äòa‚Äô, ‚Äòan‚Äô, & many more. They are in most cases not useful and should be removed.\nThere are certain tasks in which these words are useful such as Part-of-Speech(POS) tagging, language translation. Stopwords are compiled for many languages, for english language we can use the list from the nltk package.\n\nSTOPWORDS = set(stopwords.words('english'))\n\ndef remove_stopwords(text):\n    return ' '.join([word for word in text.split() if word not in STOPWORDS])\n\n\n# Remove stopwords\ndf.text = df.text.apply(lambda text: remove_stopwords(text))\ndf.head()\n\n\n\n\n\n  \n    \n      \n      text\n      label\n    \n  \n  \n    \n      0\n      username changed\n      1\n    \n    \n      1\n      unnieeee\n      1\n    \n    \n      2\n      thanks hope youve got good book keep company\n      1\n    \n    \n      3\n      situated\n      1\n    \n    \n      4\n      youre welcome im glad liked\n      1\n    \n  \n\n\n\n\n\n\nNumbers\nWe may remove numbers if they are not useful in our analysis. But analysis in the financial domain, numbers are very useful.\n\ndf.text = df.text.str.replace(r'\\d+', '', regex=True)\n\n\n\nExtra whitespaces\nAfter usually after preprocessing the text there might be extra whitespaces that might be created after transforming, removing various characters. Also, there is a need to remove all the new line, tab characters as well from our text.\n\ndef remove_whitespaces(text):\n    return \" \".join(text.split())\n\n\ntext = \"  Whitespaces in the beginning are removed  \\t as well \\n  as in between  the text   \"\n\nclean_text = \" \".join(text.split())\nclean_text\n\n'Whitespaces in the beginning are removed as well as in between the text'\n\n\n\ndf.text = df.text.apply(lambda x: remove_whitespaces(x))\n\n\n\nFrequent words\nPreviously we have removed stopwords which are common in any language. If we are working in any domain, we can also remove the common words used in that domain which don‚Äôt provide us with much information.\n\ndef freq_words(text):\n    tokens = word_tokenize(text)\n    FrequentWords = []\n    \n    for word in tokens:\n        counter[word] += 1\n    \n    for (word, word_count) in counter.most_common(10):\n        FrequentWords.append(word)\n    return FrequentWords\n\ndef remove_fw(text, FrequentWords):\n    tokens = word_tokenize(text)\n    without_fw = []\n    for word in tokens:\n        if word not in FrequentWords:\n            without_fw.append(word)\n\n    without_fw = ' '.join(without_fw)\n    return without_fw\n\ncounter = Counter()\n\n\ntext = \"\"\"\nNatural Language Processing is the technology used to aid computers to understand the human‚Äôs natural language. It‚Äôs not an easy task teaching machines to understand how we communicate. Leand Romaf, an experienced software engineer who is passionate at teaching people how artificial intelligence systems work, says that ‚Äúin recent years, there have been significant breakthroughs in empowering computers to understand language just as we do.‚Äù This article will give a simple introduction to Natural Language Processing and how it can be achieved. Natural Language Processing, usually shortened as NLP, is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable. Most NLP techniques rely on machine learning to derive meaning from human languages.\n\"\"\"\n\n\nFrequentWords = freq_words(text)\nprint(FrequentWords)\n\n[',', 'to', '.', 'is', 'the', 'understand', 'Natural', 'Language', 'Processing', 'computers']\n\n\n\nfw_result = remove_fw(text, FrequentWords)\nfw_result\n\n'technology used aid human ‚Äô s natural language It ‚Äô s not an easy task teaching machines how we communicate Leand Romaf an experienced software engineer who passionate at teaching people how artificial intelligence systems work says that ‚Äú in recent years there have been significant breakthroughs in empowering language just as we do. ‚Äù This article will give a simple introduction and how it can be achieved usually shortened as NLP a branch of artificial intelligence that deals with interaction between and humans using natural language The ultimate objective of NLP read decipher and make sense of human languages in a manner that valuable Most NLP techniques rely on machine learning derive meaning from human languages'\n\n\n\n\nRare words\nRare words are similar to frequent words. We can remove them because they are so less that they cannot add any value to the purpose.\n\ndef rare_words(text):\n    # tokenization\n    tokens = word_tokenize(text)\n    for word in tokens:\n        counter[word]= +1\n\n    RareWords = []\n    number_rare_words = 10\n    # take top 10 frequent words\n    frequentWords = counter.most_common()\n    for (word, word_count) in frequentWords[:-number_rare_words:-1]:\n        RareWords.append(word)\n\n    return RareWords\n\ndef remove_rw(text, RareWords):\n    tokens = word_tokenize(text)\n    without_rw = []\n    for word in tokens:\n        if word not in RareWords:\n            without_rw.append(word)\n\n    without_rw = ' '.join(without_fw)\n    return without_rw\n\ncounter = Counter()\n\n\ntext = \"\"\"\nNatural Language Processing is the technology used to aid computers to understand the human‚Äôs natural language. It‚Äôs not an easy task teaching machines to understand how we communicate. Leand Romaf, an experienced software engineer who is passionate at teaching people how artificial intelligence systems work, says that ‚Äúin recent years, there have been significant breakthroughs in empowering computers to understand language just as we do.‚Äù This article will give a simple introduction to Natural Language Processing and how it can be achieved. Natural Language Processing, usually shortened as NLP, is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable. Most NLP techniques rely on machine learning to derive meaning from human languages.\n\"\"\"\n\n\nRareWords = rare_words(text)\nRareWords\n\n['from',\n 'meaning',\n 'derive',\n 'learning',\n 'machine',\n 'on',\n 'rely',\n 'techniques',\n 'Most']\n\n\n\nrw_result = remove_fw(text, RareWords)\nrw_result\n\n'Natural Language Processing is the technology used to aid computers to understand the human ‚Äô s natural language . It ‚Äô s not an easy task teaching machines to understand how we communicate . Leand Romaf , an experienced software engineer who is passionate at teaching people how artificial intelligence systems work , says that ‚Äú in recent years , there have been significant breakthroughs in empowering computers to understand language just as we do. ‚Äù This article will give a simple introduction to Natural Language Processing and how it can be achieved . Natural Language Processing , usually shortened as NLP , is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language . The ultimate objective of NLP is to read , decipher , understand , and make sense of the human languages in a manner that is valuable . NLP to human languages .'"
  },
  {
    "objectID": "posts/2021-04-19/Text-Preprocessing.html#conversion-of-emoji-to-words",
    "href": "posts/2021-04-19/Text-Preprocessing.html#conversion-of-emoji-to-words",
    "title": "Text Preprocessing",
    "section": "Conversion of Emoji to Words",
    "text": "Conversion of Emoji to Words\nTo remove or not is done based on the purpose of the application. Example if we are building a sentiment analysis system emoji‚Äôs can be useful.\n‚ÄúThe movie was üî•‚Äù or ‚ÄúThe movie was üí©‚Äù\nIf we remove the emoji‚Äôs the meaning of the sentence changes completely. In these cases we can convert emoji‚Äôs to words.\ndemoji requires an initial data download from the Unicode Consortium‚Äôs emoji code repository.\nOn first use of the package, call download_codes(). This will store the Unicode hex-notated symbols at ~/.demoji/codes.json for future use.\nRead more about demoji on pypi.org\n\ndemoji.download_codes()\n\nDownloading emoji data ...\n... OK (Got response in 1.35 seconds)\nWriting emoji data to C:\\Users\\sagar\\.demoji\\codes.json ...\n... OK\n\n\n\ndef emoji_to_words(text):\n    return demoji.replace_with_desc(text, sep=\"__\")\n\n\ntext = \"game is on üî• üö£üèº\"\nemoji_to_words(text)\n\n'game is on __fire__ __person rowing boat: medium-light skin tone__'"
  },
  {
    "objectID": "posts/2021-04-19/Text-Preprocessing.html#conversion-of-emoticons-to-words",
    "href": "posts/2021-04-19/Text-Preprocessing.html#conversion-of-emoticons-to-words",
    "title": "Text Preprocessing",
    "section": "Conversion of Emoticons to Words",
    "text": "Conversion of Emoticons to Words\nAs we did for emoji‚Äôs, we convert emoticons to words for the same purpose.\n\ndef emoticons_to_words(text):\n    for emot in EMOTICONS:\n        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n    return text\n\n\ntext = \"Hey there!! :-)\"\nemoticons_to_words(text)\n\n'Hey there!! Happy_face_smiley'"
  },
  {
    "objectID": "posts/2021-04-19/Text-Preprocessing.html#converting-numbers-to-words",
    "href": "posts/2021-04-19/Text-Preprocessing.html#converting-numbers-to-words",
    "title": "Text Preprocessing",
    "section": "Converting Numbers to Words",
    "text": "Converting Numbers to Words\nIf our analysis require us to use information based on the numbers in the text, we can convert them to words.\nRead more about num2words on github\n\ndef nums_to_words(text):\n    new_text = []\n    for word in text.split():\n        if word.isdigit():\n            new_text.append(num2words(word))\n        else:\n            new_text.append(word)\n    return \" \".join(new_text)\n\n\ntext = \"I ran this track 30 times\"\nnums_to_words(text)\n\n'I ran this track thirty times'"
  },
  {
    "objectID": "posts/2021-04-19/Text-Preprocessing.html#chat-words-conversion",
    "href": "posts/2021-04-19/Text-Preprocessing.html#chat-words-conversion",
    "title": "Text Preprocessing",
    "section": "Chat words Conversion",
    "text": "Chat words Conversion\nThe more we use social media, we have become lazy to type the whole phrase or word. Due to which slang words came into existance such as ‚Äúomg‚Äù which represents ‚ÄúOh my god‚Äù. Such slang words don‚Äôt provide much information and if we need to use them we have to convert them.\nThank you: GitHub repo for the list of slang words\n\n\nCode\nchat_words = \"\"\"\nAFAIK=As Far As I Know\nAFK=Away From Keyboard\nASAP=As Soon As Possible\nATK=At The Keyboard\nATM=At The Moment\nA3=Anytime, Anywhere, Anyplace\nBAK=Back At Keyboard\nBBL=Be Back Later\nBBS=Be Back Soon\nBFN=Bye For Now\nB4N=Bye For Now\nBRB=Be Right Back\nBRT=Be Right There\nBTW=By The Way\nB4=Before\nB4N=Bye For Now\nCU=See You\nCUL8R=See You Later\nCYA=See You\nFAQ=Frequently Asked Questions\nFC=Fingers Crossed\nFWIW=For What It's Worth\nFYI=For Your Information\nGAL=Get A Life\nGG=Good Game\nGN=Good Night\nGMTA=Great Minds Think Alike\nGR8=Great!\nG9=Genius\nIC=I See\nICQ=I Seek you (also a chat program)\nILU=ILU: I Love You\nIMHO=In My Honest/Humble Opinion\nIMO=In My Opinion\nIOW=In Other Words\nIRL=In Real Life\nKISS=Keep It Simple, Stupid\nLDR=Long Distance Relationship\nLMAO=Laugh My A.. Off\nLOL=Laughing Out Loud\nLTNS=Long Time No See\nL8R=Later\nMTE=My Thoughts Exactly\nM8=Mate\nNRN=No Reply Necessary\nOIC=Oh I See\nPITA=Pain In The A..\nPRT=Party\nPRW=Parents Are Watching\nQPSA?=Que Pasa?\nROFL=Rolling On The Floor Laughing\nROFLOL=Rolling On The Floor Laughing Out Loud\nROTFLMAO=Rolling On The Floor Laughing My A.. Off\nSK8=Skate\nSTATS=Your sex and age\nASL=Age, Sex, Location\nTHX=Thank You\nTTFN=Ta-Ta For Now!\nTTYL=Talk To You Later\nU=You\nU2=You Too\nU4E=Yours For Ever\nWB=Welcome Back\nWTF=What The F...\nWTG=Way To Go!\nWUF=Where Are You From?\nW8=Wait...\n7K=Sick:-D Laugher\nOMG=Oh my god\"\"\"\n\n\n\nchat_words_dict = dict()\nchat_words_set = set()\n\ndef cw_conversion(text):\n    new_text = []\n    for word in text.split():\n        if word.upper() in chat_words_set:\n            new_text.append(chat_words_dict[word.upper()])\n        else:\n            new_text.append(word)\n    return \" \".join(new_text)\n\nfor line in chat_words.split('\\n'):\n    if line != '':\n        cw, cw_expanded = line.split('=')[0], line.split('=')[1]\n        \n        chat_words_set.add(cw)\n        chat_words_dict[cw] = cw_expanded\n\n\ntext = \"omg that's awesome.\"\ncw_conversion(text)\n\n\"Oh my god that's awesome.\""
  },
  {
    "objectID": "posts/2021-04-19/Text-Preprocessing.html#expanding-contractions",
    "href": "posts/2021-04-19/Text-Preprocessing.html#expanding-contractions",
    "title": "Text Preprocessing",
    "section": "Expanding Contractions",
    "text": "Expanding Contractions\nContractions are words or combinations of words created by dropping a few letters and replacing those letters by an apostrophe.\nExample: - don‚Äôt: do not - we‚Äôll: we will\nOur nlp model don‚Äôt understand these contractions i.e.¬†they don‚Äôt understand that ‚Äúdon‚Äôt‚Äù and ‚Äúdo not‚Äù are the same thing. If our problem statement requires them then we can expand them or else leave it as it is.\n\ndef expand_contractions(text):\n    expanded_text = []\n    for line in text:\n        expanded_text.append(contractions.fix(line))\n    return expanded_text\n\n\ntext = [\"I'll be there within 15 minutes.\", \"It's awesome to meet your new friends.\"]\nexpand_contractions(text)\n\n['I will be there within 15 minutes.',\n 'it is awesome to meet your new friends.']"
  },
  {
    "objectID": "posts/2021-04-19/Text-Preprocessing.html#stemming",
    "href": "posts/2021-04-19/Text-Preprocessing.html#stemming",
    "title": "Text Preprocessing",
    "section": "Stemming",
    "text": "Stemming\nIn stemming we reduce the word to it‚Äôs base or root form by removing the suffix characters from the word. It is one of the technique to normalize text.\nStemming for root word ‚Äúlike‚Äù include: - ‚Äúlikes‚Äù - ‚Äúliked‚Äù - ‚Äúlikely‚Äù - ‚Äúliking‚Äù\nStemmed word doesn‚Äôt always match the words in our dictionary such as: - console -> consol - company -> compani - welcome -> welcom\nDue to which stemming is not performed in all nlp tasks.\nThere are various algorithms used for stemming but the most widely used is PorterStemmer. In this post we have used the PorterStemmer as well.\n\nstemmer = PorterStemmer()\n\ndef stem_words(text):\n    return ' '.join([stemmer.stem(word) for word in text.split()])\n\n\ndf['text_stemmed'] = df.text.apply(lambda text: stem_words(text))\ndf[['text', 'text_stemmed']].head()\n\n\n\n\n\n  \n    \n      \n      text\n      text_stemmed\n    \n  \n  \n    \n      0\n      username changed\n      usernam chang\n    \n    \n      1\n      unnieeee\n      unnieee\n    \n    \n      2\n      thanks hope youve got good book keep company\n      thank hope youv got good book keep compani\n    \n    \n      3\n      situated\n      situat\n    \n    \n      4\n      youre welcome im glad liked\n      your welcom im glad like\n    \n  \n\n\n\n\nPorterStemmer can be used only for english. If we are working with other than english then we can use SnowballStemmer.\n\nSnowballStemmer.languages\n\n('arabic',\n 'danish',\n 'dutch',\n 'english',\n 'finnish',\n 'french',\n 'german',\n 'hungarian',\n 'italian',\n 'norwegian',\n 'porter',\n 'portuguese',\n 'romanian',\n 'russian',\n 'spanish',\n 'swedish')"
  },
  {
    "objectID": "posts/2021-04-19/Text-Preprocessing.html#lemmatization",
    "href": "posts/2021-04-19/Text-Preprocessing.html#lemmatization",
    "title": "Text Preprocessing",
    "section": "Lemmatization",
    "text": "Lemmatization\nLemmatization tried to perform the similar task as that of stemming i.e.¬†trying to reduce the inflection words to it‚Äôs base form. But lemmatization does it by using a different approach.\nLemmatizations takes into consideration of the morphological analysis of the word. It tries to reduce to words to it‚Äôs dictionary form which is known as lemma.\n\nlemmatizer = WordNetLemmatizer()\n\ndef text_lemmatize(text):\n    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n\n\ndf['text_lemmatized'] = df.text.apply(lambda text: text_lemmatize(text))\ndf[['text', 'text_stemmed', 'text_lemmatized']].head()\n\n\n\n\n\n  \n    \n      \n      text\n      text_stemmed\n      text_lemmatized\n    \n  \n  \n    \n      0\n      username changed\n      usernam chang\n      username changed\n    \n    \n      1\n      unnieeee\n      unnieee\n      unnieeee\n    \n    \n      2\n      thanks hope youve got good book keep company\n      thank hope youv got good book keep compani\n      thanks hope youve got good book keep company\n    \n    \n      3\n      situated\n      situat\n      situated\n    \n    \n      4\n      youre welcome im glad liked\n      your welcom im glad like\n      youre welcome im glad liked\n    \n  \n\n\n\n\nDifference between Stemming and Lemmatization:\n\n\n\n\n\n\n\nStemming\nLemmatization\n\n\n\n\nFast compared to lemmatization\nSlow compared to stemming\n\n\nReduces the word to it‚Äôs base form by removing the suffix\nUses lexical knowledge to get the base form of the word\n\n\nDoes not always provide meaning or dictionary form of the original word\nResulting words are always meaningful and dictionary words"
  },
  {
    "objectID": "posts/2021-04-19/Text-Preprocessing.html#spelling-correction",
    "href": "posts/2021-04-19/Text-Preprocessing.html#spelling-correction",
    "title": "Text Preprocessing",
    "section": "Spelling Correction",
    "text": "Spelling Correction\nWe as human always make mistake. Normally incorrect spelling in text are know as typos.\nSince the NLP model doesn‚Äôt know the difference between a correct and an incorrect word. For the model ‚Äúthanks‚Äù and ‚Äúthnks‚Äù are two different words. Therefore, spelling correction is an important step to bring the incorrect words in the correct format.\n\nspell = SpellChecker()\n\ndef correct_spelling(text):\n    correct_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            correct_text.append(spell.correction(word))\n        else:\n            correct_text.append(word)\n    return \" \".join(correct_text)\n\n\ntext = \"Hi, hwo are you doin? I'm good thnks for asking\"\ncorrect_spelling(text)\n\n\"Hi, how are you doing I'm good thanks for asking\"\n\n\n\ntext = \"hw are you doin? I'm god thnks\"\ncorrect_spelling(text)\n\n\"he are you doing I'm god thanks\""
  },
  {
    "objectID": "posts/2021-04-19/Text-Preprocessing.html#convert-accented-characters-to-ascii-characters",
    "href": "posts/2021-04-19/Text-Preprocessing.html#convert-accented-characters-to-ascii-characters",
    "title": "Text Preprocessing",
    "section": "Convert accented characters to ASCII characters",
    "text": "Convert accented characters to ASCII characters\nAccent marks (also referred to as diacritics or diacriticals) usually appear above a character when we press the character for a long time. These need to be remove cause the model cannot distinguish between ‚Äúd√®√®p‚Äù and ‚Äúdeep‚Äù. It will consider them as two different words.\n\ndef accented_to_ascii(text):\n    return unidecode.unidecode(text)\n\n\ntext = \"This is an example text with accented characters like d√®√®p l√®arning √°nd c√∂mputer v√≠s√≠√∂n etc.\"\naccented_to_ascii(text)\n\n'This is an example text with accented characters like deep learning and computer vision etc.'"
  },
  {
    "objectID": "posts/2021-04-19/Text-Preprocessing.html#conclusion",
    "href": "posts/2021-04-19/Text-Preprocessing.html#conclusion",
    "title": "Text Preprocessing",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, most of the text pre-processing techniques are explanied. I‚Äôll update this post as I learn more techniques to pre-process text.\nShare if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "posts/2022-01-31/Song-Popularity-EDA.html",
    "href": "posts/2022-01-31/Song-Popularity-EDA.html",
    "title": "Song Popularity EDA",
    "section": "",
    "text": "This Python notebook is the Python version of Song Popularity EDA - Live Coding Fun by Martin Henze\nPurpose of this notebook is to recreate the plots in python for learning purpose.\nThe recording of the live-coding session can be found on Abhishek Thakur‚Äôs YouTube channel:"
  },
  {
    "objectID": "posts/2022-01-31/Song-Popularity-EDA.html#introduction",
    "href": "posts/2022-01-31/Song-Popularity-EDA.html#introduction",
    "title": "Song Popularity EDA",
    "section": "1. Introduction",
    "text": "1. Introduction\nThe competition is about Song Prediction based on a set of different features. The dataset contains the basic file such as train.csv, test.csv and submission_sample.csv. The dataset used in this competition is in tabular format. The evaluation metric used for this competition is AUC score."
  },
  {
    "objectID": "posts/2022-01-31/Song-Popularity-EDA.html#preparation",
    "href": "posts/2022-01-31/Song-Popularity-EDA.html#preparation",
    "title": "Song Popularity EDA",
    "section": "2. Preparation",
    "text": "2. Preparation\nInitially we‚Äôll load different libraries used in our analysis. Also, load the train and test data.\n\n\nCode\n# Import libraries and load the data\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport random\nimport warnings\n\nfrom plotnine import *\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")\n\n\n\n\nCode\n# Load the data\ntrain = pd.read_csv(\"/kaggle/input/song-popularity-prediction/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/song-popularity-prediction/test.csv\")"
  },
  {
    "objectID": "posts/2022-01-31/Song-Popularity-EDA.html#overview-structure-and-data-content",
    "href": "posts/2022-01-31/Song-Popularity-EDA.html#overview-structure-and-data-content",
    "title": "Song Popularity EDA",
    "section": "3. Overview: structure and data content",
    "text": "3. Overview: structure and data content\nThe first step we‚Äôll do is look at the raw data. This tell us about the different features in the dataset, missing values, and types of features (numeric, string, categorical, etc.).\n\n3.1. Look at the data\nLet‚Äôs look at the basic structure of the data\n\n\nCode\nprint('\\nInformation about Data')\ndisplay(train.info())\n\n\n\nInformation about Data\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 40000 entries, 0 to 39999\nData columns (total 15 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   id                40000 non-null  int64  \n 1   song_duration_ms  35899 non-null  float64\n 2   acousticness      36008 non-null  float64\n 3   danceability      35974 non-null  float64\n 4   energy            36025 non-null  float64\n 5   instrumentalness  36015 non-null  float64\n 6   key               35935 non-null  float64\n 7   liveness          35914 non-null  float64\n 8   loudness          36043 non-null  float64\n 9   audio_mode        40000 non-null  int64  \n 10  speechiness       40000 non-null  float64\n 11  tempo             40000 non-null  float64\n 12  time_signature    40000 non-null  int64  \n 13  audio_valence     40000 non-null  float64\n 14  song_popularity   40000 non-null  int64  \ndtypes: float64(11), int64(4)\nmemory usage: 4.6 MB\n\n\nNone\n\n\nWe find:\n\nThere are 40000 entries and 15 features in total.\nAll the column data type is either int or float i.e.¬†all the columns are numeric. This make is comparatively easier to work with compared to columns contains string type data.\nWe can also observe there are columns that contain less than 40K Non-Null values which indicates missing values in the dataset.\n\nLet‚Äôs now look at the top 20 rows of the data.\n\n\nCode\n\"\"\"Display top 20 rows of the train data\"\"\"\ndisplay(train.head(20).style.set_caption(\"First Twenty rows of Training Data\"))\n\n\n\n\n\n  First Twenty rows of Training Data\n  \n    \n      ¬†\n      id\n      song_duration_ms\n      acousticness\n      danceability\n      energy\n      instrumentalness\n      key\n      liveness\n      loudness\n      audio_mode\n      speechiness\n      tempo\n      time_signature\n      audio_valence\n      song_popularity\n    \n  \n  \n    \n      0\n      0\n      212990.000000\n      0.642286\n      0.856520\n      0.707073\n      0.002001\n      10.000000\n      nan\n      -5.619088\n      0\n      0.082570\n      158.386236\n      4\n      0.734642\n      0\n    \n    \n      1\n      1\n      nan\n      0.054866\n      0.733289\n      0.835545\n      0.000996\n      8.000000\n      0.436428\n      -5.236965\n      1\n      0.127358\n      102.752988\n      3\n      0.711531\n      1\n    \n    \n      2\n      2\n      193213.000000\n      nan\n      0.188387\n      0.783524\n      -0.002694\n      5.000000\n      0.170499\n      -4.951759\n      0\n      0.052282\n      178.685791\n      3\n      0.425536\n      0\n    \n    \n      3\n      3\n      249893.000000\n      0.488660\n      0.585234\n      0.552685\n      0.000608\n      0.000000\n      0.094805\n      -7.893694\n      0\n      0.035618\n      128.715630\n      3\n      0.453597\n      0\n    \n    \n      4\n      4\n      165969.000000\n      0.493017\n      nan\n      0.740982\n      0.002033\n      10.000000\n      0.094891\n      -2.684095\n      0\n      0.050746\n      121.928157\n      4\n      0.741311\n      0\n    \n    \n      5\n      5\n      188891.000000\n      0.035655\n      0.825919\n      0.804528\n      -0.000005\n      4.000000\n      0.120758\n      -6.122926\n      0\n      0.039012\n      115.679128\n      4\n      0.709408\n      0\n    \n    \n      6\n      6\n      161061.000000\n      0.081743\n      0.673588\n      0.880181\n      0.000327\n      0.000000\n      0.535411\n      -2.909607\n      1\n      0.030902\n      98.046205\n      4\n      0.982729\n      0\n    \n    \n      7\n      7\n      196202.000000\n      0.259747\n      0.813214\n      0.554385\n      0.000390\n      8.000000\n      0.276580\n      -7.794237\n      0\n      0.207067\n      158.626764\n      3\n      0.662987\n      1\n    \n    \n      8\n      8\n      169660.000000\n      nan\n      0.653263\n      0.917034\n      0.001748\n      0.000000\n      nan\n      -4.422089\n      0\n      0.031608\n      122.382398\n      3\n      0.297683\n      1\n    \n    \n      9\n      9\n      167245.000000\n      0.019617\n      0.595235\n      0.820039\n      0.761884\n      5.000000\n      0.181098\n      -5.154293\n      0\n      0.054493\n      110.524824\n      4\n      0.535453\n      0\n    \n    \n      10\n      10\n      128274.000000\n      0.614007\n      0.397899\n      0.346820\n      0.002853\n      3.000000\n      0.132549\n      nan\n      1\n      0.059512\n      87.363516\n      3\n      0.671581\n      1\n    \n    \n      11\n      11\n      213121.000000\n      0.044053\n      0.817874\n      0.729679\n      0.003660\n      5.000000\n      0.137938\n      -4.880149\n      0\n      0.038814\n      124.199541\n      4\n      0.816472\n      1\n    \n    \n      12\n      12\n      219730.000000\n      0.339275\n      0.660707\n      nan\n      nan\n      0.000000\n      0.223173\n      -12.005655\n      0\n      0.089726\n      164.877811\n      3\n      0.322253\n      1\n    \n    \n      13\n      13\n      nan\n      0.455778\n      0.448538\n      0.754924\n      nan\n      nan\n      0.076379\n      -3.158905\n      0\n      0.034837\n      118.664526\n      3\n      0.862989\n      0\n    \n    \n      14\n      14\n      nan\n      0.462876\n      0.384318\n      0.653525\n      0.781326\n      6.000000\n      nan\n      -10.362441\n      0\n      0.065149\n      141.581118\n      3\n      0.432883\n      0\n    \n    \n      15\n      15\n      nan\n      0.059284\n      0.164167\n      0.877743\n      0.002113\n      8.000000\n      0.227997\n      -3.627678\n      0\n      0.246330\n      174.445180\n      4\n      0.530006\n      1\n    \n    \n      16\n      16\n      248851.000000\n      0.097600\n      0.718901\n      0.618376\n      0.002925\n      4.000000\n      0.075377\n      -7.715512\n      1\n      0.083494\n      96.831665\n      4\n      0.935569\n      0\n    \n    \n      17\n      17\n      153340.000000\n      0.012866\n      0.715635\n      0.796742\n      0.002236\n      6.000000\n      0.101808\n      -4.879090\n      0\n      0.172036\n      120.830046\n      3\n      0.497743\n      0\n    \n    \n      18\n      18\n      170983.000000\n      0.123631\n      0.524386\n      0.566983\n      nan\n      5.000000\n      nan\n      -7.312097\n      0\n      0.210055\n      114.609197\n      4\n      0.951705\n      1\n    \n    \n      19\n      19\n      266726.000000\n      0.021030\n      0.323277\n      nan\n      -0.000861\n      nan\n      0.239698\n      -12.692935\n      1\n      0.031522\n      124.811208\n      4\n      0.350090\n      1\n    \n  \n\n\n\nWe find:\n\nThere are missing values that can be seen as nan in the table above\nThe id column seems to have values in increasing order\nThe values in the features are in different scales\n\nNow, let‚Äôs look at some basic statistics about our features in the data\n\n\nCode\ndisplay(train.describe().style.set_caption(\"Basic statistics about Train Data\"))\n\n\n\n\n\n  Basic statistics about Train Data\n  \n    \n      ¬†\n      id\n      song_duration_ms\n      acousticness\n      danceability\n      energy\n      instrumentalness\n      key\n      liveness\n      loudness\n      audio_mode\n      speechiness\n      tempo\n      time_signature\n      audio_valence\n      song_popularity\n    \n  \n  \n    \n      count\n      40000.000000\n      35899.000000\n      36008.000000\n      35974.000000\n      36025.000000\n      36015.000000\n      35935.000000\n      35914.000000\n      36043.000000\n      40000.000000\n      40000.000000\n      40000.000000\n      40000.000000\n      40000.000000\n      40000.000000\n    \n    \n      mean\n      19999.500000\n      193165.847572\n      0.276404\n      0.570951\n      0.683932\n      0.036527\n      5.042605\n      0.198514\n      -7.407596\n      0.321150\n      0.094107\n      116.562815\n      3.394375\n      0.580645\n      0.364400\n    \n    \n      std\n      11547.149720\n      45822.127679\n      0.297928\n      0.190010\n      0.212662\n      0.150024\n      3.372728\n      0.151670\n      3.877198\n      0.466924\n      0.083591\n      26.167911\n      0.524405\n      0.237351\n      0.481268\n    \n    \n      min\n      0.000000\n      25658.000000\n      -0.013551\n      0.043961\n      -0.001682\n      -0.004398\n      0.000000\n      0.027843\n      -32.117911\n      0.000000\n      0.015065\n      62.055779\n      2.000000\n      0.013398\n      0.000000\n    \n    \n      25%\n      9999.750000\n      166254.500000\n      0.039618\n      0.424760\n      0.539276\n      0.000941\n      2.000000\n      0.111796\n      -9.578139\n      0.000000\n      0.038500\n      96.995309\n      3.000000\n      0.398669\n      0.000000\n    \n    \n      50%\n      19999.500000\n      186660.000000\n      0.140532\n      0.608234\n      0.704453\n      0.001974\n      5.000000\n      0.135945\n      -6.345413\n      0.000000\n      0.055881\n      113.795959\n      3.000000\n      0.598827\n      0.000000\n    \n    \n      75%\n      29999.250000\n      215116.000000\n      0.482499\n      0.718464\n      0.870503\n      0.003225\n      8.000000\n      0.212842\n      -4.620711\n      1.000000\n      0.118842\n      128.517383\n      4.000000\n      0.759635\n      1.000000\n    \n    \n      max\n      39999.000000\n      491671.000000\n      1.065284\n      0.957131\n      1.039741\n      1.075415\n      11.000000\n      1.065298\n      -0.877346\n      1.000000\n      0.560748\n      219.163578\n      5.000000\n      1.022558\n      1.000000\n    \n  \n\n\n\nWe find:\n\nMost of the features are in the range of 0 and 1\nThere are features with only negative values (loudness), binary features (audio_mode) , and seems to be categorical (key and time_signature)\n\n\n\n3.2. Missing data\nNow let‚Äôs take a closer look at the missing values in the dataset\n\n\nCode\n\"\"\"Missing Values\"\"\"\nprint(f\"Train set has {train.isnull().sum().sum()} missing values, and test set has {test.isnull().sum().sum()} missing values\")\n\n\nTrain set has 32187 missing values, and test set has 7962 missing values\n\n\n\n\nCode\n# Refrence (edited): https://datavizpyr.com/visualizing-missing-data-with-seaborn-heatmap-and-displot/\nfig = plt.figure(figsize=(18,6))\n\nsns.displot(\n    data=train.isna().melt(value_name=\"missing\"),\n    y=\"variable\",\n    hue=\"missing\",\n    multiple=\"fill\",\n    aspect=3\n)\nplt.title(\"Missing values shown using Bar plot\", fontsize=17)\nplt.ylabel(\"\")\nplt.xlabel(\"\")\n\nplt.figure(figsize=(18,10))\nsns.heatmap(train.isna().transpose())\nplt.title('Heatmap showing Missing Values in Train data', fontsize=17)\nplt.ylabel(\"\")\nplt.xlabel(\"\")\nplt.show()\n\n\n<Figure size 1296x432 with 0 Axes>\n\n\n\n\n\n\n\n\n\n\nCode\ntrain_null = train.isna().sum().sort_values(ascending = False)\ntest_null = test.isna().sum().sort_values(ascending = False)\n\nnon_zero_train_values = train_null[train_null.values > 0]\nnon_zero_test_values = test_null[test_null.values > 0]\n\nfig, axes = plt.subplots(1,2, figsize=(15,8))\nsns.barplot(y=non_zero_test_values.index , x=non_zero_test_values.values, ax=axes[1], palette = \"viridis\")\nsns.barplot(y=non_zero_train_values.index , x=non_zero_train_values.values, ax=axes[0], palette = \"viridis\")\naxes[0].set_title(\"Train data\", fontsize=14)\naxes[1].set_title(\"Test data\", fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/2022-01-31/Song-Popularity-EDA.html#visualization---individual-features",
    "href": "posts/2022-01-31/Song-Popularity-EDA.html#visualization---individual-features",
    "title": "Song Popularity EDA",
    "section": "4. Visualization - Individual Features",
    "text": "4. Visualization - Individual Features\nAfter getting an initial idea about our features and their values, we can now dive into the visual part of the exploration. I recommend to always plot your data. Sometimes this might be challenging, e.g.¬†because you have tons of features. In that case, you want to start at least with a subset before you run any dimensionality reduction or other tools. This step is as much about spotting issues and irregularities as it is about learning more about the shapes and distributions of your features.\n\n4.1. Predictor variables\n\nIn the live session, we were building this plot step by step. (Well, we got most of the way there.) It really pays off to take the time and investigate each feature separately. This is one of the most instructive steps in the EDA process, where you aim to learn how messed up your features are. No dataset is perfect. We want to figure out how severe those imperfections are, and whether we can live with them or have to address them.\nDifferent kind of data types go best with different kind of visuals. My recommendation is to start out with density plots or histograms for numerical features, and with barcharts for those that are better expressed as types of categories.\n\n\n\nCode\nuseful_cols = [col for col in train.columns if col not in [\"id\", \"song_popularity\"]]\nnumeric_cols = [col for col in useful_cols if col not in [\"key\", \"audio_mode\", \"time_signature\"]]\n\nn_rows = 5\nn_cols = 3\nindex = 1\n\ncolors = [\"red\", \"darkblue\", \"green\"]\n\nfig = plt.figure(figsize=(16,20))\n\nfor index, col in enumerate(train[useful_cols].columns):\n    plt.subplot(n_rows,n_cols,index+1)\n    \n    if col in numeric_cols:\n        sns.kdeplot(train[col], color=random.sample(colors, 1), fill=True)\n        plt.title(col, fontsize=14)\n        plt.xlabel(\"\")\n        plt.ylabel(\"\")\n        plt.tight_layout()\n    else:\n        sns.countplot(train[col])\n        plt.title(col, fontsize=14)\n        plt.xlabel(\"\")\n        plt.ylabel(\"\")\n        plt.tight_layout()\n\nplt.subplot(n_rows,n_cols,14)\nsns.kdeplot(np.log(train['instrumentalness']), color=random.sample(colors, 1), fill=True)\nplt.title('instrumentalness (log transformed)', fontsize=14)\nplt.ylabel(\" \")\nplt.xlabel(\" \")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nWe find:\n\nOur initial impressions of the data types have largely been confirmed: audio_mode is a boolean feature, and time_signature and key are ordinal or categorical ones (or integer; although a better understanding of those musical concepts would certainly benefit from some domain knowledge.)\nA number of features are bounded between 0 and 1: accosticness, danceability, energy, liveliness, speechiness, and audio_valence.\nThe feature loudness looks like it refer to the decibel scale.\nThe distribution of instrumentalness is heavily right-skewed, and even after a log transform this feature doesn‚Äôt look very well-behaved. This might need a bit more work.\n\n\n\n4.2. Target: Song Popularity\nOn to the target itself. We figured out that song_popularity is a binary feature, and thus we can express it as boolean. Here we plot a barchart.\n\n\nCode\nsns.countplot(train.song_popularity.astype(\"bool\"))\nplt.title(\"Target: Song Popularity\", fontsize=14)\nplt.xlabel(\"\")\nplt.ylabel(\"\")\nplt.show()\n\n\n\n\n\nWe find:\n\nThere is a slight imbalance in the target distribution: a bit more than 60/40. Not super imbalanced, but something to keep in mind."
  },
  {
    "objectID": "posts/2022-01-31/Song-Popularity-EDA.html#feature-interactions",
    "href": "posts/2022-01-31/Song-Popularity-EDA.html#feature-interactions",
    "title": "Song Popularity EDA",
    "section": "5. Feature interactions",
    "text": "5. Feature interactions\nAfter learning more about each individual feature, we now want to see them interacting with one another. It‚Äôs best to perfom those steps in that order, so that you can understand and interpret the interactions in the context of the overall distributions.\n\n5.1. Target impact\nWe have seen all the feature distributions, now we want to investigate whether they look different based on the target value. Here‚Äôs an example for song_duration:\n\n\nCode\nfig = plt.figure(figsize=(16,18))\nn_rows = 4\nn_cols = 3\n\nfor index, col in enumerate(numeric_cols):\n    plt.subplot(n_rows, n_cols, index+1)\n    \n    sns.kdeplot(train[col], hue=train.song_popularity.astype(\"bool\"), fill=True)\n    plt.title(col, fontsize=14)\n    plt.xlabel(\"\")\n    plt.ylabel(\"\")\n    plt.tight_layout()\nplt.show()\n\n\n\n\n\nObservations:\n\nBy looking at the probability distribution of different variables we find that popular songs are almost exactly the same length as unpopular ones. There is a slight difference, but it‚Äôs pretty small.\n\nNow we can check the categorical features.\n\n\nCode\nfig = plt.figure(figsize=(18,5))\n\nfor index, col in enumerate([\"key\", \"audio_mode\", \"time_signature\"]):\n    plt.subplot(1,3,index+1)\n    \n    sns.countplot(train[col], hue=train.song_popularity.astype(\"bool\"))\n    plt.title(col, fontsize=14)\n    plt.xlabel(\"\")\n    plt.ylabel(\"\")\n    plt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n5.2. Feature Interaction\nHow do the predictor features interact with each other? Are there any redundancies or strong relationships? We will start out with a correlation matrix, and then look at features of interest in a bit more detail.\n\n5.2.1. Correlations overview\n\n\nCode\n# Refrence (edited): https://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec\ndef heatmap(data):\n    corr = pd.melt(data.reset_index(), id_vars='index') # Unpivot the dataframe, so we can get pair of arrays for x and y\n    corr.columns = ['x', 'y', 'value']\n    x=corr['x']\n    y=corr['y']\n    size=corr['value'].abs()\n    color=corr['value']\n    \n    fig, ax = plt.subplots(figsize=(10,10))\n    plot_grid = plt.GridSpec(1, 15, hspace=0.2, wspace=0.1) # Setup a 1x15 grid\n    ax = plt.subplot(plot_grid[:,:-1]) # Use the leftmost 14 columns of the grid for the main plot\n    \n    n_colors = 256 # Use 256 colors for the diverging color palette\n    palette = sns.diverging_palette(20, 220, n=n_colors) # Create the palette\n    color_min, color_max = [-1, 1] # Range of values that will be mapped to the palette, i.e. min and max possible correlation\n    size_min, size_max = 0, 1\n    \n    def value_to_color(val):\n        val_position = float((val - color_min)) / (color_max - color_min) # position of value in the input range, relative to the length of the input range\n        val_position = min(max(val_position, 0), 1) # bound the position betwen 0 and 1\n        ind = int(val_position * (n_colors - 1)) # target index in the color palette\n        return palette[ind]\n    \n    def value_to_size(val):\n        val_position = (val - size_min) * 0.99 / (size_max - size_min) + 0.01 # position of value in the input range, relative to the length of the input range\n        val_position = min(max(val_position, 0), 1) # bound the position betwen 0 and 1\n        return val_position * size_scale\n        \n    # Mapping from column names to integer coordinates\n    x_labels = [v for v in sorted(x.unique())]\n    y_labels = [v for v in sorted(y.unique())]\n    x_to_num = {p[1]:p[0] for p in enumerate(x_labels)} \n    y_to_num = {p[1]:p[0] for p in enumerate(y_labels)} \n    \n    size_scale = 500\n    ax.scatter(\n        x=x.map(x_to_num), # Use mapping for x\n        y=y.map(y_to_num), # Use mapping for y\n        s=size.apply(value_to_size), # Vector of square sizes, proportional to size parameter\n        c=color.apply(value_to_color), # Vector of square color values, mapped to color palette\n        marker='s' # Use square as scatterplot marker\n    )\n    \n    # Show column labels on the axes\n    ax.set_xticks([x_to_num[v] for v in x_labels])\n    ax.set_xticklabels(x_labels, rotation=45, horizontalalignment='right')\n    ax.set_yticks([y_to_num[v] for v in y_labels])\n    ax.set_yticklabels(y_labels)\n    \n    ax.grid(False, 'major')\n    ax.grid(True, 'minor')\n    ax.set_xticks([t + 0.5 for t in ax.get_xticks()], minor=True)\n    ax.set_yticks([t + 0.5 for t in ax.get_yticks()], minor=True)\n        \n    ax.set_xlim([-0.5, max([v for v in x_to_num.values()]) + 0.5])\n    ax.set_ylim([-0.5, max([v for v in y_to_num.values()]) + 0.5])\n    ax.set_facecolor('#F1F1F1')\n    \n    # Add color legend on the right side of the plot\n    ax = plt.subplot(plot_grid[:,-1]) # Use the rightmost column of the plot\n\n    col_x = [0]*len(palette) # Fixed x coordinate for the bars\n    bar_y=np.linspace(color_min, color_max, n_colors) # y coordinates for each of the n_colors bars\n\n    bar_height = bar_y[1] - bar_y[0]\n    ax.barh(\n        y=bar_y,\n        width=[5]*len(palette), # Make bars 5 units wide\n        left=col_x, # Make bars start at 0\n        height=bar_height,\n        color=palette,\n        linewidth=0\n    )\n    ax.set_xlim(1, 2) # Bars are going from 0 to 5, so lets crop the plot somewhere in the middle\n    ax.grid(False) # Hide grid\n    ax.set_facecolor('white') # Make background white\n    ax.set_xticks([]) # Remove horizontal ticks\n    ax.set_yticks(np.linspace(min(bar_y), max(bar_y), 3)) # Show vertical ticks for min, middle and max\n    ax.yaxis.tick_right() # Show vertical ticks on the right\n    \n\n\n\n\nCode\nheatmap(train[numeric_cols].corr())\n\n\n\n\n\nBelow is a similar correlation heatmap but only using the lower triangle to show the correlation.\n\n\nCode\n# Refrence (edited): https://medium.com/@szabo.bibor/how-to-create-a-seaborn-correlation-heatmap-in-python-834c0686b88e\n\nfig = plt.figure(figsize=(10,10))\nmatrix = np.triu(np.ones_like(train[numeric_cols].corr(), dtype=np.bool))\nsns.heatmap(train[numeric_cols].corr(), mask=matrix, vmin=-1, vmax=1, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))\nplt.show()\n\n\n\n\n\nWe find:\n\nThere‚Äôs a strong anti-correlation between acousticness vs energy and loudness, respectively. Consequently, energy and loudness share a strong correlation.\nNone of the features individually show a notable correlation with the target song_popularity.\n\n\n\n5.2.2. Categorical feature interactions\nWhenever we‚Äôre looking at categorical features, we can assign a visualisation dimension like colour, size, or facets to those. We will start modifying our trusted density plots to look at the distributions of energy (potentially one of the more interesting numerical features) for the different values of time_signature (here encoded as colour):\n\n\nCode\nfig = plt.figure(figsize=(10,8))\nsns.kdeplot(x=\"energy\", hue=\"time_signature\", data=train, fill=True, bw=0.03)\nplt.show()\n\n\n\n\n\n\n\nCode\n(ggplot(train, aes(\"key\", \"time_signature\", fill = \"energy\")) \n + geom_tile()\n + theme(figure_size=(16,5))\n + scale_x_continuous(breaks=range(0,12)))\n\n\n\n\n\n<ggplot: (8777952485729)>\n\n\nWe find:\n\nFor time_signatures 2 and 5 we have no instances of key == 11. This is no big surprise, since those three values are already rare individually, which makes their combinations even more rare.\nThere are no clear clusters of high vs low energy features here.\nWe can see certain combinations that are particularly low energy, such as key == 2 and time_signature == 1 or 8. key == 3 and time_signature == 1 seems to be a particularly energetic combination.\n\n\n\n\n5.3. Feature Target Interaction\nOnce we have found interesting correlations we can look for clustering in the target variable.\n\n\nCode\n(ggplot(train, aes('key', 'time_signature')) \n + geom_tile(aes(fill='energy')) \n + facet_wrap(\"song_popularity\", nrow = 2) \n + theme_minimal() \n + theme(figure_size=(16, 8))\n + scale_x_continuous(breaks=range(0,12)))\n\n\n\n\n\n<ggplot: (8777950648995)>\n\n\n\n\nCode\nsns.displot(data=train, x=\"energy\", y=\"audio_valence\", col=\"song_popularity\", kind=\"kde\", fill=True, legend=True, height=8, aspect=0.75)\nplt.show()\n\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize=(12,6))\nsns.scatterplot(x=\"energy\", y=\"acousticness\", hue=\"song_popularity\", data=train)\nplt.show()\n\n\n\n\n\n\n\nMore Resources:\n\nChoosing different color palette in Seaborn\nSee also\n\nPairGrid: Subplot grid for plotting pairwise relationships\nrelplot: Combine a relational plot and a FacetGrid\ndisplot: Combine a distribution plot and a FacetGrid\ncatplot: Combine a categorical plot and a FacetGrid\nlmplot: Combine a regression plot and a FacetGrid\n\n\nSpecial Thanks to Martin Henze for sharing his knowledge during the live coding session. Also, thank you Abhishek Thakur for hosting these wonderful sessions for people to learn. I look forward to learn more.\nShare if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "posts/2021-04-12/What_is_NLP.html",
    "href": "posts/2021-04-12/What_is_NLP.html",
    "title": "What is Natural Language Processing?",
    "section": "",
    "text": "Natural language processing is a branch of Artificial Intelligence which aims to bridge the gap between how a computer and human communicate with each other. The two major handles used for communication are speech and written i.e.¬†text."
  },
  {
    "objectID": "posts/2021-04-12/What_is_NLP.html#history-of-natural-language-processing",
    "href": "posts/2021-04-12/What_is_NLP.html#history-of-natural-language-processing",
    "title": "What is Natural Language Processing?",
    "section": "History of Natural Language Processing",
    "text": "History of Natural Language Processing\nThe dawn of NLP can be dated back to the early 1900s. In 1950, Alan Turing published his famous article ‚ÄúComputing Machinery and Intelligence‚Äù which proposed what is now called the Turing test as a criterion of intelligence. It tests the ability of the computer program to impersonate a human in a real-time conversation with a human judge where the judge is unable to distinguish the human from the computer program. In 1954, the Georgetown experiment automatically translated more than sixty Russian words into English.\nIn 1957, Noam Chomsky‚Äôs Syntactic Structures a rule-based system of syntactic structures with ‚Äúuniversal grammar‚Äù was an incredible advancement. Up to the 1980‚Äôs most of the NLP systems were based on complex hand-written rules but in the late 1980s by the introduction of machine learning algorithms for language processing revolutionized the field. A steady increase in computational power resulting from Moore‚Äôs law and use of statistical models that use probabilistic measures to map the features making up the input data. Watson an artificial intelligence software designed as a question answering system won the Jeopardy contest, defeating the best human players in February 2011.\nDevelopment of famous virtual assistants like Siri in 2011, Amazon Alexa in 2014, and Google Assistant in 2016. The use of deep learning produced better results than the state-of-the-art in many natural language processing tasks, such as machine translation, text classification, and many more. Recent advancements include the use of network architecture of the transformer which is based on the attention mechanism that has produced better results in various NLP tasks.\nWe humans in our daily life overlook the powerful ability of our human brain to read, understand the meaning of a word, it‚Äôs context (how does it relate to each other), understand humor, sarcasm, and thousand other things. How do we teach this to a computer?"
  },
  {
    "objectID": "posts/2021-04-12/What_is_NLP.html#challenges",
    "href": "posts/2021-04-12/What_is_NLP.html#challenges",
    "title": "What is Natural Language Processing?",
    "section": "Challenges",
    "text": "Challenges\n1. Ambiguity:  In a natural language, words are unique but their meaning may differ based on the context in which it is used. One classical example used is: - The bank is a financial institution where customers can save or borrow money. - Tom was sitting by the banks of the river.\nIn this example, we can see that the word ‚Äúbank‚Äù is used in two different ways. The word is the same but the meaning is different. This is because the context in which the word is used is different.\n2. Co-Referencing: It is a process to find all the phrases in the document that refer to the same entity. Example: Harry kept the paneer on the plate and ate it. Here it refers to the paneer that he ate which was kept on the plate.\n3. Information Extraction: Identifying phrases in a language that refer to specific types of entities and relations in text. Named Entity Recognition (NER) is the task used to identify the names of people, organizations, places, etc, in a text. Example: Tom used to work at FedEx and lives in Mumbai, India. where Person = Tom, organization = FedEx and Place = Mumbai, India\n4. Personality, intention, emotions, and style: Different authors may have different personalities, intentions, emotions, and styles to convey the same idea. Based on these factors the underlying idea can be interpreted in different ways. Use of humor or sarcasm may convey a meaning that is opposite of the literal one."
  },
  {
    "objectID": "posts/2021-04-12/What_is_NLP.html#applications",
    "href": "posts/2021-04-12/What_is_NLP.html#applications",
    "title": "What is Natural Language Processing?",
    "section": "Applications",
    "text": "Applications\n1. Machine Translation:  The idea behind machine translation is to develop a system that is capable of translating text from one language to another without any human intervention. Only translating the text from one language to another is not the key. Understanding the meaning behind the text and translating it to some other language is the crux of it. Example: Google Translate\n2. Automatic summarization:  We all love to read storybooks and always a good storybook will have a summary at the end that highlights the important things about the story. Likewise take any piece of text, a story, a news article, etc, and develop a system that can automatically summary the piece of text. Example: Inshorts ‚Äì an app that summarizes each news article in 60 words.\n3. Sentiment Analysis:  It deals with the study of extracting opinions and sentiment that are not always explicitly expressed. For instance it helps the company to understand the level of consumer satisfaction for its goods and services. Example: ‚ÄúI love the new iPhone and it has a great camera.‚Äù.\nAnother branch of sentiment analysis is ‚ÄúAspect based Sentiment Analysis‚Äù where it tries to extract opinions for each aspect from the customer review. Example: ‚ÄúThe new iPhone is great it has a great camera but the battery life is not that good.‚Äù Here the customer is satisfied with the camera aspect of the iPhone but not the battery life.\n4. Text Classification:  Organizing text documents into predefined categories enables to classify the information or any activity. Examples: Classifying an email as spam or not spam.\n5. Question Answering:  Question answering deals with a system that can answer questions posed by humans in natural language. Sounds simple yet building the knowledge base, understanding the text, and to answer in natural language is altogether a thing in itself.\n6. Chatbots:  Chatbots are a piece of software that can simulate a conversation (or chat) with a user in natural language through websites, apps, messaging applications, etc. Chatbots are a natural evolution of question answering system but are one step further with their ability to understand the text and engage in a conversation.\n7. Speech Recognition:  Using our voice to interact with our phones has become a common phenomenon. For example to ask questions to our voice assistants like Google Assistant/Siri/Cortana, use of voice to type a piece of text. Recognizing speech has replaced the method by which we interact with our devices and made it so convenient.\nRecent advancements in NLP have deepened our knowledge on how to tackle the various challenges in NLP. Also, this new decade will be filled with excitement and breakthroughs that awaits us. Stay tunned to deep dive into the world of NLP.\nShare if you like it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sagar Thacker",
    "section": "",
    "text": "Blog Series:\n\nNatural Language Processing\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nSong Popularity EDA\n\n\n\n\n\n\n\nKaggle\n\n\n\n\nSong Popularity Prediction is a competition on Kaggle. This post is all about performing exploratory data analysis following the coding session by Martin Henze.\n\n\n\n\n\n\nMay 31, 2021\n\n\nSagar Thacker\n\n\n\n\n\n\n  \n\n\n\n\nText Preprocessing\n\n\n\n\n\n\n\nText Preprocessing\n\n\nNatural Language Processing\n\n\nNLP\n\n\n\n\nVairous preprocessing steps required to clean and prepare your data in NLP\n\n\n\n\n\n\nApr 19, 2021\n\n\nSagar Thacker\n\n\n\n\n\n\n  \n\n\n\n\nWhat is Natural Language Processing?\n\n\n\n\n\n\n\nNatural Language Processing\n\n\nNLP\n\n\n\n\nA brief summary of what is natural language processing, it‚Äôs challenges and applications.\n\n\n\n\n\n\nApr 12, 2021\n\n\nSagar Thacker\n\n\n\n\n\n\n  \n\n\n\n\nPath to become a Machine Learning Expert\n\n\n\n\n\n\n\nMachine Learning\n\n\nLearning Path\n\n\n\n\nComprehensive learning path to become an expert in Machine Learning\n\n\n\n\n\n\nApr 5, 2021\n\n\nSagar Thacker\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Hello  I‚Äôm Sagar Thacker A Software & Machine Learning Engineer",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     Github\n  \n  \n    \n     Email\n  \n  \n    \n     Portfolio\n  \n  \n    \n     Resume\nI am a geek and an adventurer who is passionate about solving pressing challenges across computational linguistics, healthcare, and information access. Proficient in developing machine learning and statistical models for identifying patterns and extracting valuable insights."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Hello  I‚Äôm Sagar Thacker A Software & Machine Learning Engineer",
    "section": "Education",
    "text": "Education\nUniversity at Buffalo | Buffalo, NY Masters in Computer Science | Aug 2021 - Feb 2023\nUniversity of Mumbai | Mumbai, MH Bachelors of Engineering in Computer Science | Aug 2014 - June 2018"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Hello  I‚Äôm Sagar Thacker A Software & Machine Learning Engineer",
    "section": "Experience",
    "text": "Experience\nManpower Group Services (Client: FedEx) | Data Analyst | Jan 2020 - Apr 2021\nTata Consultancy Services | Assistant System Engineer | Jul 2018 - Nov 2019"
  },
  {
    "objectID": "nlp_series.html",
    "href": "nlp_series.html",
    "title": "Natural Language Processing",
    "section": "",
    "text": "What is Natural Language Processing\nText Preprocessing"
  }
]