[
  {
    "objectID": "kaggle.html",
    "href": "kaggle.html",
    "title": "Kaggle Competitions",
    "section": "",
    "text": "Playground Series Season 3, Episode 4\n\nExploratory Data Analysis\nEnsemble Model"
  },
  {
    "objectID": "posts/kaggle/pss3e3-eda.html",
    "href": "posts/kaggle/pss3e3-eda.html",
    "title": "EDA - Playground Series Season 3, Episode 4",
    "section": "",
    "text": "Code\n# Import libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.options.display.max_columns = None\n%matplotlib inline"
  },
  {
    "objectID": "posts/kaggle/pss3e3-eda.html#data-description",
    "href": "posts/kaggle/pss3e3-eda.html#data-description",
    "title": "EDA - Playground Series Season 3, Episode 4",
    "section": "1. Data Description",
    "text": "1. Data Description\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Credit Card Fraud Detection. Feature distributions are close to, but not exactly the same, as the original.\nWe are given three files:\n\ntrain.csv - the training dataset; Class is the target\ntest.csv - the test dataset; our objective is to predict Class\nsample_submission.csv - a sample submission file in the correct format\n\nDescription of each column:\n\nFeature Description\n\n\n\n\n\n\nFeature\nDescription\n\n\n\n\nid\nIndentifier for unique rows\n\n\nTime\nNumber of seconds elapsed between this transaction and the first transaction in the dataset\n\n\nV1-V28\nFeatures generated from the original dataset\n\n\nAmount\nTransaction amount\n\n\nClass\nTarget Feature: 1 for fraudulent transactions, 0 otherwise"
  },
  {
    "objectID": "posts/kaggle/pss3e3-eda.html#overview-and-structure",
    "href": "posts/kaggle/pss3e3-eda.html#overview-and-structure",
    "title": "EDA - Playground Series Season 3, Episode 4",
    "section": "2. Overview and Structure",
    "text": "2. Overview and Structure\n\n__dirname = '../input/playground-series-s3e4/'\n\ntrain = pd.read_csv(__dirname + 'train.csv')\ntest = pd.read_csv(__dirname + 'test.csv')\n\n\n# Display top 5 rows of train set\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      id\n      Time\n      V1\n      V2\n      V3\n      V4\n      V5\n      V6\n      V7\n      V8\n      V9\n      V10\n      V11\n      V12\n      V13\n      V14\n      V15\n      V16\n      V17\n      V18\n      V19\n      V20\n      V21\n      V22\n      V23\n      V24\n      V25\n      V26\n      V27\n      V28\n      Amount\n      Class\n    \n  \n  \n    \n      0\n      0\n      0.0\n      2.074329\n      -0.129425\n      -1.137418\n      0.412846\n      -0.192638\n      -1.210144\n      0.110697\n      -0.263477\n      0.742144\n      0.108782\n      -1.070243\n      -0.234910\n      -1.099360\n      0.502467\n      0.169318\n      0.065688\n      -0.306957\n      -0.323800\n      0.103348\n      -0.292969\n      -0.334701\n      -0.887840\n      0.336701\n      -0.110835\n      -0.291459\n      0.207733\n      -0.076576\n      -0.059577\n      1.98\n      0\n    \n    \n      1\n      1\n      0.0\n      1.998827\n      -1.250891\n      -0.520969\n      -0.894539\n      -1.122528\n      -0.270866\n      -1.029289\n      0.050198\n      -0.109948\n      0.908773\n      0.836798\n      -0.056580\n      -0.120990\n      -0.144028\n      -0.039582\n      1.653057\n      -0.253599\n      -0.814354\n      0.716784\n      0.065717\n      0.054848\n      -0.038367\n      0.133518\n      -0.461928\n      -0.465491\n      -0.464655\n      -0.009413\n      -0.038238\n      84.00\n      0\n    \n    \n      2\n      2\n      0.0\n      0.091535\n      1.004517\n      -0.223445\n      -0.435249\n      0.667548\n      -0.988351\n      0.948146\n      -0.084789\n      -0.042027\n      -0.818383\n      -0.376512\n      -0.226546\n      -0.552869\n      -0.886466\n      -0.180890\n      0.230286\n      0.590579\n      -0.321590\n      -0.433959\n      -0.021375\n      -0.326725\n      -0.803736\n      0.154495\n      0.951233\n      -0.506919\n      0.085046\n      0.224458\n      0.087356\n      2.69\n      0\n    \n    \n      3\n      3\n      0.0\n      1.979649\n      -0.184949\n      -1.064206\n      0.120125\n      -0.215238\n      -0.648829\n      -0.087826\n      -0.035367\n      0.885838\n      -0.007527\n      0.637441\n      0.676960\n      -1.504823\n      0.554039\n      -0.824356\n      -0.527267\n      -0.095838\n      -0.312519\n      0.642659\n      -0.340089\n      -0.095514\n      -0.079792\n      0.167701\n      -0.042939\n      0.000799\n      -0.096148\n      -0.057780\n      -0.073839\n      1.00\n      0\n    \n    \n      4\n      4\n      0.0\n      1.025898\n      -0.171827\n      1.203717\n      1.243900\n      -0.636572\n      1.099074\n      -0.938651\n      0.569239\n      0.692665\n      -0.097495\n      1.338869\n      1.391399\n      -0.128167\n      -0.081836\n      0.100548\n      -0.338937\n      0.090864\n      -0.423645\n      -0.731939\n      -0.203628\n      0.099157\n      0.608908\n      0.027901\n      -0.262813\n      0.257834\n      -0.252829\n      0.108338\n      0.021051\n      1.00\n      0\n    \n  \n\n\n\n\n\n2.1. Data Structure and Statistics\n\nprint(f\"Training set has {train.shape[0]} rows and {train.shape[1]} columns\")\nprint(f\"Testing set has {test.shape[0]} rows and {test.shape[1]} columns\")\n\nTraining set has 219129 rows and 32 columns\nTesting set has 146087 rows and 31 columns\n\n\n\ntrain.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 219129 entries, 0 to 219128\nData columns (total 32 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   id      219129 non-null  int64  \n 1   Time    219129 non-null  float64\n 2   V1      219129 non-null  float64\n 3   V2      219129 non-null  float64\n 4   V3      219129 non-null  float64\n 5   V4      219129 non-null  float64\n 6   V5      219129 non-null  float64\n 7   V6      219129 non-null  float64\n 8   V7      219129 non-null  float64\n 9   V8      219129 non-null  float64\n 10  V9      219129 non-null  float64\n 11  V10     219129 non-null  float64\n 12  V11     219129 non-null  float64\n 13  V12     219129 non-null  float64\n 14  V13     219129 non-null  float64\n 15  V14     219129 non-null  float64\n 16  V15     219129 non-null  float64\n 17  V16     219129 non-null  float64\n 18  V17     219129 non-null  float64\n 19  V18     219129 non-null  float64\n 20  V19     219129 non-null  float64\n 21  V20     219129 non-null  float64\n 22  V21     219129 non-null  float64\n 23  V22     219129 non-null  float64\n 24  V23     219129 non-null  float64\n 25  V24     219129 non-null  float64\n 26  V25     219129 non-null  float64\n 27  V26     219129 non-null  float64\n 28  V27     219129 non-null  float64\n 29  V28     219129 non-null  float64\n 30  Amount  219129 non-null  float64\n 31  Class   219129 non-null  int64  \ndtypes: float64(30), int64(2)\nmemory usage: 53.5 MB\n\n\n\ntest.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 146087 entries, 0 to 146086\nData columns (total 31 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   id      146087 non-null  int64  \n 1   Time    146087 non-null  float64\n 2   V1      146087 non-null  float64\n 3   V2      146087 non-null  float64\n 4   V3      146087 non-null  float64\n 5   V4      146087 non-null  float64\n 6   V5      146087 non-null  float64\n 7   V6      146087 non-null  float64\n 8   V7      146087 non-null  float64\n 9   V8      146087 non-null  float64\n 10  V9      146087 non-null  float64\n 11  V10     146087 non-null  float64\n 12  V11     146087 non-null  float64\n 13  V12     146087 non-null  float64\n 14  V13     146087 non-null  float64\n 15  V14     146087 non-null  float64\n 16  V15     146087 non-null  float64\n 17  V16     146087 non-null  float64\n 18  V17     146087 non-null  float64\n 19  V18     146087 non-null  float64\n 20  V19     146087 non-null  float64\n 21  V20     146087 non-null  float64\n 22  V21     146087 non-null  float64\n 23  V22     146087 non-null  float64\n 24  V23     146087 non-null  float64\n 25  V24     146087 non-null  float64\n 26  V25     146087 non-null  float64\n 27  V26     146087 non-null  float64\n 28  V27     146087 non-null  float64\n 29  V28     146087 non-null  float64\n 30  Amount  146087 non-null  float64\ndtypes: float64(30), int64(1)\nmemory usage: 34.6 MB\n\n\nWe find:\n\nAll the columns in the both train and test set are either int64 or float64.\nBased on the value for Non-Null we can observe we don’t have any missing values in our datasets.\nAs categorical type features are not present, it reduces some pain points while data preprocessing.\n\n\ntrain.describe()\n\n\n\n\n\n  \n    \n      \n      id\n      Time\n      V1\n      V2\n      V3\n      V4\n      V5\n      V6\n      V7\n      V8\n      V9\n      V10\n      V11\n      V12\n      V13\n      V14\n      V15\n      V16\n      V17\n      V18\n      V19\n      V20\n      V21\n      V22\n      V23\n      V24\n      V25\n      V26\n      V27\n      V28\n      Amount\n      Class\n    \n  \n  \n    \n      count\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n      219129.000000\n    \n    \n      mean\n      109564.000000\n      62377.415376\n      0.096008\n      0.048345\n      0.592102\n      0.069273\n      -0.161555\n      0.133688\n      -0.128224\n      0.149534\n      -0.048337\n      -0.039758\n      0.153632\n      -0.061038\n      0.014330\n      0.067649\n      0.108643\n      0.013650\n      0.036815\n      -0.033927\n      -0.008302\n      0.009708\n      -0.031064\n      -0.050852\n      -0.050531\n      -0.002992\n      0.124005\n      0.009881\n      0.014034\n      0.017313\n      66.359803\n      0.002140\n    \n    \n      std\n      63257.237906\n      25620.348569\n      1.395425\n      1.159805\n      1.132884\n      1.253125\n      1.069530\n      1.202411\n      0.817207\n      0.716212\n      1.054143\n      0.821889\n      0.976946\n      0.998470\n      1.039145\n      0.801335\n      0.891613\n      0.786654\n      0.691709\n      0.784454\n      0.739928\n      0.439521\n      0.422777\n      0.597812\n      0.318175\n      0.593100\n      0.406741\n      0.473867\n      0.233355\n      0.164859\n      150.795017\n      0.046214\n    \n    \n      min\n      0.000000\n      0.000000\n      -29.807725\n      -44.247914\n      -19.722872\n      -5.263650\n      -37.591259\n      -25.659750\n      -31.179799\n      -28.903442\n      -8.756951\n      -22.092656\n      -4.190145\n      -16.180165\n      -4.373778\n      -15.585021\n      -4.155728\n      -11.778839\n      -20.756768\n      -7.456060\n      -4.281628\n      -18.679066\n      -14.689621\n      -8.748979\n      -11.958588\n      -2.836285\n      -3.958591\n      -1.858672\n      -9.234767\n      -4.551680\n      0.000000\n      0.000000\n    \n    \n      25%\n      54782.000000\n      47933.000000\n      -0.846135\n      -0.573728\n      -0.027154\n      -0.769256\n      -0.847346\n      -0.631835\n      -0.646730\n      -0.095948\n      -0.711444\n      -0.499563\n      -0.576969\n      -0.476890\n      -0.671601\n      -0.329905\n      -0.461596\n      -0.461077\n      -0.406675\n      -0.496990\n      -0.463035\n      -0.167927\n      -0.190418\n      -0.473099\n      -0.174478\n      -0.332540\n      -0.126080\n      -0.318330\n      -0.050983\n      -0.009512\n      5.990000\n      0.000000\n    \n    \n      50%\n      109564.000000\n      63189.000000\n      0.385913\n      0.046937\n      0.735895\n      0.064856\n      -0.229929\n      -0.087778\n      -0.098970\n      0.111219\n      -0.131323\n      -0.106034\n      0.090545\n      0.087649\n      -0.016837\n      0.049266\n      0.178975\n      0.054550\n      -0.013949\n      -0.039451\n      -0.002935\n      -0.037702\n      -0.042858\n      -0.032856\n      -0.063307\n      0.038708\n      0.145934\n      -0.086388\n      0.015905\n      0.022163\n      21.900000\n      0.000000\n    \n    \n      75%\n      164346.000000\n      77519.000000\n      1.190661\n      0.814145\n      1.306110\n      0.919353\n      0.356856\n      0.482388\n      0.385567\n      0.390976\n      0.583715\n      0.403967\n      0.917392\n      0.608480\n      0.695547\n      0.460837\n      0.791255\n      0.531777\n      0.410978\n      0.446448\n      0.455718\n      0.126750\n      0.109187\n      0.354910\n      0.060221\n      0.394566\n      0.402926\n      0.253869\n      0.076814\n      0.066987\n      68.930000\n      0.000000\n    \n    \n      max\n      219128.000000\n      120580.000000\n      2.430494\n      16.068473\n      6.145578\n      12.547997\n      34.581260\n      16.233967\n      39.824099\n      18.270586\n      13.423914\n      15.878405\n      9.417789\n      5.406614\n      5.976265\n      6.078453\n      4.693323\n      5.834992\n      8.845303\n      4.847887\n      4.090974\n      15.407839\n      22.062945\n      6.163541\n      12.734391\n      4.572739\n      3.111624\n      3.402344\n      13.123618\n      23.263746\n      7475.000000\n      1.000000\n    \n  \n\n\n\n\n\ntest.describe()\n\n\n\n\n\n  \n    \n      \n      id\n      Time\n      V1\n      V2\n      V3\n      V4\n      V5\n      V6\n      V7\n      V8\n      V9\n      V10\n      V11\n      V12\n      V13\n      V14\n      V15\n      V16\n      V17\n      V18\n      V19\n      V20\n      V21\n      V22\n      V23\n      V24\n      V25\n      V26\n      V27\n      V28\n      Amount\n    \n  \n  \n    \n      count\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n      146087.000000\n    \n    \n      mean\n      292172.000000\n      144637.928166\n      0.512929\n      -0.013098\n      -0.697478\n      -0.273258\n      0.321856\n      -0.050204\n      0.073419\n      0.043803\n      -0.071620\n      0.013962\n      -0.249980\n      0.108453\n      -0.127648\n      -0.151857\n      -0.199467\n      0.027958\n      -0.052971\n      0.128470\n      -0.008261\n      -0.056678\n      0.044729\n      0.175461\n      0.018471\n      0.016029\n      -0.118352\n      -0.015199\n      0.006236\n      0.002035\n      66.182463\n    \n    \n      std\n      42171.828725\n      14258.025396\n      1.628455\n      1.247749\n      1.292522\n      1.365752\n      1.146215\n      1.332880\n      0.946681\n      0.749513\n      0.924996\n      0.932453\n      0.881086\n      0.686018\n      0.916833\n      0.832182\n      0.774293\n      0.811819\n      0.713056\n      0.798006\n      0.722225\n      0.458364\n      0.449017\n      0.710704\n      0.359597\n      0.633929\n      0.479720\n      0.446154\n      0.255935\n      0.174613\n      153.151535\n    \n    \n      min\n      219129.000000\n      120580.000000\n      -34.755944\n      -37.803827\n      -18.934952\n      -5.497560\n      -25.639591\n      -14.133040\n      -18.715915\n      -26.926164\n      -4.823352\n      -12.333747\n      -4.333619\n      -8.836286\n      -4.131766\n      -14.172557\n      -4.072435\n      -7.639810\n      -11.868164\n      -4.342548\n      -4.823554\n      -26.412867\n      -13.087263\n      -5.392649\n      -12.814296\n      -2.789084\n      -3.361564\n      -1.743223\n      -9.412538\n      -8.262339\n      0.000000\n    \n    \n      25%\n      255650.500000\n      132698.000000\n      -0.679988\n      -0.715885\n      -1.619268\n      -1.021205\n      -0.418547\n      -0.891441\n      -0.570042\n      -0.231824\n      -0.634695\n      -0.636147\n      -0.957520\n      -0.324639\n      -0.717325\n      -0.617641\n      -0.737472\n      -0.451743\n      -0.572054\n      -0.380886\n      -0.383668\n      -0.237529\n      -0.166715\n      -0.393667\n      -0.135059\n      -0.368957\n      -0.409938\n      -0.284914\n      -0.066037\n      -0.057447\n      5.990000\n    \n    \n      50%\n      292172.000000\n      144493.000000\n      0.285798\n      0.009058\n      -0.719060\n      -0.482945\n      0.306851\n      -0.372813\n      0.118545\n      0.014979\n      -0.075909\n      -0.065457\n      -0.182940\n      0.124780\n      -0.098396\n      -0.065481\n      -0.167537\n      0.076469\n      -0.123968\n      0.123625\n      -0.002966\n      -0.096729\n      0.058393\n      0.250169\n      0.017835\n      0.029727\n      -0.142325\n      -0.069342\n      -0.003539\n      -0.026955\n      21.790000\n    \n    \n      75%\n      328693.500000\n      156140.000000\n      1.974015\n      0.827420\n      0.073874\n      0.369725\n      0.955997\n      0.302724\n      0.734503\n      0.296969\n      0.513770\n      0.564146\n      0.453913\n      0.581384\n      0.504763\n      0.395024\n      0.305768\n      0.546893\n      0.372321\n      0.677771\n      0.374562\n      0.065753\n      0.244817\n      0.749555\n      0.167514\n      0.562138\n      0.182937\n      0.216632\n      0.069334\n      0.066954\n      66.000000\n    \n    \n      max\n      365215.000000\n      172790.000000\n      2.452901\n      12.390128\n      4.492640\n      11.232928\n      24.352818\n      16.596635\n      27.023955\n      12.098322\n      7.888980\n      14.735004\n      6.204939\n      5.107089\n      3.928334\n      7.869385\n      5.374923\n      5.570906\n      7.136535\n      3.758750\n      4.929496\n      15.829261\n      15.333546\n      5.771245\n      17.481609\n      4.541724\n      4.555960\n      3.374748\n      12.673968\n      13.093229\n      4630.600000\n    \n  \n\n\n\n\nWe find:\n\nThe feature names don’t help us understand what the feature is about or how to interpret it.\nFor most of the features the minimum and maximum values are to different extremes whereas their 1st quatile, median, and 3rd quatile are close to each other.\n\nFor example, V1 has minimum value as -34.755944 and maximum value as 2.452901.\n25%: -0.679988, 50%: 0.285798, and 75%: 1.974015\n\n\n\n\n2.2. Missing values\n\n# Check for missing values in train set\nprint(f\"Number of missing values in training set: {sum(train.isna().sum())}\")\nprint(f\"Number of missing values in testing set: {sum(test.isna().sum())}\")\n\nNumber of missing values in training set: 0\nNumber of missing values in testing set: 0\n\n\nWe find:\n\nAs seen in the info() results we confirm we don’t have any missing values in the both datasets."
  },
  {
    "objectID": "posts/kaggle/pss3e3-eda.html#visualizations",
    "href": "posts/kaggle/pss3e3-eda.html#visualizations",
    "title": "EDA - Playground Series Season 3, Episode 4",
    "section": "3. Visualizations",
    "text": "3. Visualizations\n\n# Extract the columns which contains 'V'\nv_columns = [col for col in train.columns if col[0] == 'V']\nuseful_columns = [col for col in train.columns if col not in ['id', 'Time']]\n\n\n3.1. Univariate Analysis\n\nsns.countplot(x='Class', data=train)\nplt.title('Distribution of Class', fontsize=14)\nplt.show()\n\n\n\n\n\ntrain.Class.value_counts() / train.shape[0] * 100\n\n0    99.785971\n1     0.214029\nName: Class, dtype: float64\n\n\nWe find:\n\nThe two classes are highly imbalanced.\n~99.78% of the data is labelled as Class 0\n~0.21% of the data is labelled as Class 1.\n\n\nax = train[v_columns].plot(kind='box', figsize=(20, 8))\nplt.title('Box plots of features start\\'s with V', fontsize=14)\nplt.show()\n\n\n\n\nWe find:\n\nSimilar, to what we saw in the result from the describe() method, most of the data is centered around 0.\nAlso, there are a lot of data points that appear to be outliers.\nFew of the columns, for example, V1 appear to be skewed.\n\n\n# Explore distribution of each feature for train and test sets.\n\nfig, axes = plt.subplots(nrows=29, ncols=3, figsize=(20, 5*29))\n\nfor row, col in enumerate(v_columns + ['Amount']):\n    sns.kdeplot(data=train, x=col, ax=axes[row, 0], fill=True)\n    sns.kdeplot(data=test, x=col, ax=axes[row, 0], fill=True)\n    axes[row, 0].set_title('Density plot: ' + col, fontsize=14)\n    axes[row, 0].legend()\n    axes[row, 0].set_xlabel('')\n\n    sns.boxplot(data=train, x=col, ax=axes[row, 1], orient='h')\n    axes[row, 1].set_title('Box plot (Train): ' + col, fontsize=14)\n    axes[row, 1].set_xlabel('')\n\n    sns.boxplot(data=test, x=col, ax=axes[row, 2], orient='h')\n    axes[row, 2].set_title('Box plot (Test): ' + col, fontsize=14)\n    axes[row, 2].set_xlabel('')\n\nfig.tight_layout()\nplt.show()\n\n\n\n\nWe find:\n\nThe distribution of train and test data are quite similar.\nThis helps to the model to better perform on the unseen data.\n\n\n\n3.2. Bivariate Analysis\n\nCreating a scatter plot for all combinations of features would be explode and be overwhelming.\nI would encourage you to play with different combinations and look at the data distribution.\nUsing pairplot is also an good option but the graphs are too tiny to observe. You can definititely try using it and then explore any plot you find interesting.\n\nBelow are some plot I found interesting.\n\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(18, 12))\n\nsns.scatterplot(data=train, x='V24', y='V2', hue=train.Class, ax=axes[0, 0])\naxes[0, 0].set_title('V24 vs V2', fontsize=14)\n\nsns.scatterplot(data=train, x='V10', y='V9', hue=train.Class, ax=axes[0, 1])\naxes[0, 1].set_title('V10 vs V9', fontsize=14)\n\nsns.scatterplot(data=train, x='V17', y='V2', hue=train.Class, ax=axes[0, 2])\naxes[0, 2].set_title('V17 vs V2', fontsize=14)\n\nsns.scatterplot(data=train, x='V20', y='V21', hue=train.Class, ax=axes[1, 0])\naxes[1, 0].set_title('V20 vs V21', fontsize=14)\n\nsns.scatterplot(data=train, x='V21', y='V22', hue=train.Class, ax=axes[1, 1])\naxes[1, 1].set_title('V21 vs V22', fontsize=14)\n\nsns.scatterplot(data=train, x='V21', y='V2', hue=train.Class, ax=axes[1, 2])\naxes[1, 2].set_title('V21 vs V2', fontsize=14)\n\nsns.scatterplot(data=train, x='V24', y='V1', hue=train.Class, ax=axes[2, 0])\naxes[2, 0].set_title('V24 vs V1', fontsize=14)\n\nsns.scatterplot(data=train, x='V15', y='V19', hue=train.Class, ax=axes[2, 1])\naxes[2, 1].set_title('V15 vs V19', fontsize=14)\n\nsns.scatterplot(data=train, x='V20', y='Amount',\n                hue=train.Class, ax=axes[2, 2])\naxes[2, 2].set_title('V20 vs Amount', fontsize=14)\n\nfig.tight_layout()\nplt.show()\n\n\n\n\nWe find:\n\nFeatures such as V9 and V10, V21 and V22 show some degree of correlation.\nWhereas others are distributed in ramdom fashion.\nV15 and V19 are totall random and doesn’t show any patterns that can be useful for us.\nThere are small clusters, example in figure V24 vs V1 where we don’t find any samples of data with Class 1.\n\n\ntrain.V20/(train.Amount + 1e-6)\n\n0        -0.147964\n1         0.000782\n2        -0.007946\n3        -0.340089\n4        -0.203627\n            ...   \n219124   -0.000985\n219125   -0.004856\n219126   -0.214989\n219127   -0.002170\n219128    0.015379\nLength: 219129, dtype: float64\n\n\n\n# Look at heatmap for correlation between different numeric features\n\ncorr = train[v_columns + ['Amount']].corr()\nfig = plt.figure(figsize=(25, 12))\nsns.heatmap(corr, annot=True, fmt=\".2f\")\nplt.show()\n\n\n\n\nWe find:\n\nV20 and Amount has the highest value of positive correlation, followed by V5 and V6, followed by V21 and V22\nV2 and Amount has the highest value of negative correlation, followed by V5 and Amount, followed by V12 and V14"
  },
  {
    "objectID": "posts/kaggle/pss3e3-eda.html#conclusion",
    "href": "posts/kaggle/pss3e3-eda.html#conclusion",
    "title": "EDA - Playground Series Season 3, Episode 4",
    "section": "4. Conclusion",
    "text": "4. Conclusion\nThank you for reading and I hope you found this notebook helpful. You can find my modeling notebook here\nAny feedback is welcomed, I aim to learn and improve my skillset in my kaggle journey.\nUpvote if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "posts/kaggle/pss3e4-ensemble-model.html",
    "href": "posts/kaggle/pss3e4-ensemble-model.html",
    "title": "Modeling - Playground Series Season 3, Episode 4",
    "section": "",
    "text": "To read more about Exploratory Data Analysis for this problem statement, you can read my notebook here\nIt was my first try at a kaggle comptetion and I got a lot to learn. This notebook helped me get a score of:\nPosition: 138/641"
  },
  {
    "objectID": "posts/kaggle/pss3e4-ensemble-model.html#data-description",
    "href": "posts/kaggle/pss3e4-ensemble-model.html#data-description",
    "title": "Modeling - Playground Series Season 3, Episode 4",
    "section": "1. Data Description",
    "text": "1. Data Description\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Credit Card Fraud Detection. Feature distributions are close to, but not exactly the same, as the original.\nWe are given three files:\n\ntrain.csv - the training dataset; Class is the target\ntest.csv - the test dataset; our objective is to predict Class\nsample_submission.csv - a sample submission file in the correct format\n\nDescription of each column:\n\nFeature Description\n\n\n\n\n\n\nFeature\nDescription\n\n\n\n\nid\nIndentifier for unique rows\n\n\nTime\nNumber of seconds elapsed between this transaction and the first transaction in the dataset\n\n\nV1-V28\nFeatures generated from the original dataset\n\n\nAmount\nTransaction amount\n\n\nClass\nTarget Feature: 1 for fraudulent transactions, 0 otherwise"
  },
  {
    "objectID": "posts/kaggle/pss3e4-ensemble-model.html#data-preparation",
    "href": "posts/kaggle/pss3e4-ensemble-model.html#data-preparation",
    "title": "Modeling - Playground Series Season 3, Episode 4",
    "section": "2. Data Preparation",
    "text": "2. Data Preparation\n\n__dirname = '../input/playground-series-s3e4/'\nog_data = '/kaggle/input/creditcardfraud/creditcard.csv'\n\ntrain = pd.read_csv(__dirname + 'train.csv')\ntest = pd.read_csv(__dirname + 'test.csv')\noriginal = pd.read_csv(og_data)\nsubmissions = pd.read_csv(__dirname + 'sample_submission.csv')\n\n\nprint(f\"Training set has {train.shape[0]} rows and {train.shape[1]} columns\")\nprint(f\"Testing set has {test.shape[0]} rows and {test.shape[1]} columns\")\nprint(\n    f\"Original set has {original.shape[0]} rows and {original.shape[1]} columns\")\n\nTraining set has 219129 rows and 32 columns\nTesting set has 146087 rows and 31 columns\nOriginal set has 284807 rows and 31 columns\n\n\n\n# Check for missing values\nprint(f\"Number of missing values in training set: {sum(train.isna().sum())}\")\nprint(f\"Number of missing values in testing set: {sum(test.isna().sum())}\")\nprint(\n    f\"Number of missing values in original set: {sum(original.isna().sum())}\")\n\nNumber of missing values in training set: 0\nNumber of missing values in testing set: 0\nNumber of missing values in original set: 0\n\n\n\n# Merge train and original\ntrain = pd.concat([train, original], axis=0, ignore_index=True)\n\n\ncols = [col for col in train.columns if col not in ['id', 'Time']]\nprint(' Number of duplicates with new criteria:',\n      train[cols].duplicated().sum())\n\n Number of duplicates with new criteria: 9254\n\n\n\n# Drop Duplicates\ntrain.drop_duplicates(subset=cols, inplace=True, keep='first')\n\n\n# Drop columns\ntrain.drop(['id', 'Time'], axis=1, inplace=True)\ntest.drop(['id', 'Time'], axis=1, inplace=True)"
  },
  {
    "objectID": "posts/kaggle/pss3e4-ensemble-model.html#modeling",
    "href": "posts/kaggle/pss3e4-ensemble-model.html#modeling",
    "title": "Modeling - Playground Series Season 3, Episode 4",
    "section": "3. Modeling",
    "text": "3. Modeling\n\n# Perform feature scaling\nscaler = RobustScaler()\n\ncols = test.columns\ntrain[cols] = scaler.fit_transform(train[cols])\ntest[cols] = scaler.transform(test[cols])\n\n\nX_train, y_train = train.drop('Class', axis=1), train.Class\n\n\n# Dictionary to save model results\nresults = defaultdict(lambda: defaultdict(list))\n\n\n# Function to plot feature importance\ndef plot_feature_imp(df, col):\n    df = pd.concat(df, axis=1).head(15)\n    df.sort_values(col).plot(kind='barh', figsize=(\n        15, 10), title=\"Feature Imp Across Folds\")\n    plt.show()\n\n\n3.1. Baseline Model\n\nn_folds = 5\nseed = 42\nmodel = 'Logistic Regression'\n\nskf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\ntest_pred = 0\n\nresults[model] = defaultdict(list)\n\nfor idx, (train_idx, test_idx) in enumerate(skf.split(X_train, y_train)):\n    X, y = X_train.iloc[train_idx], y_train.iloc[train_idx]\n    X_valid, y_valid = X_train.iloc[test_idx], y_train.iloc[test_idx]\n\n    lr = LogisticRegression()\n\n    # Fit the model\n    lr.fit(X, y)\n    # Predict on validation set\n    pred_proba = lr.predict_proba(X_valid)[:, 1]\n\n    # Predict on test set\n    test_pred_proba = lr.predict_proba(test)[:, 1]\n    test_pred += test_pred_proba/n_folds\n\n    # Calcualte AUC score\n    auc = roc_auc_score(y_valid, pred_proba)\n\n    # Feature Importance\n    feature_imp = pd.DataFrame(index=X_train.columns,\n                               data=lr.coef_[0],\n                               columns=[f\"{idx + 1}_importance\"])\n\n    print(f\"Fold={idx+1}, AUC score: {auc:.2f}\")\n\n    # Save Results\n    results[model]['auc'].append(auc)\n    results[model]['model'].append(lr)\n    results[model]['feature_imp'].append(feature_imp)\n\nresults[model]['test_pred'] = test_pred\nprint(f\"Mean AUC: {np.mean(results[model]['auc'])}\")\n\nFold=1, AUC score: 0.85\nFold=2, AUC score: 0.88\nFold=3, AUC score: 0.88\nFold=4, AUC score: 0.87\nFold=5, AUC score: 0.86\nMean AUC: 0.8679439362536849\n\n\n\n# Plot feature importance for logistic regression model\nplot_feature_imp(results[model]['feature_imp'], \"2_importance\")\n\n\n\n\n\n# Made a baseline submission to see how the model performs\nlr_submission = submissions.copy()\nlr_submission['Class'] = results[model]['test_pred']\nlr_submission.to_csv('lr_submission.csv', index=False)\n\n\nlr_submission.head()\n\n\n\n\n\n  \n    \n      \n      id\n      Class\n    \n  \n  \n    \n      0\n      219129\n      0.001192\n    \n    \n      1\n      219130\n      0.000621\n    \n    \n      2\n      219131\n      0.000201\n    \n    \n      3\n      219132\n      0.000699\n    \n    \n      4\n      219133\n      0.000361\n    \n  \n\n\n\n\n\n\n3.2. XGBoost Classifier\n\nxgb_params = {\n    'n_estimators': 2000,\n    'min_child_weight': 96,\n    'max_depth': 7,\n    'learning_rate': 0.18,\n    'subsample': 0.95,\n    'colsample_bytree': 0.95,\n    'reg_lambda': 1.50,\n    'reg_alpha': 1.50,\n    'gamma': 1.50,\n    'max_bin': 512,\n    'random_state': seed,\n    'objective': 'binary:logistic',\n    'tree_method': 'hist',\n    'eval_metric': 'auc'\n}\n\n\nn_folds = 5\nseed = 42\nmodel = 'XGBoostClassifier'\n\nskf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\ntest_pred = 0\n\nresults[model] = defaultdict(list)\n\nfor idx, (train_idx, test_idx) in enumerate(skf.split(X_train, y_train)):\n    X, y = X_train.iloc[train_idx], y_train.iloc[train_idx]\n    X_valid, y_valid = X_train.iloc[test_idx], y_train.iloc[test_idx]\n\n    xgb = XGBClassifier(**xgb_params)\n\n    # Fit the model\n    xgb.fit(X, y)\n    # Predict on validation set\n    pred_proba = xgb.predict_proba(X_valid)[:, 1]\n\n    # Predict on test set\n    test_pred_proba = xgb.predict_proba(test)[:, 1]\n    test_pred += test_pred_proba/n_folds\n\n    # Calcualte AUC score\n    auc = roc_auc_score(y_valid, pred_proba)\n\n    # Feature Importance\n    feature_imp = pd.DataFrame(index=X_train.columns,\n                               data=xgb.feature_importances_,\n                               columns=[f\"{idx + 1}_importance\"])\n\n    print(f\"Fold={idx+1}, AUC score: {auc:.2f}\")\n\n    # Save Results\n    results[model]['auc'].append(auc)\n    results[model]['model'].append(lr)\n    results[model]['feature_imp'].append(feature_imp)\n\nresults[model]['test_pred'] = test_pred\nprint(f\"Mean AUC: {np.mean(results[model]['auc'])}\")\n\nFold=1, AUC score: 0.88\nFold=2, AUC score: 0.90\nFold=3, AUC score: 0.91\nFold=4, AUC score: 0.89\nFold=5, AUC score: 0.90\nMean AUC: 0.8951748159271997\n\n\n\n# Plot feature importance for XGBoost model\nplot_feature_imp(results[model]['feature_imp'], \"2_importance\")\n\n\n\n\n\n# Made a submission to see how the model performs\nxgb_submission = submissions.copy()\nxgb_submission['Class'] = results[model]['test_pred']\nxgb_submission.to_csv('xgb_submission.csv', index=False)\n\n\nxgb_submission.head()\n\n\n\n\n\n  \n    \n      \n      id\n      Class\n    \n  \n  \n    \n      0\n      219129\n      0.001166\n    \n    \n      1\n      219130\n      0.000573\n    \n    \n      2\n      219131\n      0.000367\n    \n    \n      3\n      219132\n      0.001084\n    \n    \n      4\n      219133\n      0.000131\n    \n  \n\n\n\n\n\n\n3.3. LightGBM Classifier\n\nlgbm_params = {\n    'n_estimators': 500,\n    'learning_rate': 0.1,\n    'num_leaves': 195,\n    'max_depth': 9,\n    'min_data_in_leaf': 46,\n    'lambda_l1': 0.01,\n    'lambda_l2': 0.6,\n    'min_gain_to_split': 1.42,\n    'bagging_fraction': 0.45,\n    'feature_fraction': 0.3,\n    'verbosity': -1,\n    'boosting_type': 'dart',\n    'random_state': seed,\n    'objective': 'binary'\n}\n\n\nn_folds = 5\nseed = 42\nmodel = 'LGBMClassifier'\n\nskf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\ntest_pred = 0\n\nresults[model] = defaultdict(list)\n\nfor idx, (train_idx, test_idx) in enumerate(skf.split(X_train, y_train)):\n    X, y = X_train.iloc[train_idx], y_train.iloc[train_idx]\n    X_valid, y_valid = X_train.iloc[test_idx], y_train.iloc[test_idx]\n\n    lgbm = LGBMClassifier(**lgbm_params)\n\n    # Fit the model\n    lgbm.fit(X, y)\n    # Predict on validation set\n    pred_proba = lgbm.predict_proba(X_valid)[:, 1]\n\n    # Predict on test set\n    test_pred_proba = lgbm.predict_proba(test)[:, 1]\n    test_pred += test_pred_proba/n_folds\n\n    # Calcualte AUC score\n    auc = roc_auc_score(y_valid, pred_proba)\n\n    # Feature Importance\n    feature_imp = pd.DataFrame(index=X_train.columns,\n                               data=lgbm.feature_importances_,\n                               columns=[f\"{idx + 1}_importance\"])\n\n    print(f\"Fold={idx+1}, AUC score: {auc:.2f}\")\n\n    # Save Results\n    results[model]['auc'].append(auc)\n    results[model]['model'].append(lr)\n    results[model]['feature_imp'].append(feature_imp)\n\nresults[model]['test_pred'] = test_pred\nprint(f\"Mean AUC: {np.mean(results[model]['auc'])}\")\n\n[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n[LightGBM] [Warning] bagging_fraction is set=0.45, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45\n[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n[LightGBM] [Warning] min_gain_to_split is set=1.42, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=1.42\n[LightGBM] [Warning] lambda_l2 is set=0.6, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6\nFold=1, AUC score: 0.89\n[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n[LightGBM] [Warning] bagging_fraction is set=0.45, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45\n[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n[LightGBM] [Warning] min_gain_to_split is set=1.42, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=1.42\n[LightGBM] [Warning] lambda_l2 is set=0.6, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6\nFold=2, AUC score: 0.90\n[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n[LightGBM] [Warning] bagging_fraction is set=0.45, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45\n[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n[LightGBM] [Warning] min_gain_to_split is set=1.42, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=1.42\n[LightGBM] [Warning] lambda_l2 is set=0.6, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6\nFold=3, AUC score: 0.91\n[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n[LightGBM] [Warning] bagging_fraction is set=0.45, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45\n[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n[LightGBM] [Warning] min_gain_to_split is set=1.42, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=1.42\n[LightGBM] [Warning] lambda_l2 is set=0.6, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6\nFold=4, AUC score: 0.89\n[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n[LightGBM] [Warning] bagging_fraction is set=0.45, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45\n[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n[LightGBM] [Warning] min_gain_to_split is set=1.42, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=1.42\n[LightGBM] [Warning] lambda_l2 is set=0.6, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6\nFold=5, AUC score: 0.90\nMean AUC: 0.8997556737837178\n\n\n\n# Plot feature importance for LightGBM model\nplot_feature_imp(results[model]['feature_imp'], \"4_importance\")\n\n\n\n\n\n# Made a submission to see how the model performs\nlgbm_submission = submissions.copy()\nlgbm_submission['Class'] = results[model]['test_pred']\nlgbm_submission.to_csv('lgbm_submission.csv', index=False)\n\n\nlgbm_submission.head()\n\n\n\n\n\n  \n    \n      \n      id\n      Class\n    \n  \n  \n    \n      0\n      219129\n      0.001045\n    \n    \n      1\n      219130\n      0.000622\n    \n    \n      2\n      219131\n      0.000337\n    \n    \n      3\n      219132\n      0.000817\n    \n    \n      4\n      219133\n      0.000307\n    \n  \n\n\n\n\n\n\n3.4. CatBoot Classifier\n\ncatboost_params = {\n    'n_estimators': 500,\n    'learning_rate': 0.1,\n    'one_hot_max_size': 12,\n    'depth': 9,\n    'l2_leaf_reg': 0.6,\n    'colsample_bylevel': 0.06,\n    'min_data_in_leaf': 12,\n    'bootstrap_type': 'Bernoulli',\n    'verbose': False,\n    'random_state': seed,\n    'objective': 'Logloss',\n    'eval_metric': 'AUC'\n}\n\n\nn_folds = 5\nseed = 42\nmodel = 'CatBoostClassifier'\n\nskf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\ntest_pred = 0\n\nresults[model] = defaultdict(list)\n\nfor idx, (train_idx, test_idx) in enumerate(skf.split(X_train, y_train)):\n    X, y = X_train.iloc[train_idx], y_train.iloc[train_idx]\n    X_valid, y_valid = X_train.iloc[test_idx], y_train.iloc[test_idx]\n\n    catb = CatBoostClassifier(**catboost_params)\n\n    # Fit the model\n    catb.fit(X, y)\n    # Predict on validation set\n    pred_proba = catb.predict_proba(X_valid)[:, 1]\n\n    # Predict on test set\n    test_pred_proba = catb.predict_proba(test)[:, 1]\n    test_pred += test_pred_proba/n_folds\n\n    # Calcualte AUC score\n    auc = roc_auc_score(y_valid, pred_proba)\n\n    # Feature Importance\n    feature_imp = pd.DataFrame(index=X_train.columns,\n                               data=catb.feature_importances_,\n                               columns=[f\"{idx + 1}_importance\"])\n\n    print(f\"Fold={idx+1}, AUC score: {auc:.2f}\")\n\n    # Save Results\n    results[model]['auc'].append(auc)\n    results[model]['model'].append(lr)\n    results[model]['feature_imp'].append(feature_imp)\n\nresults[model]['test_pred'] = test_pred\nprint(f\"Mean AUC: {np.mean(results[model]['auc'])}\")\n\nFold=1, AUC score: 0.87\nFold=2, AUC score: 0.88\nFold=3, AUC score: 0.88\nFold=4, AUC score: 0.89\nFold=5, AUC score: 0.89\nMean AUC: 0.877921430469104\n\n\n\n# Plot feature importance for CatBoost model\nplot_feature_imp(results[model]['feature_imp'], \"2_importance\")\n\n\n\n\n\n# Made a submission to see how the model performs\ncatb_submission = submissions.copy()\ncatb_submission['Class'] = results[model]['test_pred']\ncatb_submission.to_csv('catb_submission.csv', index=False)\n\n\ncatb_submission.head()\n\n\n\n\n\n  \n    \n      \n      id\n      Class\n    \n  \n  \n    \n      0\n      219129\n      0.000671\n    \n    \n      1\n      219130\n      0.000433\n    \n    \n      2\n      219131\n      0.000124\n    \n    \n      3\n      219132\n      0.000599\n    \n    \n      4\n      219133\n      0.000103\n    \n  \n\n\n\n\n\n\n3.5. Ensemble Results\n\na = 0.1\nb = 0.4\nc = 0.3\nd = 0.2\n\npred = a * lr_submission['Class'] + b * xgb_submission['Class'] + \\\n    c * lgbm_submission['Class'] + d * catb_submission['Class']\n\n# Made a submission to see how the model performs\nsubmission = submissions.copy()\nsubmission['Class'] = pred\nsubmission.to_csv('submission_CatB.csv', index=False)\n\n\na = 0\nb = 0.4\nc = 0.4\nd = 0.2\n\npred = a * lr_submission['Class'] + b * xgb_submission['Class'] + \\\n    c * lgbm_submission['Class'] + d * catb_submission['Class']\n\n# Made a submission to see how the model performs\nsubmission = submissions.copy()\nsubmission['Class'] = pred\nsubmission.to_csv('submission_noLR.csv', index=False)"
  },
  {
    "objectID": "posts/kaggle/pss3e4-ensemble-model.html#conclusion",
    "href": "posts/kaggle/pss3e4-ensemble-model.html#conclusion",
    "title": "Modeling - Playground Series Season 3, Episode 4",
    "section": "4. Conclusion",
    "text": "4. Conclusion\nThank you for reading so far and I hope you found this notebook helpful.\nUpvote if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "posts/kaggle/song-popularity-eda.html",
    "href": "posts/kaggle/song-popularity-eda.html",
    "title": "Song Popularity EDA",
    "section": "",
    "text": "This Python notebook is the Python version of Song Popularity EDA - Live Coding Fun by Martin Henze\nPurpose of this notebook is to recreate the plots in python for learning purpose.\nThe recording of the live-coding session can be found on Abhishek Thakur’s YouTube channel:"
  },
  {
    "objectID": "posts/kaggle/song-popularity-eda.html#introduction",
    "href": "posts/kaggle/song-popularity-eda.html#introduction",
    "title": "Song Popularity EDA",
    "section": "1. Introduction",
    "text": "1. Introduction\nThe competition is about Song Prediction based on a set of different features. The dataset contains the basic file such as train.csv, test.csv and submission_sample.csv. The dataset used in this competition is in tabular format. The evaluation metric used for this competition is AUC score."
  },
  {
    "objectID": "posts/kaggle/song-popularity-eda.html#preparation",
    "href": "posts/kaggle/song-popularity-eda.html#preparation",
    "title": "Song Popularity EDA",
    "section": "2. Preparation",
    "text": "2. Preparation\nInitially we’ll load different libraries used in our analysis. Also, load the train and test data.\n\n\nCode\n# Import libraries and load the data\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport random\nimport warnings\n\nfrom plotnine import *\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")\n\n\n\n\nCode\n# Load the data\ntrain = pd.read_csv(\"/kaggle/input/song-popularity-prediction/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/song-popularity-prediction/test.csv\")"
  },
  {
    "objectID": "posts/kaggle/song-popularity-eda.html#overview-structure-and-data-content",
    "href": "posts/kaggle/song-popularity-eda.html#overview-structure-and-data-content",
    "title": "Song Popularity EDA",
    "section": "3. Overview: structure and data content",
    "text": "3. Overview: structure and data content\nThe first step we’ll do is look at the raw data. This tell us about the different features in the dataset, missing values, and types of features (numeric, string, categorical, etc.).\n\n3.1. Look at the data\nLet’s look at the basic structure of the data\n\n\nCode\nprint('\\nInformation about Data')\ndisplay(train.info())\n\n\n\nInformation about Data\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 40000 entries, 0 to 39999\nData columns (total 15 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   id                40000 non-null  int64  \n 1   song_duration_ms  35899 non-null  float64\n 2   acousticness      36008 non-null  float64\n 3   danceability      35974 non-null  float64\n 4   energy            36025 non-null  float64\n 5   instrumentalness  36015 non-null  float64\n 6   key               35935 non-null  float64\n 7   liveness          35914 non-null  float64\n 8   loudness          36043 non-null  float64\n 9   audio_mode        40000 non-null  int64  \n 10  speechiness       40000 non-null  float64\n 11  tempo             40000 non-null  float64\n 12  time_signature    40000 non-null  int64  \n 13  audio_valence     40000 non-null  float64\n 14  song_popularity   40000 non-null  int64  \ndtypes: float64(11), int64(4)\nmemory usage: 4.6 MB\n\n\nNone\n\n\nWe find:\n\nThere are 40000 entries and 15 features in total.\nAll the column data type is either int or float i.e. all the columns are numeric. This make is comparatively easier to work with compared to columns contains string type data.\nWe can also observe there are columns that contain less than 40K Non-Null values which indicates missing values in the dataset.\n\nLet’s now look at the top 20 rows of the data.\n\n\nCode\n\"\"\"Display top 20 rows of the train data\"\"\"\ndisplay(train.head(20).style.set_caption(\"First Twenty rows of Training Data\"))\n\n\n\n\n\n  First Twenty rows of Training Data\n  \n    \n       \n      id\n      song_duration_ms\n      acousticness\n      danceability\n      energy\n      instrumentalness\n      key\n      liveness\n      loudness\n      audio_mode\n      speechiness\n      tempo\n      time_signature\n      audio_valence\n      song_popularity\n    \n  \n  \n    \n      0\n      0\n      212990.000000\n      0.642286\n      0.856520\n      0.707073\n      0.002001\n      10.000000\n      nan\n      -5.619088\n      0\n      0.082570\n      158.386236\n      4\n      0.734642\n      0\n    \n    \n      1\n      1\n      nan\n      0.054866\n      0.733289\n      0.835545\n      0.000996\n      8.000000\n      0.436428\n      -5.236965\n      1\n      0.127358\n      102.752988\n      3\n      0.711531\n      1\n    \n    \n      2\n      2\n      193213.000000\n      nan\n      0.188387\n      0.783524\n      -0.002694\n      5.000000\n      0.170499\n      -4.951759\n      0\n      0.052282\n      178.685791\n      3\n      0.425536\n      0\n    \n    \n      3\n      3\n      249893.000000\n      0.488660\n      0.585234\n      0.552685\n      0.000608\n      0.000000\n      0.094805\n      -7.893694\n      0\n      0.035618\n      128.715630\n      3\n      0.453597\n      0\n    \n    \n      4\n      4\n      165969.000000\n      0.493017\n      nan\n      0.740982\n      0.002033\n      10.000000\n      0.094891\n      -2.684095\n      0\n      0.050746\n      121.928157\n      4\n      0.741311\n      0\n    \n    \n      5\n      5\n      188891.000000\n      0.035655\n      0.825919\n      0.804528\n      -0.000005\n      4.000000\n      0.120758\n      -6.122926\n      0\n      0.039012\n      115.679128\n      4\n      0.709408\n      0\n    \n    \n      6\n      6\n      161061.000000\n      0.081743\n      0.673588\n      0.880181\n      0.000327\n      0.000000\n      0.535411\n      -2.909607\n      1\n      0.030902\n      98.046205\n      4\n      0.982729\n      0\n    \n    \n      7\n      7\n      196202.000000\n      0.259747\n      0.813214\n      0.554385\n      0.000390\n      8.000000\n      0.276580\n      -7.794237\n      0\n      0.207067\n      158.626764\n      3\n      0.662987\n      1\n    \n    \n      8\n      8\n      169660.000000\n      nan\n      0.653263\n      0.917034\n      0.001748\n      0.000000\n      nan\n      -4.422089\n      0\n      0.031608\n      122.382398\n      3\n      0.297683\n      1\n    \n    \n      9\n      9\n      167245.000000\n      0.019617\n      0.595235\n      0.820039\n      0.761884\n      5.000000\n      0.181098\n      -5.154293\n      0\n      0.054493\n      110.524824\n      4\n      0.535453\n      0\n    \n    \n      10\n      10\n      128274.000000\n      0.614007\n      0.397899\n      0.346820\n      0.002853\n      3.000000\n      0.132549\n      nan\n      1\n      0.059512\n      87.363516\n      3\n      0.671581\n      1\n    \n    \n      11\n      11\n      213121.000000\n      0.044053\n      0.817874\n      0.729679\n      0.003660\n      5.000000\n      0.137938\n      -4.880149\n      0\n      0.038814\n      124.199541\n      4\n      0.816472\n      1\n    \n    \n      12\n      12\n      219730.000000\n      0.339275\n      0.660707\n      nan\n      nan\n      0.000000\n      0.223173\n      -12.005655\n      0\n      0.089726\n      164.877811\n      3\n      0.322253\n      1\n    \n    \n      13\n      13\n      nan\n      0.455778\n      0.448538\n      0.754924\n      nan\n      nan\n      0.076379\n      -3.158905\n      0\n      0.034837\n      118.664526\n      3\n      0.862989\n      0\n    \n    \n      14\n      14\n      nan\n      0.462876\n      0.384318\n      0.653525\n      0.781326\n      6.000000\n      nan\n      -10.362441\n      0\n      0.065149\n      141.581118\n      3\n      0.432883\n      0\n    \n    \n      15\n      15\n      nan\n      0.059284\n      0.164167\n      0.877743\n      0.002113\n      8.000000\n      0.227997\n      -3.627678\n      0\n      0.246330\n      174.445180\n      4\n      0.530006\n      1\n    \n    \n      16\n      16\n      248851.000000\n      0.097600\n      0.718901\n      0.618376\n      0.002925\n      4.000000\n      0.075377\n      -7.715512\n      1\n      0.083494\n      96.831665\n      4\n      0.935569\n      0\n    \n    \n      17\n      17\n      153340.000000\n      0.012866\n      0.715635\n      0.796742\n      0.002236\n      6.000000\n      0.101808\n      -4.879090\n      0\n      0.172036\n      120.830046\n      3\n      0.497743\n      0\n    \n    \n      18\n      18\n      170983.000000\n      0.123631\n      0.524386\n      0.566983\n      nan\n      5.000000\n      nan\n      -7.312097\n      0\n      0.210055\n      114.609197\n      4\n      0.951705\n      1\n    \n    \n      19\n      19\n      266726.000000\n      0.021030\n      0.323277\n      nan\n      -0.000861\n      nan\n      0.239698\n      -12.692935\n      1\n      0.031522\n      124.811208\n      4\n      0.350090\n      1\n    \n  \n\n\n\nWe find:\n\nThere are missing values that can be seen as nan in the table above\nThe id column seems to have values in increasing order\nThe values in the features are in different scales\n\nNow, let’s look at some basic statistics about our features in the data\n\n\nCode\ndisplay(train.describe().style.set_caption(\"Basic statistics about Train Data\"))\n\n\n\n\n\n  Basic statistics about Train Data\n  \n    \n       \n      id\n      song_duration_ms\n      acousticness\n      danceability\n      energy\n      instrumentalness\n      key\n      liveness\n      loudness\n      audio_mode\n      speechiness\n      tempo\n      time_signature\n      audio_valence\n      song_popularity\n    \n  \n  \n    \n      count\n      40000.000000\n      35899.000000\n      36008.000000\n      35974.000000\n      36025.000000\n      36015.000000\n      35935.000000\n      35914.000000\n      36043.000000\n      40000.000000\n      40000.000000\n      40000.000000\n      40000.000000\n      40000.000000\n      40000.000000\n    \n    \n      mean\n      19999.500000\n      193165.847572\n      0.276404\n      0.570951\n      0.683932\n      0.036527\n      5.042605\n      0.198514\n      -7.407596\n      0.321150\n      0.094107\n      116.562815\n      3.394375\n      0.580645\n      0.364400\n    \n    \n      std\n      11547.149720\n      45822.127679\n      0.297928\n      0.190010\n      0.212662\n      0.150024\n      3.372728\n      0.151670\n      3.877198\n      0.466924\n      0.083591\n      26.167911\n      0.524405\n      0.237351\n      0.481268\n    \n    \n      min\n      0.000000\n      25658.000000\n      -0.013551\n      0.043961\n      -0.001682\n      -0.004398\n      0.000000\n      0.027843\n      -32.117911\n      0.000000\n      0.015065\n      62.055779\n      2.000000\n      0.013398\n      0.000000\n    \n    \n      25%\n      9999.750000\n      166254.500000\n      0.039618\n      0.424760\n      0.539276\n      0.000941\n      2.000000\n      0.111796\n      -9.578139\n      0.000000\n      0.038500\n      96.995309\n      3.000000\n      0.398669\n      0.000000\n    \n    \n      50%\n      19999.500000\n      186660.000000\n      0.140532\n      0.608234\n      0.704453\n      0.001974\n      5.000000\n      0.135945\n      -6.345413\n      0.000000\n      0.055881\n      113.795959\n      3.000000\n      0.598827\n      0.000000\n    \n    \n      75%\n      29999.250000\n      215116.000000\n      0.482499\n      0.718464\n      0.870503\n      0.003225\n      8.000000\n      0.212842\n      -4.620711\n      1.000000\n      0.118842\n      128.517383\n      4.000000\n      0.759635\n      1.000000\n    \n    \n      max\n      39999.000000\n      491671.000000\n      1.065284\n      0.957131\n      1.039741\n      1.075415\n      11.000000\n      1.065298\n      -0.877346\n      1.000000\n      0.560748\n      219.163578\n      5.000000\n      1.022558\n      1.000000\n    \n  \n\n\n\nWe find:\n\nMost of the features are in the range of 0 and 1\nThere are features with only negative values (loudness), binary features (audio_mode) , and seems to be categorical (key and time_signature)\n\n\n\n3.2. Missing data\nNow let’s take a closer look at the missing values in the dataset\n\n\nCode\n\"\"\"Missing Values\"\"\"\nprint(f\"Train set has {train.isnull().sum().sum()} missing values, and test set has {test.isnull().sum().sum()} missing values\")\n\n\nTrain set has 32187 missing values, and test set has 7962 missing values\n\n\n\n\nCode\n# Refrence (edited): https://datavizpyr.com/visualizing-missing-data-with-seaborn-heatmap-and-displot/\nfig = plt.figure(figsize=(18,6))\n\nsns.displot(\n    data=train.isna().melt(value_name=\"missing\"),\n    y=\"variable\",\n    hue=\"missing\",\n    multiple=\"fill\",\n    aspect=3\n)\nplt.title(\"Missing values shown using Bar plot\", fontsize=17)\nplt.ylabel(\"\")\nplt.xlabel(\"\")\n\nplt.figure(figsize=(18,10))\nsns.heatmap(train.isna().transpose())\nplt.title('Heatmap showing Missing Values in Train data', fontsize=17)\nplt.ylabel(\"\")\nplt.xlabel(\"\")\nplt.show()\n\n\n<Figure size 1296x432 with 0 Axes>\n\n\n\n\n\n\n\n\n\n\nCode\ntrain_null = train.isna().sum().sort_values(ascending = False)\ntest_null = test.isna().sum().sort_values(ascending = False)\n\nnon_zero_train_values = train_null[train_null.values > 0]\nnon_zero_test_values = test_null[test_null.values > 0]\n\nfig, axes = plt.subplots(1,2, figsize=(15,8))\nsns.barplot(y=non_zero_test_values.index , x=non_zero_test_values.values, ax=axes[1], palette = \"viridis\")\nsns.barplot(y=non_zero_train_values.index , x=non_zero_train_values.values, ax=axes[0], palette = \"viridis\")\naxes[0].set_title(\"Train data\", fontsize=14)\naxes[1].set_title(\"Test data\", fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/kaggle/song-popularity-eda.html#visualization---individual-features",
    "href": "posts/kaggle/song-popularity-eda.html#visualization---individual-features",
    "title": "Song Popularity EDA",
    "section": "4. Visualization - Individual Features",
    "text": "4. Visualization - Individual Features\nAfter getting an initial idea about our features and their values, we can now dive into the visual part of the exploration. I recommend to always plot your data. Sometimes this might be challenging, e.g. because you have tons of features. In that case, you want to start at least with a subset before you run any dimensionality reduction or other tools. This step is as much about spotting issues and irregularities as it is about learning more about the shapes and distributions of your features.\n\n4.1. Predictor variables\n\nIn the live session, we were building this plot step by step. (Well, we got most of the way there.) It really pays off to take the time and investigate each feature separately. This is one of the most instructive steps in the EDA process, where you aim to learn how messed up your features are. No dataset is perfect. We want to figure out how severe those imperfections are, and whether we can live with them or have to address them.\nDifferent kind of data types go best with different kind of visuals. My recommendation is to start out with density plots or histograms for numerical features, and with barcharts for those that are better expressed as types of categories.\n\n\n\nCode\nuseful_cols = [col for col in train.columns if col not in [\"id\", \"song_popularity\"]]\nnumeric_cols = [col for col in useful_cols if col not in [\"key\", \"audio_mode\", \"time_signature\"]]\n\nn_rows = 5\nn_cols = 3\nindex = 1\n\ncolors = [\"red\", \"darkblue\", \"green\"]\n\nfig = plt.figure(figsize=(16,20))\n\nfor index, col in enumerate(train[useful_cols].columns):\n    plt.subplot(n_rows,n_cols,index+1)\n    \n    if col in numeric_cols:\n        sns.kdeplot(train[col], color=random.sample(colors, 1), fill=True)\n        plt.title(col, fontsize=14)\n        plt.xlabel(\"\")\n        plt.ylabel(\"\")\n        plt.tight_layout()\n    else:\n        sns.countplot(train[col])\n        plt.title(col, fontsize=14)\n        plt.xlabel(\"\")\n        plt.ylabel(\"\")\n        plt.tight_layout()\n\nplt.subplot(n_rows,n_cols,14)\nsns.kdeplot(np.log(train['instrumentalness']), color=random.sample(colors, 1), fill=True)\nplt.title('instrumentalness (log transformed)', fontsize=14)\nplt.ylabel(\" \")\nplt.xlabel(\" \")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nWe find:\n\nOur initial impressions of the data types have largely been confirmed: audio_mode is a boolean feature, and time_signature and key are ordinal or categorical ones (or integer; although a better understanding of those musical concepts would certainly benefit from some domain knowledge.)\nA number of features are bounded between 0 and 1: accosticness, danceability, energy, liveliness, speechiness, and audio_valence.\nThe feature loudness looks like it refer to the decibel scale.\nThe distribution of instrumentalness is heavily right-skewed, and even after a log transform this feature doesn’t look very well-behaved. This might need a bit more work.\n\n\n\n4.2. Target: Song Popularity\nOn to the target itself. We figured out that song_popularity is a binary feature, and thus we can express it as boolean. Here we plot a barchart.\n\n\nCode\nsns.countplot(train.song_popularity.astype(\"bool\"))\nplt.title(\"Target: Song Popularity\", fontsize=14)\nplt.xlabel(\"\")\nplt.ylabel(\"\")\nplt.show()\n\n\n\n\n\nWe find:\n\nThere is a slight imbalance in the target distribution: a bit more than 60/40. Not super imbalanced, but something to keep in mind."
  },
  {
    "objectID": "posts/kaggle/song-popularity-eda.html#feature-interactions",
    "href": "posts/kaggle/song-popularity-eda.html#feature-interactions",
    "title": "Song Popularity EDA",
    "section": "5. Feature interactions",
    "text": "5. Feature interactions\nAfter learning more about each individual feature, we now want to see them interacting with one another. It’s best to perfom those steps in that order, so that you can understand and interpret the interactions in the context of the overall distributions.\n\n5.1. Target impact\nWe have seen all the feature distributions, now we want to investigate whether they look different based on the target value. Here’s an example for song_duration:\n\n\nCode\nfig = plt.figure(figsize=(16,18))\nn_rows = 4\nn_cols = 3\n\nfor index, col in enumerate(numeric_cols):\n    plt.subplot(n_rows, n_cols, index+1)\n    \n    sns.kdeplot(train[col], hue=train.song_popularity.astype(\"bool\"), fill=True)\n    plt.title(col, fontsize=14)\n    plt.xlabel(\"\")\n    plt.ylabel(\"\")\n    plt.tight_layout()\nplt.show()\n\n\n\n\n\nObservations:\n\nBy looking at the probability distribution of different variables we find that popular songs are almost exactly the same length as unpopular ones. There is a slight difference, but it’s pretty small.\n\nNow we can check the categorical features.\n\n\nCode\nfig = plt.figure(figsize=(18,5))\n\nfor index, col in enumerate([\"key\", \"audio_mode\", \"time_signature\"]):\n    plt.subplot(1,3,index+1)\n    \n    sns.countplot(train[col], hue=train.song_popularity.astype(\"bool\"))\n    plt.title(col, fontsize=14)\n    plt.xlabel(\"\")\n    plt.ylabel(\"\")\n    plt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n5.2. Feature Interaction\nHow do the predictor features interact with each other? Are there any redundancies or strong relationships? We will start out with a correlation matrix, and then look at features of interest in a bit more detail.\n\n5.2.1. Correlations overview\n\n\nCode\n# Refrence (edited): https://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec\ndef heatmap(data):\n    corr = pd.melt(data.reset_index(), id_vars='index') # Unpivot the dataframe, so we can get pair of arrays for x and y\n    corr.columns = ['x', 'y', 'value']\n    x=corr['x']\n    y=corr['y']\n    size=corr['value'].abs()\n    color=corr['value']\n    \n    fig, ax = plt.subplots(figsize=(10,10))\n    plot_grid = plt.GridSpec(1, 15, hspace=0.2, wspace=0.1) # Setup a 1x15 grid\n    ax = plt.subplot(plot_grid[:,:-1]) # Use the leftmost 14 columns of the grid for the main plot\n    \n    n_colors = 256 # Use 256 colors for the diverging color palette\n    palette = sns.diverging_palette(20, 220, n=n_colors) # Create the palette\n    color_min, color_max = [-1, 1] # Range of values that will be mapped to the palette, i.e. min and max possible correlation\n    size_min, size_max = 0, 1\n    \n    def value_to_color(val):\n        val_position = float((val - color_min)) / (color_max - color_min) # position of value in the input range, relative to the length of the input range\n        val_position = min(max(val_position, 0), 1) # bound the position betwen 0 and 1\n        ind = int(val_position * (n_colors - 1)) # target index in the color palette\n        return palette[ind]\n    \n    def value_to_size(val):\n        val_position = (val - size_min) * 0.99 / (size_max - size_min) + 0.01 # position of value in the input range, relative to the length of the input range\n        val_position = min(max(val_position, 0), 1) # bound the position betwen 0 and 1\n        return val_position * size_scale\n        \n    # Mapping from column names to integer coordinates\n    x_labels = [v for v in sorted(x.unique())]\n    y_labels = [v for v in sorted(y.unique())]\n    x_to_num = {p[1]:p[0] for p in enumerate(x_labels)} \n    y_to_num = {p[1]:p[0] for p in enumerate(y_labels)} \n    \n    size_scale = 500\n    ax.scatter(\n        x=x.map(x_to_num), # Use mapping for x\n        y=y.map(y_to_num), # Use mapping for y\n        s=size.apply(value_to_size), # Vector of square sizes, proportional to size parameter\n        c=color.apply(value_to_color), # Vector of square color values, mapped to color palette\n        marker='s' # Use square as scatterplot marker\n    )\n    \n    # Show column labels on the axes\n    ax.set_xticks([x_to_num[v] for v in x_labels])\n    ax.set_xticklabels(x_labels, rotation=45, horizontalalignment='right')\n    ax.set_yticks([y_to_num[v] for v in y_labels])\n    ax.set_yticklabels(y_labels)\n    \n    ax.grid(False, 'major')\n    ax.grid(True, 'minor')\n    ax.set_xticks([t + 0.5 for t in ax.get_xticks()], minor=True)\n    ax.set_yticks([t + 0.5 for t in ax.get_yticks()], minor=True)\n        \n    ax.set_xlim([-0.5, max([v for v in x_to_num.values()]) + 0.5])\n    ax.set_ylim([-0.5, max([v for v in y_to_num.values()]) + 0.5])\n    ax.set_facecolor('#F1F1F1')\n    \n    # Add color legend on the right side of the plot\n    ax = plt.subplot(plot_grid[:,-1]) # Use the rightmost column of the plot\n\n    col_x = [0]*len(palette) # Fixed x coordinate for the bars\n    bar_y=np.linspace(color_min, color_max, n_colors) # y coordinates for each of the n_colors bars\n\n    bar_height = bar_y[1] - bar_y[0]\n    ax.barh(\n        y=bar_y,\n        width=[5]*len(palette), # Make bars 5 units wide\n        left=col_x, # Make bars start at 0\n        height=bar_height,\n        color=palette,\n        linewidth=0\n    )\n    ax.set_xlim(1, 2) # Bars are going from 0 to 5, so lets crop the plot somewhere in the middle\n    ax.grid(False) # Hide grid\n    ax.set_facecolor('white') # Make background white\n    ax.set_xticks([]) # Remove horizontal ticks\n    ax.set_yticks(np.linspace(min(bar_y), max(bar_y), 3)) # Show vertical ticks for min, middle and max\n    ax.yaxis.tick_right() # Show vertical ticks on the right\n    \n\n\n\n\nCode\nheatmap(train[numeric_cols].corr())\n\n\n\n\n\nBelow is a similar correlation heatmap but only using the lower triangle to show the correlation.\n\n\nCode\n# Refrence (edited): https://medium.com/@szabo.bibor/how-to-create-a-seaborn-correlation-heatmap-in-python-834c0686b88e\n\nfig = plt.figure(figsize=(10,10))\nmatrix = np.triu(np.ones_like(train[numeric_cols].corr(), dtype=np.bool))\nsns.heatmap(train[numeric_cols].corr(), mask=matrix, vmin=-1, vmax=1, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))\nplt.show()\n\n\n\n\n\nWe find:\n\nThere’s a strong anti-correlation between acousticness vs energy and loudness, respectively. Consequently, energy and loudness share a strong correlation.\nNone of the features individually show a notable correlation with the target song_popularity.\n\n\n\n5.2.2. Categorical feature interactions\nWhenever we’re looking at categorical features, we can assign a visualisation dimension like colour, size, or facets to those. We will start modifying our trusted density plots to look at the distributions of energy (potentially one of the more interesting numerical features) for the different values of time_signature (here encoded as colour):\n\n\nCode\nfig = plt.figure(figsize=(10,8))\nsns.kdeplot(x=\"energy\", hue=\"time_signature\", data=train, fill=True, bw=0.03)\nplt.show()\n\n\n\n\n\n\n\nCode\n(ggplot(train, aes(\"key\", \"time_signature\", fill = \"energy\")) \n + geom_tile()\n + theme(figure_size=(16,5))\n + scale_x_continuous(breaks=range(0,12)))\n\n\n\n\n\n<ggplot: (8777952485729)>\n\n\nWe find:\n\nFor time_signatures 2 and 5 we have no instances of key == 11. This is no big surprise, since those three values are already rare individually, which makes their combinations even more rare.\nThere are no clear clusters of high vs low energy features here.\nWe can see certain combinations that are particularly low energy, such as key == 2 and time_signature == 1 or 8. key == 3 and time_signature == 1 seems to be a particularly energetic combination.\n\n\n\n\n5.3. Feature Target Interaction\nOnce we have found interesting correlations we can look for clustering in the target variable.\n\n\nCode\n(ggplot(train, aes('key', 'time_signature')) \n + geom_tile(aes(fill='energy')) \n + facet_wrap(\"song_popularity\", nrow = 2) \n + theme_minimal() \n + theme(figure_size=(16, 8))\n + scale_x_continuous(breaks=range(0,12)))\n\n\n\n\n\n<ggplot: (8777950648995)>\n\n\n\n\nCode\nsns.displot(data=train, x=\"energy\", y=\"audio_valence\", col=\"song_popularity\", kind=\"kde\", fill=True, legend=True, height=8, aspect=0.75)\nplt.show()\n\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize=(12,6))\nsns.scatterplot(x=\"energy\", y=\"acousticness\", hue=\"song_popularity\", data=train)\nplt.show()\n\n\n\n\n\n\n\nMore Resources:\n\nChoosing different color palette in Seaborn\nSee also\n\nPairGrid: Subplot grid for plotting pairwise relationships\nrelplot: Combine a relational plot and a FacetGrid\ndisplot: Combine a distribution plot and a FacetGrid\ncatplot: Combine a categorical plot and a FacetGrid\nlmplot: Combine a regression plot and a FacetGrid\n\n\nSpecial Thanks to Martin Henze for sharing his knowledge during the live coding session. Also, thank you Abhishek Thakur for hosting these wonderful sessions for people to learn. I look forward to learn more.\nShare if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "posts/mlops/introduction.html",
    "href": "posts/mlops/introduction.html",
    "title": "Introduction to MLOps",
    "section": "",
    "text": "For as long as I can remember, Jupyter Notebooks have been my go-to tool for machine learning (ML) projects. When it comes to experimentation, prototyping, and data exploration, Jupyter Notebooks undoubtedly reign supreme. However, automating and operationalizing ML products presents a significant challenge.\nThat’s where MLOps (Machine Learning Operations) comes into play. MLOps refers to a set of practices aimed at fostering collaboration and communication between data scientists and operations professionals. It bridges the gap between ML development and deployment, streamlining the process and ensuring efficient and scalable ML product deployment.\nSource: Neptune.ai"
  },
  {
    "objectID": "posts/mlops/introduction.html#drawbacks-on-working-with-jupyter-notebooks",
    "href": "posts/mlops/introduction.html#drawbacks-on-working-with-jupyter-notebooks",
    "title": "Introduction to MLOps",
    "section": "Drawbacks on working with Jupyter Notebooks",
    "text": "Drawbacks on working with Jupyter Notebooks\n\nLack of reproducibility: The flexibility to execute cells independently and out of order can make reproducing experiments challenging. It is crucial to carefully document the order of cell execution and ensure that all necessary cells are run to achieve reproducible results.\nIssues with version control: Jupyter Notebooks are not designed to work seamlessly with version control systems like Git. The JSON-based structure of notebooks makes it difficult to track changes, merge conflicts, and collaborate effectively when multiple team members are working on the same notebook simultaneously.\nLack of scalability: Jupyter Notebooks may face limitations in dealing with large datasets or computationally demanding tasks. Due to their single kernel architecture, they may struggle with memory restrictions and long execution times when working with big data or complex machine learning models.\nLimited code organization: While Jupyter Notebooks allow for code organization using cells and sections, larger notebooks can become challenging to navigate and maintain. As the size of the notebook grows, it can be harder to find specific sections of code, leading to reduced code readability and maintainability.\nPerformance limitations: Jupyter Notebooks excel in providing an interactive and exploratory environment, but they may not be the most performant option for computationally intensive tasks. For tasks requiring high-speed execution or efficient memory utilization, alternative tools like Python scripts or specialized frameworks may be more suitable."
  },
  {
    "objectID": "posts/mlops/introduction.html#devops-vs-mlops",
    "href": "posts/mlops/introduction.html#devops-vs-mlops",
    "title": "Introduction to MLOps",
    "section": "DevOps vs MLOps",
    "text": "DevOps vs MLOps\nDevOps principles have gained widespread acceptance in the software development industry due to their ability to integrate and automate software development and IT operations, leading to iterative improvements, fast feedback, collaboration, and automation. MLOps principles, derived from DevOps, aim to bring these best practices to the realm of machine learning and enable faster deployment of ML models into production.\nHowever, there are notable differences in how MLOps operates compared to DevOps:\n\nExperimental nature: MLOps involves extensive experimentation by data scientists and ML/DL engineers. They need to manage data and code to ensure reproducibility while experimenting with different features such as hyperparameters, parameters, and models. Reproducibility remains a challenge in the ML/DL sector, which sets it apart from the more deterministic nature of traditional software development in DevOps.\nHybrid team composition: MLOps teams have a hybrid structure that includes data scientists or ML researchers alongside software engineers. While data scientists focus on experimentation, model development, and exploratory data analysis, they may lack the expertise of software engineers in building production-ready services. This combination of skill sets is essential for successfully deploying ML models in production. `\nTesting: Testing in MLOps goes beyond conventional code tests like unit testing and integration testing. It encompasses model validation, model training, and other specific tasks associated with testing an ML system. The unique challenges of testing ML models require specialized techniques and frameworks to ensure model accuracy and reliability.\nAutomated deployment: Deploying an offline-trained ML model as a prediction service requires a multi-step pipeline in MLOps. Automating the tasks that data scientists manually perform before model deployment adds complexity to the process. It involves automating model retraining, validation, and deployment steps to ensure efficient and seamless deployment of updated models.\nProduction performance degradation and Training-Serving Skew: ML models in production can experience reduced performance due to changing data profiles or suboptimal coding. Unlike traditional software systems, ML models are sensitive to changes in data and require monitoring and adaptation to maintain optimal performance. Training-Serving Skew refers to discrepancies between how data is handled in the training and serving pipelines, which can further impact model performance.\nMonitoring: Monitoring is essential for ML models in production. It involves tracking the performance of deployed models and monitoring the summary statistics of the data used to build the models. Monitoring helps identify deviations from expected values, triggering alerts or initiating a roll-back process when necessary. Since data profiles and statistics can change over time, ongoing monitoring is critical for maintaining model effectiveness."
  },
  {
    "objectID": "posts/mlops/introduction.html#mlops-principles",
    "href": "posts/mlops/introduction.html#mlops-principles",
    "title": "Introduction to MLOps",
    "section": "MLOps Principles",
    "text": "MLOps Principles\nMLOps, which stands for Machine Learning Operations, encompasses a set of principles and practices aimed at streamlining the lifecycle of machine learning projects and promoting collaboration and communication between teams. Here are some key principles of MLOps:\n\nVersion control: Implementing version control systems, such as Git, enables tracking and management of changes to ML models, code, and data. It ensures reproducibility, facilitates collaboration and helps teams work together effectively.\nContinuous Integration and Continuous Deployment (CI/CD): MLOps encourages the use of CI/CD pipelines for automating the build, testing, and deployment of ML models. This iterative approach enables fast feedback, reduces errors, and accelerates the development and deployment process.\nInfrastructure as Code (IaC): MLOps embraces the concept of treating infrastructure as code using tools like Terraform or AWS CloudFormation. By defining infrastructure configurations using code, teams can easily manage and version control their cloud infrastructure. IaC enables reproducibility by providing a consistent and automated way to provision and manage resources in the cloud. It also facilitates scalability, allowing teams to easily scale up or down their infrastructure as needed, ensuring efficient and cost-effective deployments for ML models.\nModel Monitoring: Monitoring ML models in production is vital to detect performance issues, data drift, and anomalies. Dedicated monitoring tools help track model performance, identify deviations, and trigger alerts for timely updates and maintenance.\nCollaboration and Communication: Effective collaboration and communication between data scientists, engineers, and operations teams are critical in MLOps. Sharing knowledge, documenting processes, and fostering a collaborative culture enhance team productivity and ensure the successful delivery of ML projects.\nAutomated Testing: Implementing automated testing frameworks ensures the quality and reliability of ML models. This includes unit tests, integration tests, and performance tests that validate model behavior and catch potential issues early in the development process.\n\n Source: End-to-End Machine Learning Platforms By Ian Hellström"
  },
  {
    "objectID": "posts/mlops/introduction.html#mlops-maturity-model",
    "href": "posts/mlops/introduction.html#mlops-maturity-model",
    "title": "Introduction to MLOps",
    "section": "MLOps maturity model",
    "text": "MLOps maturity model\nThe MLOps maturity model represents the level of proficiency and scalability in managing and operationalizing machine learning (ML) systems within an organization. It illustrates how effectively the company can develop, implement, monitor, and maintain ML models. The stages of MLOps maturity may vary depending on the framework or model used, but they generally progress as follows:\n\nLevel 0: No MLOps\n\nManaging the complete lifecycle of ML models is challenging.\nTeams are diverse, and releases are cumbersome.\nLack of transparency and feedback from deployed models.\n\nLevel 1: DevOps but no MLOps\n\nReleases are less cumbersome compared to Level 0 but still rely heavily on the Data Team for each new model.\nLimited feedback on model performance in production.\nDifficulties in tracing and reproducing results.\n\nLevel 2: Automated Training\n\nThe training environment is fully managed and traceable.\nModels can be easily reproduced.\nReleases are performed manually but with reduced friction.\n\nLevel 3: Automated Model Deployment\n\nReleases are automated and have low friction.\nFull traceability from deployment back to the original data.\nThe entire environment is managed, including training, testing, and production stages.\n\nLevel 4: Full MLOps Automated Operations\n\nThe entire system is automated and easily monitored.\nProduction systems provide insights for continuous improvement and can automatically incorporate new models.\nApproaching a zero-downtime system with high availability."
  },
  {
    "objectID": "posts/mlops/introduction.html#bonus-reading-materials",
    "href": "posts/mlops/introduction.html#bonus-reading-materials",
    "title": "Introduction to MLOps",
    "section": "Bonus Reading Materials",
    "text": "Bonus Reading Materials\n\nMLOps Maturity Model - Azure\nMade with ML - By Goku Mohandas\nMLOps Primer\n\nThank you for reading and I hope you found this notebook helpful. Upvote if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "posts/mlops/aws_setup.html",
    "href": "posts/mlops/aws_setup.html",
    "title": "AWS Instance Setup",
    "section": "",
    "text": "Welcome to our blog post where we’ll guide you through setting up your AWS instance and installing all the necessary requirements for your work. We understand the importance of a smooth and efficient setup process, so we’ll provide step-by-step instructions to ensure you have everything you need to get started.\nLet’s dive in and get your environment up and running seamlessly!"
  },
  {
    "objectID": "posts/mlops/aws_setup.html#setup-aws-instance",
    "href": "posts/mlops/aws_setup.html#setup-aws-instance",
    "title": "AWS Instance Setup",
    "section": "Setup AWS Instance",
    "text": "Setup AWS Instance\n\nGo to https://aws.amazon.com to Sign in / Create an AWS Account.\nTo launch EC2 instance, click on to services on the left-top corner of the page. Select Compute and EC2.\n\n\n\nTo launch a new instance, click on Launch Instance.\n\n\n\nSet any Name to the instance and select Ubuntu in the Application and OS Images section. Also, choose Ubuntu Server 20.04 LTS (HVM), SSD Volume Type as the Amazon Machine Image (AMI).\n\n\n\nSelect t2.xlarge as the Instance type for our instance. As 16GiB of memory should be ideal for our work.\n\n\n\nIf you don’t already have a Key pair, you can create a new key pair. You would be asked to download and save your key pair.\n\n\n\n\n\n\n\nTip\n\n\n\nSave your key pair at ~/.ssh/ folder.\n\n\n\n\n\n\nLastly, increase the storage to 30 GiB as we would be working with large file and docker images with would consume some space.\n\n\n\nClick on Launch Instance to create and start the new instance."
  },
  {
    "objectID": "posts/mlops/aws_setup.html#connect-to-instance-via-ssh",
    "href": "posts/mlops/aws_setup.html#connect-to-instance-via-ssh",
    "title": "AWS Instance Setup",
    "section": "Connect to Instance via SSH",
    "text": "Connect to Instance via SSH\nOnce your instance is running, you can go to the instance summary page to checkout your Public IPv4 address which will be used to connect to the instance via ssh.\nTo check whether you can establish a connection to your instance:\nssh -i ~/.ssh/mlops-zc-key.pem ubuntu@<your-public-ipv4-address>\nEg. ssh -i ~/.ssh/mlops-zc-key ubuntu@34.236.146.20\n\n\n\n\n\n\nbad permissions error\n\n\n\nIf you receive an error like:\nIt is required that your private key files are NOT accessible by others. This private key will be ignored.\nChange the file permission using the command:\nchmod go-r ~/.ssh/mlops-zc-key.pem\n\n\nIf asked about “Are you sure you want to continue connecting (yes/no/[fingerprint])?”, type yes.\nRather than manually entering the whole command, you can save the configurations at ~/.ssh/config file. If you don’t already have an file name config in the ~/.ssh/ directory then go to your .ssh directory and use the command touch config to create the file.\nTo edit the file use the command vim ~/.ssh/config.\nIn the file add the following details:\nHost mlops-zoomcamp\n    HostName <your-public-ipv4-address>\n    User ubuntu\n    IdentityFile ~/.ssh/mlops-zc-key.pem\n\n\n\n\n\n\nCaution\n\n\n\nEvery time you stop and start your instance you would have to edit the config file and change the public ipv4 address with the new ipv4 address."
  },
  {
    "objectID": "posts/mlops/aws_setup.html#install-softwares",
    "href": "posts/mlops/aws_setup.html#install-softwares",
    "title": "AWS Instance Setup",
    "section": "Install Softwares",
    "text": "Install Softwares\n\nPython\n\nCreate a directory called downloads and move into the directory.\n\nmkdir downloads\ncd downloads\n\nDownload and Install Anaconda\n\nwget https://repo.anaconda.com/archive/Anaconda3-2023.03-1-Linux-x86_64.sh\nbash Anaconda3-2023.03-1-Linux-x86_64.sh\n\nFor the following prompts:\n\nAnaconda3 will now be installed into this location:\n/home/ubuntu/anaconda3\nPress Enter\nDo you wish the installer to initialize Anaconda3\nby running conda init? [yes|no]\n[no] >>>\nType yes\nPython installed 🎉\n\n\nDocker\n\nUpdate packages using the command\n\nsudo apt update\n\nInstall Docker\n\nsudo apt install docker.io\n\nCheck docker is installed by running hello-world program in docker:\n\nsudo docker run hello-world\nWe don’t want to use sudo everytime we run docker. To do that, create the docker group and add your user:\n\nCreate the docker group.\n\nsudo groupadd docker\n\nAdd your user to the docker group.\n\nsudo usermod -aG docker $USER\n\nLog out and log back in so that your group membership is re-evaluated.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you’re running Linux in a virtual machine, it may be necessary to restart the virtual machine for changes to take effect.\n\n\n\nTo check if you can run docker without using sudo, use the command\n\ndocker --help\nDocker installed 🎉\n\n\ndocker-compose\n\nMove to the home directory. It will look like\n\nubuntu@ip-172-31-19-228:~/downloads$ cd ..\nubuntu@ip-172-31-19-228:~$ ls\nanaconda3 downloads\n\nCreate a folder named soft and move into the folder.\n\nmkdir soft\ncd soft\n\nDownload docker-compose and make the file executable\n\nwget https://github.com/docker/compose/releases/download/v2.18.1/docker-compose-linux-x86_64 -O docker-compose\n\nchmod +x docker-compose\nTo access docker-compose from any location we need to add the PATH to the bashrc file. Again move to the home directory.\n\nOpen the .bashrc file using the command\n\nvim .bashrc\n\nMove to the end of the file and press i to go into Insert Mode.\nAdd the following peice of code in the file.\n\nexport PATH=\"${HOME}/soft:${PATH}\"\n\nSave and Exit the file:\n\nPress esc key\nType :wq and hit enter\n\n\n\nRun the command to execute the file:\n\nsource .bashrc\n\nCross-check by typing which docker-compose. You’ll get the output as /home/ubuntu/soft/docker-compose\n\ndocker-compose installed 🎉\n\n\n\n\n\n\nSTOP EC2 instance\n\n\n\nPlease remember to stop the EC2 instance after completing your work to avoid incurring any additional charges.\n\n\nThank you for reading and I hope you found this notebook helpful. Upvote if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "posts/ml/machine-learning-expert-path.html",
    "href": "posts/ml/machine-learning-expert-path.html",
    "title": "Path to become a Machine Learning Expert",
    "section": "",
    "text": "Path to becoming a Machine Learning (ML) Expert made easy. There are a lot of resources out there that can be overwhelming at the start. But don’t worry this learning path would provide structure and lay the foundational knowledge to begin a career in ML."
  },
  {
    "objectID": "posts/ml/machine-learning-expert-path.html#learn-the-basics-of-descriptive-statistics-inferential-statistics-and-math-used-in-machine-learning",
    "href": "posts/ml/machine-learning-expert-path.html#learn-the-basics-of-descriptive-statistics-inferential-statistics-and-math-used-in-machine-learning",
    "title": "Path to become a Machine Learning Expert",
    "section": "1. Learn the basics of Descriptive Statistics, Inferential Statistics and Math used in Machine Learning",
    "text": "1. Learn the basics of Descriptive Statistics, Inferential Statistics and Math used in Machine Learning\nUnderstanding the math used in ML can help in building the foundation strong. Udacity offers courses on descriptive statistics and inferential statistics. These courses are free and use excel to teach the concepts.\nAlong with statistics and probabilities, concepts on linear algebra, multivariate calculus, optimization functions and many more form the building blocks for ML. There is an awesome youtube channel that makes these concepts very easy to learn. 3Brown1Blue focuses on teaching mathematics using a distinct visual perspective.\nMore resources:\n\nComputational Linear Algebra for Coders\nProf. Gilbert Strang’s Linear Algebra book/course\nMatrix Cookbook by Kaare Brandt Petersen & Michael Syskind Pedersen\nThink Stats (Exploratory Data Analysis in Python) by Allen Downey\nConvex Optimization by Stephen Boyd and Lieven Vandenberghe\nEssentials of Metaheuristics by Sean Luke"
  },
  {
    "objectID": "posts/ml/machine-learning-expert-path.html#learn-the-basics-of-python-and-its-packages",
    "href": "posts/ml/machine-learning-expert-path.html#learn-the-basics-of-python-and-its-packages",
    "title": "Path to become a Machine Learning Expert",
    "section": "2. Learn the basics of Python and it’s packages",
    "text": "2. Learn the basics of Python and it’s packages\nFirst, let’s install Python. The easiest way to do this is by installing Anaconda. All the packages that are required come along with Anaconda.\nYou can start from learning the basics of Python i.e. data structures, functions, class, etc. and it’s libraries. I started learning about python in my college days, I read the book Learn Python the Hard Way. A very good book for beginners. Introduction to Python Programming by Udacity is a free course that covers the basics of Python. Introduction to Python is another free course by Analytics Vidhya. Another free course by Google is Google’s Python Class.\nNext, learn about how to use Regular Expression (also called regex) in Python. It will come in use for data cleaning, especially if you are working with text data. Learn regular expressions through Google class. A very good beginner tutorial for learning regular expression in python on Analytics Vidhya. Cheatsheet for Regex.\nNow comes the fun part of learning the various libraries in Python. Numpy, Pandas, Matplotlib, Seaborn, and Sklearn are the packages heavily used in ML.\n\nNumpy provides a high-performance multidimensional array and basic tools to compute with and manipulate these arrays. Numpy quickstart tutorial is a good place to start. This will form a good foundation for this to come. Practice numpy by solving 100 numpy exercises to solve.\nPandas is used for data manipulation and analysis. The most used package in Python is Pandas. Intro to pandas data structure provides a detailed tutorial on pandas. A short course by Kaggle on pandas.\nMatplotlib is a visualization library in python. In the matplotlib tutorial, you will learn the basics of Python data visualization, the anatomy of a Matplotlib plot, and much more. Official documentation of matplotlib is one of the best ways to learn the library.\nSeaborn is another visualization library built on top of matplotlib. Kaggle short course on data visualization provides a good start point to learn the library."
  },
  {
    "objectID": "posts/ml/machine-learning-expert-path.html#data-explorationcleaningpreparation",
    "href": "posts/ml/machine-learning-expert-path.html#data-explorationcleaningpreparation",
    "title": "Path to become a Machine Learning Expert",
    "section": "3. Data Exploration/Cleaning/Preparation",
    "text": "3. Data Exploration/Cleaning/Preparation\nReal-world data is unstructured, contains missing values, outliers, typos, etc. This step is one of the most important steps for a data analyst to perform because how good the model will perform will depend on the quality of the data.\nLearn different stages of data explorations:\n\nVariable Identification, Univariate and Multivariate analysis\nMissing values treatment\nOutlier treatment\nFeature Engineering\n\nAdditional resources:\n\nYou can also refer to the data exploration guide.\nBook on Python for Data Analysis by Wes McKinney"
  },
  {
    "objectID": "posts/ml/machine-learning-expert-path.html#introduction-to-machine-learning",
    "href": "posts/ml/machine-learning-expert-path.html#introduction-to-machine-learning",
    "title": "Path to become a Machine Learning Expert",
    "section": "4. Introduction to Machine Learning",
    "text": "4. Introduction to Machine Learning\nNow it’s time to enter the belly of the beast. There are various resources to learn ML and I would suggest the following courses:\n\nMachine Learning by Stanford (Coursera) The Machine Learning course by Andrew Ng is one of the best courses out there and covers all the basic algorithms. Also, it introduces all the advanced topics in a very simple manner which is easy to understand. However, this course is taught in Octave rather than the popular languages like R/Python. Also, this course is NOT free but you can apply for financial aid.\nMachine Learning A-Z™: Hands-On Python & R In Data Science (Udemy) Good course for beginners. Explore complex topics such as natural language processing (NLP), reinforcement learning (RL), deep learning (DL) among many others. Tons of practice exercise and quizzes. This course is NOT free but comparatively not expensive.\nMachine Learning (edx) This is an advanced course that has the highest math prerequisite out of any other course in this list. You’ll need a very firm grasp of Linear Algebra, Calculus, Probability, and programming. This course is free of cost but to acquire a certificate payment is required.\nComprehensive learning path for Data Science (Analytics Vidhya) This course covers every topic right from the beginning. Installing Python, data cleaning and preparation, Machine learning concepts, deep learning, and NLP. This course is free and does not come with any certification.\n\nBooks:\n\nThe Hundred Page Machine Learning Book\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n\nList of best books for machine learning.\nAfter learning about the various techniques in ML the next natural thing to do is apply those techniques. What better place than Kaggle. It is one of the most popular websites among data science enthusiasts. Below two problem statement can be a good starting problem statement to begin with.\n\nTitanic: Machine Learning from Disaster\nHouse Prices: Advanced Regression Techniques"
  },
  {
    "objectID": "posts/ml/machine-learning-expert-path.html#deep-learning",
    "href": "posts/ml/machine-learning-expert-path.html#deep-learning",
    "title": "Path to become a Machine Learning Expert",
    "section": "5. Deep Learning",
    "text": "5. Deep Learning\nUsing the idea to mimic a human brain has been around since the 1900s. There were various algorithms and techniques developed for the same but due to the lack of computing power, it was difficult to run those algorithms.\nDue to the improvements in the hardware and the introduction to using GPUs to compute caught the attention of people passionate about working on neural net-based models. Today, state of the art results can be obtained using deep neural networks.\nCourses from deeplearning.ai on Coursera are one of the most popular and fantastic courses on deep learning.\n\nNeural Networks and Deep Learning\nDeep Learning Specialization\n\nBoth the courses are paid but financial aid is available for both of them.\nAdditional Resources:\n\nDeep Learning Summer School, Montreal 2015\nDeep Learning for Perception, Virginia Tech, Electrical, and Computer Engineering\nCS231N 2017\nA blog that explains concepts on Convolutional Neural Nets (CNN)\n(Book) Deep Learning – Methods and Applications\n(Youtube Channel) DeepLearning.TV\nDeep Learning book from MIT\nNeural Networks and Deep Learning online Book\nComprehensive resources on deeplearning.net"
  },
  {
    "objectID": "posts/ml/machine-learning-expert-path.html#natural-language-processing",
    "href": "posts/ml/machine-learning-expert-path.html#natural-language-processing",
    "title": "Path to become a Machine Learning Expert",
    "section": "6. Natural Language Processing",
    "text": "6. Natural Language Processing\nNatural language processing (NLP) is a branch of Artificial Intelligence which aims to bridge the gap between how a computer and human communicate with each other. The two major handles used for communication are speech and written i.e. text. If you are unfamiliar with what NLP is, this blog could help in understanding what NLP is.\nCourses:\n\n(Youtube) Natural Language Processing by University of Michigan\nSpeech and Language Processing\nStanford CS224N: NLP with Deep Learning Winter 2019 – Stanford\nLecture Collection on Natural Language Processing with Deep Learning (Winter 2017) – Stanford\nCS224d: Deep Learning for Natural Language Processing – Stanford\nNatural Language Processing Specialization offered by deeplearning.ai on Coursera (Intermediate level)\nNatural Language Processing offered by National Research University Higher School of Economics on Coursera (Advanced level course)\n\nMachine Learning in itself is a huge domain and the only way to master it is to explore and practice. I cannot stress more on practice because without practice is like trying to play the guitar without any strings.\nPopular blogs to follow:\n\nAnalytics Vidhya\nMachine Learning Mastery\nTowards Data Science\nKDnuggets\n\nAdditional Resources:\n\nA Complete Python Tutorial to Learn Data Science from Scratch\nA Comprehensive Learning Path for Deep Learning in 2019 on Analytics Vidhya\nLearning Path to Master Computer Vision in 2020 on Analytics Vidhya\nA Comprehensive Learning Path to Understand and Master NLP in 2020 on Analytics Vidhya\nA Comprehensive Guide to Understand and Implement Text Classification in Python on Analytics Vidhya\nCollection of datasets for NLP\nA comprehensive Learning path to becoming a data scientist in 2020 free course on Analytics Vidhya\n\nI wish you all the best on your journey to becoming a machine learning expert.\nShare if you like it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html",
    "href": "posts/nlp/text-preprocessing.html",
    "title": "Text Preprocessing",
    "section": "",
    "text": "In any machine learning task, cleaning and pre-processing of the data is a very important step. The better we can represent our data, the better the model training and prediction can be expected.\nSpecially in the domain of Natural Language Processing (NLP) the data is unstructured. It become crucial to clean and properly format it based on the task at hand. There are various pre-processing steps that can be performed but not necessary to perform all. These steps should be applied based on the problem statement.\nExample: Sentiment analysis on twitter data can required to remove hashtags, emoticons, etc. but this may not be the case if we are doing the same analysis on customer feedback data.\nHere we are using the twitter_sample dataset from the nltk library."
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#lower-casing",
    "href": "posts/nlp/text-preprocessing.html#lower-casing",
    "title": "Text Preprocessing",
    "section": "Lower Casing",
    "text": "Lower Casing\nLowercasing is a common text preprocessing technique. It helps to transform all the text in same case.  Examples ‘The’, ‘the’, ‘ThE’ -> ‘the’\nThis is also useful to find all the duplicates since words in different cases are treated as separate words and becomes difficult for us to remove redundant words in all different case combination.\nThis may not be helpful when we do tasks like Part of Speech tagging (where proper casing gives some information about Nouns and so on) and Sentiment Analysis (where upper casing refers to anger and so on)\n\ndf.text = df.text.str.lower()\ndf.head(2)\n\n\n\n\n\n  \n    \n      \n      text\n      label\n    \n  \n  \n    \n      0\n      username changed! :d\n      1\n    \n    \n      1\n      @kimtaaeyeonss unnieeee!!!:)\n      1"
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#remove-redundant-features",
    "href": "posts/nlp/text-preprocessing.html#remove-redundant-features",
    "title": "Text Preprocessing",
    "section": "Remove Redundant Features",
    "text": "Remove Redundant Features\n\n\n\n\n\n\nNote\n\n\n\nNote: How you define redundant features varies based on the problem statement.\n\n\n\nURL’s\nURL stands for Uniform Resource Locator. If present in a text, it represents the location of another website.\nIf we are performing any websites backlink analysis, in that case URL’s are useful to keep. Otherwise, they don’t provide any information. So we can remove them from our text.\n\ndf.text = df.text.str.replace(r'https?://\\S+|www\\.\\S+', '', regex=True)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      text\n      label\n    \n  \n  \n    \n      0\n      username changed! :d\n      1\n    \n    \n      1\n      @kimtaaeyeonss unnieeee!!!:)\n      1\n    \n    \n      2\n      @amyewest thanks! i hope you've got a good book to keep you company. :-)\n      1\n    \n    \n      3\n      :) where are you situated? @hijay09\n      1\n    \n    \n      4\n      @egaroo you're welcome, i'm glad you liked it :)\n      1\n    \n  \n\n\n\n\n\n\nE-mail\nE-mail id’s are common in customer feedback data and they do not provide any useful information. So we remove them from the text.\nTwitter data that we are using does not contain any email id’s. Hence, please find the code snipper with an dummy example to remove e-mail id’s.\n\ntext = 'I have being trying to contact xyz via email to xyz@abc.co.in but there is no response.'\nre.sub(r'\\S+@\\S+', '', text)\n\n'I have being trying to contact xyz via email to  but there is no response.'\n\n\n\n\nDate\nDates can be represented in various formats and can be difficult at times to remove them. They are unlikely to contain any useful information for predicting the labels.\nBelow I have used dummy text to showcase the following task.\n\ntext = \"Today is 22/12/2020 and after two days on 24-12-2020 our vacation starts until 25th.09.2021\"\n\n# 1. Remove date formats like: dd/mm/yy(yy), dd-mm-yy(yy), dd(st|nd|rd).mm/yy(yy)\nre.sub(r'\\d{1,2}(st|nd|rd|th)?[-./]\\d{1,2}[-./]\\d{2,4}', '', text)\n\n'Today is  and after two days on  our vacation starts until '\n\n\n\ntext = \"Today is 11th of January, 2021 when I am writing this post. I hope to post this by February 15th or max to max by 20 may 21 or 20th-December-21\"\n\n# 2. Remove date formats like: 20 apr 21, April 15th, 11th of April, 2021\npattern = re.compile(\n    r'(\\d{1,2})?(st|nd|rd|th)?[-./,]?\\s?(of)?\\s?([J|j]an(uary)?|[F|f]eb(ruary)?|[Mm]ar(ch)?|[Aa]pr(il)?|[Mm]ay|[Jj]un(e)?|[Jj]ul(y)?|[Aa]ug(ust)?|[Ss]ep(tember)?|[Oo]ct(ober)?|[Nn]ov(ember)?|[Dd]ec(ember)?)\\s?(\\d{1,2})?(st|nd|rd|th)?\\s?[-./,]?\\s?(\\d{2,4})?')\npattern.sub(r'', text)\n\n'Today is  when I am writing this post. I hope to post this byor max to max by or '\n\n\nThere are various formats in which dates are represented and the above regex can be customized in many ways. Above, “byor” got combined cause we are trying multiple format in single regex pattern. You can customize the above expression accordingly to your need.\n\n\nHTML Tags\nIf we are extracting data from various websites, it is possible that the data also contains HTML tags. These tags does not provide any information and should be removed. These tags can be removed using regex or by using BeautifulSoup library.\n\n# Dummy text\ntext = \"\"\"\n<title>Below is a dummy html code.</title>\n<body>\n    <p>All the html opening and closing brackets should be remove.</p>\n    <a href=\"https://www.abc.com\">Company Site</a>\n</body>\n\"\"\"\n\n\n# Using regex to remove html tags\npattern = re.compile('<.*?>')\npattern.sub('', text)\n\n'\\nBelow is a dummy html code.\\n\\n    All the html opening and closing brackets should be remove.\\n    Company Site\\n\\n'\n\n\n\n# Using Beautiful Soup\ndef remove_html(text):\n    clean_text = BeautifulSoup(text).get_text()\n    return clean_text\n\n\nremove_html(text)\n\n'\\nBelow is a dummy html code.\\n\\nAll the html opening and closing brackets should be remove.\\nCompany Site\\n\\n'\n\n\n\n\nEmojis\nAs more and more people have started using social media emoji’s play a very crucial role. Emoji’s are used to express emotions that are universally understood.\nIn some analysis such as sentiment analysis emoji’s can be useful. We can convert them to words or create some new features based on them. For some analysis we need to remove them. Find the below code snippet used to remove the emoji’s.\n\n\nCode\n# Reference: https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               u\"\\U0001f926-\\U0001f937\"\n                               u\"\\U00010000-\\U0010ffff\"\n                               u\"\\u2640-\\u2642\"\n                               u\"\\u2600-\\u2B55\"\n                               u\"\\u200d\"\n                               u\"\\u23cf\"\n                               u\"\\u23e9\"\n                               u\"\\u231a\"\n                               u\"\\ufe0f\"  # dingbats\n                               u\"\\u3030\"\n                               \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\n\ntext = \"game is on 🔥🔥. Hilarious😂\"\nremove_emoji(text)\n\n'game is on . Hilarious'\n\n\n\n# Remove emoji's from text\ndf.text = df.text.apply(lambda x: remove_emoji(x))\n\n\n\nEmoticons\nEmoji’s and Emoticons are different. Yes!! Emoticons are used to express facial expressions using keyboard characters such as letters, numbers, and pucntuation marks. Where emjoi’s are small images.\nThanks to Neel Shah for curating a dictionary of emoticons and their description. We shall use this dictionary and remove the emoticons from our text.\n\n\nCode\nEMOTICONS = {\n    u\":‑\\)\": \"Happy face or smiley\",\n    u\":\\)\": \"Happy face or smiley\",\n    u\":-\\]\": \"Happy face or smiley\",\n    u\":\\]\": \"Happy face or smiley\",\n    u\":-3\": \"Happy face smiley\",\n    u\":3\": \"Happy face smiley\",\n    u\":->\": \"Happy face smiley\",\n    u\":>\": \"Happy face smiley\",\n    u\"8-\\)\": \"Happy face smiley\",\n    u\":o\\)\": \"Happy face smiley\",\n    u\":-\\}\": \"Happy face smiley\",\n    u\":\\}\": \"Happy face smiley\",\n    u\":-\\)\": \"Happy face smiley\",\n    u\":c\\)\": \"Happy face smiley\",\n    u\":\\^\\)\": \"Happy face smiley\",\n    u\"=\\]\": \"Happy face smiley\",\n    u\"=\\)\": \"Happy face smiley\",\n    u\":‑D\": \"Laughing, big grin or laugh with glasses\",\n    u\":D\": \"Laughing, big grin or laugh with glasses\",\n    u\"8‑D\": \"Laughing, big grin or laugh with glasses\",\n    u\"8D\": \"Laughing, big grin or laugh with glasses\",\n    u\"X‑D\": \"Laughing, big grin or laugh with glasses\",\n    u\"XD\": \"Laughing, big grin or laugh with glasses\",\n    u\"=D\": \"Laughing, big grin or laugh with glasses\",\n    u\"=3\": \"Laughing, big grin or laugh with glasses\",\n    u\"B\\^D\": \"Laughing, big grin or laugh with glasses\",\n    u\":-\\)\\)\": \"Very happy\",\n    u\":‑\\(\": \"Frown, sad, andry or pouting\",\n    u\":-\\(\": \"Frown, sad, andry or pouting\",\n    u\":\\(\": \"Frown, sad, andry or pouting\",\n    u\":‑c\": \"Frown, sad, andry or pouting\",\n    u\":c\": \"Frown, sad, andry or pouting\",\n    u\":‑<\": \"Frown, sad, andry or pouting\",\n    u\":<\": \"Frown, sad, andry or pouting\",\n    u\":‑\\[\": \"Frown, sad, andry or pouting\",\n    u\":\\[\": \"Frown, sad, andry or pouting\",\n    u\":-\\|\\|\": \"Frown, sad, andry or pouting\",\n    u\">:\\[\": \"Frown, sad, andry or pouting\",\n    u\":\\{\": \"Frown, sad, andry or pouting\",\n    u\":@\": \"Frown, sad, andry or pouting\",\n    u\">:\\(\": \"Frown, sad, andry or pouting\",\n    u\":'‑\\(\": \"Crying\",\n    u\":'\\(\": \"Crying\",\n    u\":'‑\\)\": \"Tears of happiness\",\n    u\":'\\)\": \"Tears of happiness\",\n    u\"D‑':\": \"Horror\",\n    u\"D:<\": \"Disgust\",\n    u\"D:\": \"Sadness\",\n    u\"D8\": \"Great dismay\",\n    u\"D;\": \"Great dismay\",\n    u\"D=\": \"Great dismay\",\n    u\"DX\": \"Great dismay\",\n    u\":‑O\": \"Surprise\",\n    u\":O\": \"Surprise\",\n    u\":‑o\": \"Surprise\",\n    u\":o\": \"Surprise\",\n    u\":-0\": \"Shock\",\n    u\"8‑0\": \"Yawn\",\n    u\">:O\": \"Yawn\",\n    u\":-\\*\": \"Kiss\",\n    u\":\\*\": \"Kiss\",\n    u\":X\": \"Kiss\",\n    u\";‑\\)\": \"Wink or smirk\",\n    u\";\\)\": \"Wink or smirk\",\n    u\"\\*-\\)\": \"Wink or smirk\",\n    u\"\\*\\)\": \"Wink or smirk\",\n    u\";‑\\]\": \"Wink or smirk\",\n    u\";\\]\": \"Wink or smirk\",\n    u\";\\^\\)\": \"Wink or smirk\",\n    u\":‑,\": \"Wink or smirk\",\n    u\";D\": \"Wink or smirk\",\n    u\":‑P\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":P\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"X‑P\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"XP\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":‑Þ\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":Þ\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":b\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"d:\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"=p\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\">:P\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":‑/\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":/\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":-[.]\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:[(\\\\\\)]\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:/\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":[(\\\\\\)]\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=/\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=[(\\\\\\)]\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":L\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=L\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":S\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":‑\\|\": \"Straight face\",\n    u\":\\|\": \"Straight face\",\n    u\":$\": \"Embarrassed or blushing\",\n    u\":‑x\": \"Sealed lips or wearing braces or tongue-tied\",\n    u\":x\": \"Sealed lips or wearing braces or tongue-tied\",\n    u\":‑#\": \"Sealed lips or wearing braces or tongue-tied\",\n    u\":#\": \"Sealed lips or wearing braces or tongue-tied\",\n    u\":‑&\": \"Sealed lips or wearing braces or tongue-tied\",\n    u\":&\": \"Sealed lips or wearing braces or tongue-tied\",\n    u\"O:‑\\)\": \"Angel, saint or innocent\",\n    u\"O:\\)\": \"Angel, saint or innocent\",\n    u\"0:‑3\": \"Angel, saint or innocent\",\n    u\"0:3\": \"Angel, saint or innocent\",\n    u\"0:‑\\)\": \"Angel, saint or innocent\",\n    u\"0:\\)\": \"Angel, saint or innocent\",\n    u\":‑b\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"0;\\^\\)\": \"Angel, saint or innocent\",\n    u\">:‑\\)\": \"Evil or devilish\",\n    u\">:\\)\": \"Evil or devilish\",\n    u\"\\}:‑\\)\": \"Evil or devilish\",\n    u\"\\}:\\)\": \"Evil or devilish\",\n    u\"3:‑\\)\": \"Evil or devilish\",\n    u\"3:\\)\": \"Evil or devilish\",\n    u\">;\\)\": \"Evil or devilish\",\n    u\"\\|;‑\\)\": \"Cool\",\n    u\"\\|‑O\": \"Bored\",\n    u\":‑J\": \"Tongue-in-cheek\",\n    u\"#‑\\)\": \"Party all night\",\n    u\"%‑\\)\": \"Drunk or confused\",\n    u\"%\\)\": \"Drunk or confused\",\n    u\":-###..\": \"Being sick\",\n    u\":###..\": \"Being sick\",\n    u\"<:‑\\|\": \"Dump\",\n    u\"\\(>_<\\)\": \"Troubled\",\n    u\"\\(>_<\\)>\": \"Troubled\",\n    u\"\\(';'\\)\": \"Baby\",\n    u\"\\(\\^\\^>``\": \"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(\\^_\\^;\\)\": \"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-;\\)\": \"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(~_~;\\) \\(・\\.・;\\)\": \"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-\\)zzz\": \"Sleeping\",\n    u\"\\(\\^_-\\)\": \"Wink\",\n    u\"\\(\\(\\+_\\+\\)\\)\": \"Confused\",\n    u\"\\(\\+o\\+\\)\": \"Confused\",\n    u\"\\(o\\|o\\)\": \"Ultraman\",\n    u\"\\^_\\^\": \"Joyful\",\n    u\"\\(\\^_\\^\\)/\": \"Joyful\",\n    u\"\\(\\^O\\^\\)／\": \"Joyful\",\n    u\"\\(\\^o\\^\\)／\": \"Joyful\",\n    u\"\\(__\\)\": \"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"_\\(\\._\\.\\)_\": \"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<\\(_ _\\)>\": \"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<m\\(__\\)m>\": \"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(__\\)m\": \"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(_ _\\)m\": \"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"\\('_'\\)\": \"Sad or Crying\",\n    u\"\\(/_;\\)\": \"Sad or Crying\",\n    u\"\\(T_T\\) \\(;_;\\)\": \"Sad or Crying\",\n    u\"\\(;_;\": \"Sad of Crying\",\n    u\"\\(;_:\\)\": \"Sad or Crying\",\n    u\"\\(;O;\\)\": \"Sad or Crying\",\n    u\"\\(:_;\\)\": \"Sad or Crying\",\n    u\"\\(ToT\\)\": \"Sad or Crying\",\n    u\";_;\": \"Sad or Crying\",\n    u\";-;\": \"Sad or Crying\",\n    u\";n;\": \"Sad or Crying\",\n    u\";;\": \"Sad or Crying\",\n    u\"Q\\.Q\": \"Sad or Crying\",\n    u\"T\\.T\": \"Sad or Crying\",\n    u\"QQ\": \"Sad or Crying\",\n    u\"Q_Q\": \"Sad or Crying\",\n    u\"\\(-\\.-\\)\": \"Shame\",\n    u\"\\(-_-\\)\": \"Shame\",\n    u\"\\(一一\\)\": \"Shame\",\n    u\"\\(；一_一\\)\": \"Shame\",\n    u\"\\(=_=\\)\": \"Tired\",\n    u\"\\(=\\^\\·\\^=\\)\": \"cat\",\n    u\"\\(=\\^\\·\\·\\^=\\)\": \"cat\",\n    u\"=_\\^= \": \"cat\",\n    u\"\\(\\.\\.\\)\": \"Looking down\",\n    u\"\\(\\._\\.\\)\": \"Looking down\",\n    u\"\\^m\\^\": \"Giggling with hand covering mouth\",\n    u\"\\(\\・\\・?\": \"Confusion\",\n    u\"\\(?_?\\)\": \"Confusion\",\n    u\">\\^_\\^<\": \"Normal Laugh\",\n    u\"<\\^!\\^>\": \"Normal Laugh\",\n    u\"\\^/\\^\": \"Normal Laugh\",\n    u\"\\（\\*\\^_\\^\\*）\": \"Normal Laugh\",\n    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\": \"Normal Laugh\",\n    u\"\\(^\\^\\)\": \"Normal Laugh\",\n    u\"\\(\\^\\.\\^\\)\": \"Normal Laugh\",\n    u\"\\(\\^_\\^\\.\\)\": \"Normal Laugh\",\n    u\"\\(\\^_\\^\\)\": \"Normal Laugh\",\n    u\"\\(\\^\\^\\)\": \"Normal Laugh\",\n    u\"\\(\\^J\\^\\)\": \"Normal Laugh\",\n    u\"\\(\\*\\^\\.\\^\\*\\)\": \"Normal Laugh\",\n    u\"\\(\\^—\\^\\）\": \"Normal Laugh\",\n    u\"\\(#\\^\\.\\^#\\)\": \"Normal Laugh\",\n    u\"\\（\\^—\\^\\）\": \"Waving\",\n    u\"\\(;_;\\)/~~~\": \"Waving\",\n    u\"\\(\\^\\.\\^\\)/~~~\": \"Waving\",\n    u\"\\(-_-\\)/~~~ \\($\\·\\·\\)/~~~\": \"Waving\",\n    u\"\\(T_T\\)/~~~\": \"Waving\",\n    u\"\\(ToT\\)/~~~\": \"Waving\",\n    u\"\\(\\*\\^0\\^\\*\\)\": \"Excited\",\n    u\"\\(\\*_\\*\\)\": \"Amazed\",\n    u\"\\(\\*_\\*;\": \"Amazed\",\n    u\"\\(\\+_\\+\\) \\(@_@\\)\": \"Amazed\",\n    u\"\\(\\*\\^\\^\\)v\": \"Laughing,Cheerful\",\n    u\"\\(\\^_\\^\\)v\": \"Laughing,Cheerful\",\n    u\"\\(\\(d[-_-]b\\)\\)\": \"Headphones,Listening to music\",\n    u'\\(-\"-\\)': \"Worried\",\n    u\"\\(ーー;\\)\": \"Worried\",\n    u\"\\(\\^0_0\\^\\)\": \"Eyeglasses\",\n    u\"\\(\\＾ｖ\\＾\\)\": \"Happy\",\n    u\"\\(\\＾ｕ\\＾\\)\": \"Happy\",\n    u\"\\(\\^\\)o\\(\\^\\)\": \"Happy\",\n    u\"\\(\\^O\\^\\)\": \"Happy\",\n    u\"\\(\\^o\\^\\)\": \"Happy\",\n    u\"\\)\\^o\\^\\(\": \"Happy\",\n    u\":O o_O\": \"Surprised\",\n    u\"o_0\": \"Surprised\",\n    u\"o\\.O\": \"Surpised\",\n    u\"\\(o\\.o\\)\": \"Surprised\",\n    u\"oO\": \"Surprised\",\n    u\"\\(\\*￣m￣\\)\": \"Dissatisfied\",\n    u\"\\(‘A`\\)\": \"Snubbed or Deflated\"\n}\n\n\n\ndef remove_emoticons(text):\n    emoticons_pattern = re.compile(\n        u'(' + u'|'.join(emo for emo in EMOTICONS) + u')')\n    return emoticons_pattern.sub(r'', text)\n\n\nremove_emoticons(\"Hello :->\")\n\n'Hello '\n\n\n\n# Remove emoticons from text\ndf.text = df.text.apply(lambda x: remove_emoticons(x))\n\n\n\nHashtags and Mentions\nWe are habituated to use hashtags and mentions in our tweet either to indicate the context or bring attention to an individual. Hashtags can be used to extract features, to see what’s trending and in various other applications.\nSince, we don’t require them we’ll remove them.\n\ndef remove_tags_mentions(text):\n    pattern = re.compile(r'(@\\S+|#\\S+)')\n    return pattern.sub('', text)\n\n\ntext = \"live @flippinginja on #younow - jonah and jareddddd\"\nremove_tags_mentions(text)\n\n'live  on  - jonah and jareddddd'\n\n\n\n# Remove hashtags and mentions\ndf.text = df.text.apply(lambda x: remove_tags_mentions(x))\n\n\n\nPunctuations\nPunctuations are character other than alphaters and digits. These include [!“#$%&'()*+,-./:;<=>?@\\^_`{|}~]\nIt is better remove or convert emoticons before removing the punctuations, since if we do the other we around we might loose the emoticons from the text. Another example, if the text contains $10.50 then we’ll remove the .(dot) and the value will loose it’s meaning.\n\nPUNCTUATIONS = string.punctuation\n\n\ndef remove_punctuation(text):\n    return text.translate(str.maketrans('', '', PUNCTUATIONS))\n\n\ndf.text = df[\"text\"].apply(lambda text: remove_punctuation(text))\n\n\n\n\n\n  \n    \n      \n      text\n      label\n    \n  \n  \n    \n      0\n      username changed d\n      1\n    \n    \n      1\n      unnieeee\n      1\n    \n    \n      2\n      thanks i hope youve got a good book to keep you company\n      1\n    \n    \n      3\n      where are you situated\n      1\n    \n    \n      4\n      youre welcome im glad you liked it\n      1\n    \n  \n\n\n\n\n\n\nStopwords\nStopwords are commonly occuring words in any language. Such as, in english these words are ‘the’, ‘a’, ‘an’, & many more. They are in most cases not useful and should be removed.\nThere are certain tasks in which these words are useful such as Part-of-Speech(POS) tagging, language translation. Stopwords are compiled for many languages, for english language we can use the list from the nltk package.\n\nSTOPWORDS = set(stopwords.words('english'))\n\n\ndef remove_stopwords(text):\n    return ' '.join([word for word in text.split() if word not in STOPWORDS])\n\n\n# Remove stopwords\ndf.text = df.text.apply(lambda text: remove_stopwords(text))\ndf.head()\n\n\n\n\n\n  \n    \n      \n      text\n      label\n    \n  \n  \n    \n      0\n      username changed\n      1\n    \n    \n      1\n      unnieeee\n      1\n    \n    \n      2\n      thanks hope youve got good book keep company\n      1\n    \n    \n      3\n      situated\n      1\n    \n    \n      4\n      youre welcome im glad liked\n      1\n    \n  \n\n\n\n\n\n\nNumbers\nWe may remove numbers if they are not useful in our analysis. But analysis in the financial domain, numbers are very useful.\n\ndf.text = df.text.str.replace(r'\\d+', '', regex=True)\n\n\n\nExtra whitespaces\nAfter usually after preprocessing the text there might be extra whitespaces that might be created after transforming, removing various characters. Also, there is a need to remove all the new line, tab characters as well from our text.\n\ndef remove_whitespaces(text):\n    return \" \".join(text.split())\n\n\ntext = \"  Whitespaces in the beginning are removed  \\t as well \\n  as in between  the text   \"\n\nclean_text = \" \".join(text.split())\nclean_text\n\n'Whitespaces in the beginning are removed as well as in between the text'\n\n\n\ndf.text = df.text.apply(lambda x: remove_whitespaces(x))\n\n\n\nFrequent words\nPreviously we have removed stopwords which are common in any language. If we are working in any domain, we can also remove the common words used in that domain which don’t provide us with much information.\n\ndef freq_words(text):\n    tokens = word_tokenize(text)\n    FrequentWords = []\n\n    for word in tokens:\n        counter[word] += 1\n\n    for (word, word_count) in counter.most_common(10):\n        FrequentWords.append(word)\n    return FrequentWords\n\n\ndef remove_fw(text, FrequentWords):\n    tokens = word_tokenize(text)\n    without_fw = []\n    for word in tokens:\n        if word not in FrequentWords:\n            without_fw.append(word)\n\n    without_fw = ' '.join(without_fw)\n    return without_fw\n\n\ncounter = Counter()\n\n\ntext = \"\"\"\nNatural Language Processing is the technology used to aid computers to understand the human’s natural language. It’s not an easy task teaching machines to understand how we communicate. Leand Romaf, an experienced software engineer who is passionate at teaching people how artificial intelligence systems work, says that “in recent years, there have been significant breakthroughs in empowering computers to understand language just as we do.” This article will give a simple introduction to Natural Language Processing and how it can be achieved. Natural Language Processing, usually shortened as NLP, is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable. Most NLP techniques rely on machine learning to derive meaning from human languages.\n\"\"\"\n\n\nFrequentWords = freq_words(text)\nprint(FrequentWords)\n\n[',', 'to', '.', 'is', 'the', 'understand', 'Natural', 'Language', 'Processing', 'computers']\n\n\n\nfw_result = remove_fw(text, FrequentWords)\nfw_result\n\n'technology used aid human ’ s natural language It ’ s not an easy task teaching machines how we communicate Leand Romaf an experienced software engineer who passionate at teaching people how artificial intelligence systems work says that “ in recent years there have been significant breakthroughs in empowering language just as we do. ” This article will give a simple introduction and how it can be achieved usually shortened as NLP a branch of artificial intelligence that deals with interaction between and humans using natural language The ultimate objective of NLP read decipher and make sense of human languages in a manner that valuable Most NLP techniques rely on machine learning derive meaning from human languages'\n\n\n\n\nRare words\nRare words are similar to frequent words. We can remove them because they are so less that they cannot add any value to the purpose.\n\ndef rare_words(text):\n    # tokenization\n    tokens = word_tokenize(text)\n    for word in tokens:\n        counter[word] = +1\n\n    RareWords = []\n    number_rare_words = 10\n    # take top 10 frequent words\n    frequentWords = counter.most_common()\n    for (word, word_count) in frequentWords[:-number_rare_words:-1]:\n        RareWords.append(word)\n\n    return RareWords\n\n\ndef remove_rw(text, RareWords):\n    tokens = word_tokenize(text)\n    without_rw = []\n    for word in tokens:\n        if word not in RareWords:\n            without_rw.append(word)\n\n    without_rw = ' '.join(without_fw)\n    return without_rw\n\n\ncounter = Counter()\n\n\ntext = \"\"\"\nNatural Language Processing is the technology used to aid computers to understand the human’s natural language. It’s not an easy task teaching machines to understand how we communicate. Leand Romaf, an experienced software engineer who is passionate at teaching people how artificial intelligence systems work, says that “in recent years, there have been significant breakthroughs in empowering computers to understand language just as we do.” This article will give a simple introduction to Natural Language Processing and how it can be achieved. Natural Language Processing, usually shortened as NLP, is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable. Most NLP techniques rely on machine learning to derive meaning from human languages.\n\"\"\"\n\n\nRareWords = rare_words(text)\nRareWords\n\n['from',\n 'meaning',\n 'derive',\n 'learning',\n 'machine',\n 'on',\n 'rely',\n 'techniques',\n 'Most']\n\n\n\nrw_result = remove_fw(text, RareWords)\nrw_result\n\n'Natural Language Processing is the technology used to aid computers to understand the human ’ s natural language . It ’ s not an easy task teaching machines to understand how we communicate . Leand Romaf , an experienced software engineer who is passionate at teaching people how artificial intelligence systems work , says that “ in recent years , there have been significant breakthroughs in empowering computers to understand language just as we do. ” This article will give a simple introduction to Natural Language Processing and how it can be achieved . Natural Language Processing , usually shortened as NLP , is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language . The ultimate objective of NLP is to read , decipher , understand , and make sense of the human languages in a manner that is valuable . NLP to human languages .'"
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#conversion-of-emoji-to-words",
    "href": "posts/nlp/text-preprocessing.html#conversion-of-emoji-to-words",
    "title": "Text Preprocessing",
    "section": "Conversion of Emoji to Words",
    "text": "Conversion of Emoji to Words\nTo remove or not is done based on the purpose of the application. Example if we are building a sentiment analysis system emoji’s can be useful.\n“The movie was 🔥” or “The movie was 💩”\nIf we remove the emoji’s the meaning of the sentence changes completely. In these cases we can convert emoji’s to words.\ndemoji requires an initial data download from the Unicode Consortium’s emoji code repository.\nOn first use of the package, call download_codes(). This will store the Unicode hex-notated symbols at ~/.demoji/codes.json for future use.\nRead more about demoji on pypi.org\n\ndemoji.download_codes()\n\nDownloading emoji data ...\n... OK (Got response in 1.35 seconds)\nWriting emoji data to C:\\Users\\sagar\\.demoji\\codes.json ...\n... OK\n\n\n\ndef emoji_to_words(text):\n    return demoji.replace_with_desc(text, sep=\"__\")\n\n\ntext = \"game is on 🔥 🚣🏼\"\nemoji_to_words(text)\n\n'game is on __fire__ __person rowing boat: medium-light skin tone__'"
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#conversion-of-emoticons-to-words",
    "href": "posts/nlp/text-preprocessing.html#conversion-of-emoticons-to-words",
    "title": "Text Preprocessing",
    "section": "Conversion of Emoticons to Words",
    "text": "Conversion of Emoticons to Words\nAs we did for emoji’s, we convert emoticons to words for the same purpose.\n\ndef emoticons_to_words(text):\n    for emot in EMOTICONS:\n        text = re.sub(\n            u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\", \"\").replace(\":\", \"\").split()), text)\n    return text\n\n\ntext = \"Hey there!! :-)\"\nemoticons_to_words(text)\n\n'Hey there!! Happy_face_smiley'"
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#converting-numbers-to-words",
    "href": "posts/nlp/text-preprocessing.html#converting-numbers-to-words",
    "title": "Text Preprocessing",
    "section": "Converting Numbers to Words",
    "text": "Converting Numbers to Words\nIf our analysis require us to use information based on the numbers in the text, we can convert them to words.\nRead more about num2words on github\n\ndef nums_to_words(text):\n    new_text = []\n    for word in text.split():\n        if word.isdigit():\n            new_text.append(num2words(word))\n        else:\n            new_text.append(word)\n    return \" \".join(new_text)\n\n\ntext = \"I ran this track 30 times\"\nnums_to_words(text)\n\n'I ran this track thirty times'"
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#chat-words-conversion",
    "href": "posts/nlp/text-preprocessing.html#chat-words-conversion",
    "title": "Text Preprocessing",
    "section": "Chat words Conversion",
    "text": "Chat words Conversion\nThe more we use social media, we have become lazy to type the whole phrase or word. Due to which slang words came into existance such as “omg” which represents “Oh my god”. Such slang words don’t provide much information and if we need to use them we have to convert them.\nThank you: GitHub repo for the list of slang words\n\n\nCode\nchat_words = \"\"\"\nAFAIK=As Far As I Know\nAFK=Away From Keyboard\nASAP=As Soon As Possible\nATK=At The Keyboard\nATM=At The Moment\nA3=Anytime, Anywhere, Anyplace\nBAK=Back At Keyboard\nBBL=Be Back Later\nBBS=Be Back Soon\nBFN=Bye For Now\nB4N=Bye For Now\nBRB=Be Right Back\nBRT=Be Right There\nBTW=By The Way\nB4=Before\nB4N=Bye For Now\nCU=See You\nCUL8R=See You Later\nCYA=See You\nFAQ=Frequently Asked Questions\nFC=Fingers Crossed\nFWIW=For What It's Worth\nFYI=For Your Information\nGAL=Get A Life\nGG=Good Game\nGN=Good Night\nGMTA=Great Minds Think Alike\nGR8=Great!\nG9=Genius\nIC=I See\nICQ=I Seek you (also a chat program)\nILU=ILU: I Love You\nIMHO=In My Honest/Humble Opinion\nIMO=In My Opinion\nIOW=In Other Words\nIRL=In Real Life\nKISS=Keep It Simple, Stupid\nLDR=Long Distance Relationship\nLMAO=Laugh My A.. Off\nLOL=Laughing Out Loud\nLTNS=Long Time No See\nL8R=Later\nMTE=My Thoughts Exactly\nM8=Mate\nNRN=No Reply Necessary\nOIC=Oh I See\nPITA=Pain In The A..\nPRT=Party\nPRW=Parents Are Watching\nQPSA?=Que Pasa?\nROFL=Rolling On The Floor Laughing\nROFLOL=Rolling On The Floor Laughing Out Loud\nROTFLMAO=Rolling On The Floor Laughing My A.. Off\nSK8=Skate\nSTATS=Your sex and age\nASL=Age, Sex, Location\nTHX=Thank You\nTTFN=Ta-Ta For Now!\nTTYL=Talk To You Later\nU=You\nU2=You Too\nU4E=Yours For Ever\nWB=Welcome Back\nWTF=What The F...\nWTG=Way To Go!\nWUF=Where Are You From?\nW8=Wait...\n7K=Sick:-D Laugher\nOMG=Oh my god\"\"\"\n\n\n\nchat_words_dict = dict()\nchat_words_set = set()\n\n\ndef cw_conversion(text):\n    new_text = []\n    for word in text.split():\n        if word.upper() in chat_words_set:\n            new_text.append(chat_words_dict[word.upper()])\n        else:\n            new_text.append(word)\n    return \" \".join(new_text)\n\n\nfor line in chat_words.split('\\n'):\n    if line != '':\n        cw, cw_expanded = line.split('=')[0], line.split('=')[1]\n\n        chat_words_set.add(cw)\n        chat_words_dict[cw] = cw_expanded\n\n\ntext = \"omg that's awesome.\"\ncw_conversion(text)\n\n\"Oh my god that's awesome.\""
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#expanding-contractions",
    "href": "posts/nlp/text-preprocessing.html#expanding-contractions",
    "title": "Text Preprocessing",
    "section": "Expanding Contractions",
    "text": "Expanding Contractions\nContractions are words or combinations of words created by dropping a few letters and replacing those letters by an apostrophe.\nExample:\n\ndon’t: do not\nwe’ll: we will\n\nOur nlp model don’t understand these contractions i.e. they don’t understand that “don’t” and “do not” are the same thing. If our problem statement requires them then we can expand them or else leave it as it is.\n\ndef expand_contractions(text):\n    expanded_text = []\n    for line in text:\n        expanded_text.append(contractions.fix(line))\n    return expanded_text\n\n\ntext = [\"I'll be there within 15 minutes.\",\n        \"It's awesome to meet your new friends.\"]\nexpand_contractions(text)\n\n['I will be there within 15 minutes.',\n 'it is awesome to meet your new friends.']"
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#stemming",
    "href": "posts/nlp/text-preprocessing.html#stemming",
    "title": "Text Preprocessing",
    "section": "Stemming",
    "text": "Stemming\nIn stemming we reduce the word to it’s base or root form by removing the suffix characters from the word. It is one of the technique to normalize text.\nStemming for root word “like” include:\n\n“likes”\n“liked”\n“likely”\n“liking”\n\nStemmed word doesn’t always match the words in our dictionary such as:\n\nconsole -> consol\ncompany -> compani\nwelcome -> welcom\n\nDue to which stemming is not performed in all nlp tasks.\nThere are various algorithms used for stemming but the most widely used is PorterStemmer. In this post we have used the PorterStemmer as well.\n\nstemmer = PorterStemmer()\n\n\ndef stem_words(text):\n    return ' '.join([stemmer.stem(word) for word in text.split()])\n\n\ndf['text_stemmed'] = df.text.apply(lambda text: stem_words(text))\ndf[['text', 'text_stemmed']].head()\n\n\n\n\n\n  \n    \n      \n      text\n      text_stemmed\n    \n  \n  \n    \n      0\n      username changed\n      usernam chang\n    \n    \n      1\n      unnieeee\n      unnieee\n    \n    \n      2\n      thanks hope youve got good book keep company\n      thank hope youv got good book keep compani\n    \n    \n      3\n      situated\n      situat\n    \n    \n      4\n      youre welcome im glad liked\n      your welcom im glad like\n    \n  \n\n\n\n\nPorterStemmer can be used only for english. If we are working with other than english then we can use SnowballStemmer.\n\nSnowballStemmer.languages\n\n('arabic',\n 'danish',\n 'dutch',\n 'english',\n 'finnish',\n 'french',\n 'german',\n 'hungarian',\n 'italian',\n 'norwegian',\n 'porter',\n 'portuguese',\n 'romanian',\n 'russian',\n 'spanish',\n 'swedish')"
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#lemmatization",
    "href": "posts/nlp/text-preprocessing.html#lemmatization",
    "title": "Text Preprocessing",
    "section": "Lemmatization",
    "text": "Lemmatization\nLemmatization tried to perform the similar task as that of stemming i.e. trying to reduce the inflection words to it’s base form. But lemmatization does it by using a different approach.\nLemmatizations takes into consideration of the morphological analysis of the word. It tries to reduce to words to it’s dictionary form which is known as lemma.\n\nlemmatizer = WordNetLemmatizer()\n\n\ndef text_lemmatize(text):\n    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n\n\ndf['text_lemmatized'] = df.text.apply(lambda text: text_lemmatize(text))\ndf[['text', 'text_stemmed', 'text_lemmatized']].head()\n\n\n\n\n\n  \n    \n      \n      text\n      text_stemmed\n      text_lemmatized\n    \n  \n  \n    \n      0\n      username changed\n      usernam chang\n      username changed\n    \n    \n      1\n      unnieeee\n      unnieee\n      unnieeee\n    \n    \n      2\n      thanks hope youve got good book keep company\n      thank hope youv got good book keep compani\n      thanks hope youve got good book keep company\n    \n    \n      3\n      situated\n      situat\n      situated\n    \n    \n      4\n      youre welcome im glad liked\n      your welcom im glad like\n      youre welcome im glad liked\n    \n  \n\n\n\n\nDifference between Stemming and Lemmatization:\n\n\n\n\n\n\n\nStemming\nLemmatization\n\n\n\n\nFast compared to lemmatization\nSlow compared to stemming\n\n\nReduces the word to it’s base form by removing the suffix\nUses lexical knowledge to get the base form of the word\n\n\nDoes not always provide meaning or dictionary form of the original word\nResulting words are always meaningful and dictionary words"
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#spelling-correction",
    "href": "posts/nlp/text-preprocessing.html#spelling-correction",
    "title": "Text Preprocessing",
    "section": "Spelling Correction",
    "text": "Spelling Correction\nWe as human always make mistake. Normally incorrect spelling in text are know as typos.\nSince the NLP model doesn’t know the difference between a correct and an incorrect word. For the model “thanks” and “thnks” are two different words. Therefore, spelling correction is an important step to bring the incorrect words in the correct format.\n\nspell = SpellChecker()\n\n\ndef correct_spelling(text):\n    correct_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            correct_text.append(spell.correction(word))\n        else:\n            correct_text.append(word)\n    return \" \".join(correct_text)\n\n\ntext = \"Hi, hwo are you doin? I'm good thnks for asking\"\ncorrect_spelling(text)\n\n\"Hi, how are you doing I'm good thanks for asking\"\n\n\n\ntext = \"hw are you doin? I'm god thnks\"\ncorrect_spelling(text)\n\n\"he are you doing I'm god thanks\""
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#convert-accented-characters-to-ascii-characters",
    "href": "posts/nlp/text-preprocessing.html#convert-accented-characters-to-ascii-characters",
    "title": "Text Preprocessing",
    "section": "Convert accented characters to ASCII characters",
    "text": "Convert accented characters to ASCII characters\nAccent marks (also referred to as diacritics or diacriticals) usually appear above a character when we press the character for a long time. These need to be remove cause the model cannot distinguish between “dèèp” and “deep”. It will consider them as two different words.\n\ndef accented_to_ascii(text):\n    return unidecode.unidecode(text)\n\n\ntext = \"This is an example text with accented characters like dèèp lèarning ánd cömputer vísíön etc.\"\naccented_to_ascii(text)\n\n'This is an example text with accented characters like deep learning and computer vision etc.'"
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#ekphrasis",
    "href": "posts/nlp/text-preprocessing.html#ekphrasis",
    "title": "Text Preprocessing",
    "section": "Ekphrasis",
    "text": "Ekphrasis\nCollection of lightweight text tools, geared towards text from social networks, such as Twitter or Facebook, for tokenization, word normalization, word segmentation (for splitting hashtags) and spell correction, using word statistics from 2 big corpora (english Wikipedia, twitter - 330mil english tweets).\nekphrasis was developed as part of the text processing pipeline for DataStories team’s submission for SemEval-2017 Task 4 (English), Sentiment Analysis in Twitter (source)\n\n# Referred from: https://github.com/cbaziotis/ekphrasis\n\ntext_processor = TextPreProcessor(\n    # terms that will be normalized\n    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n               'time', 'url', 'date', 'number'],\n\n    # terms that will be annotated\n    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n              'emphasis', 'censored'},\n    fix_html=True,  # fix HTML tokens\n\n    # corpus from which the word statistics are going to be used\n    # for word segmentation\n    segmenter=\"twitter\",\n\n    # corpus from which the word statistics are going to be used\n    # for spell correction\n    corrector=\"twitter\",\n\n    unpack_hashtags=True,  # perform word segmentation on hashtags\n    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n    spell_correct_elong=False,  # spell correction for elongated words\n\n    # select a tokenizer. You can use SocialTokenizer, or pass your own\n    # the tokenizer, should take as input a string and return a list of tokens\n    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n\n    # list of dictionaries, for replacing tokens extracted from the text,\n    # with other expressions. You can pass more than one dictionaries.\n    dicts=[emoticons]\n)\n\nReading twitter - 1grams ...\nReading twitter - 2grams ...\nReading twitter - 1grams ...\n\n\n\n# Clean text based on function defined above\ndf['clean_tweets'] = [\n    \" \".join(text_processor.pre_process_doc(tweet)) for tweet in df.text]\n\n\n# Display the clean text\ndf.head()\n\n\n\n\n\n  \n    \n      \n      text\n      label\n      clean_tweets\n    \n  \n  \n    \n      0\n      @ManiMint_ you're welcome :)\n      1\n      <user> you are welcome <happy>\n    \n    \n      1\n      Expired and I used BIS money now I'm broke ):(...\n      0\n      expired and i used <allcaps> bis </allcaps> mo...\n    \n    \n      2\n      Thank you :) https://t.co/DuVcLseonQ\n      1\n      thank you <happy> <url>\n    \n    \n      3\n      @DeMoorSophie Hii, can you follow me, please? ...\n      1\n      <user> hii , can you follow me , please ? i ' ...\n    \n    \n      4\n      @hellasugg @MyNamesChai and sacconejoly's (@Jo...\n      0\n      <user> <user> and sacconejoly ' s ( <user> and..."
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#conclusion",
    "href": "posts/nlp/text-preprocessing.html#conclusion",
    "title": "Text Preprocessing",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, most of the text pre-processing techniques are explanied. I’ll update this post as I learn more techniques to pre-process text.\nShare if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "posts/nlp/into-nlp.html",
    "href": "posts/nlp/into-nlp.html",
    "title": "What is Natural Language Processing?",
    "section": "",
    "text": "Natural language processing is a branch of Artificial Intelligence which aims to bridge the gap between how a computer and human communicate with each other. The two major handles used for communication are speech and written i.e. text."
  },
  {
    "objectID": "posts/nlp/into-nlp.html#history-of-natural-language-processing",
    "href": "posts/nlp/into-nlp.html#history-of-natural-language-processing",
    "title": "What is Natural Language Processing?",
    "section": "History of Natural Language Processing",
    "text": "History of Natural Language Processing\nThe dawn of NLP can be dated back to the early 1900s. In 1950, Alan Turing published his famous article “Computing Machinery and Intelligence” which proposed what is now called the Turing test as a criterion of intelligence. It tests the ability of the computer program to impersonate a human in a real-time conversation with a human judge where the judge is unable to distinguish the human from the computer program. In 1954, the Georgetown experiment automatically translated more than sixty Russian words into English.\nIn 1957, Noam Chomsky’s Syntactic Structures a rule-based system of syntactic structures with “universal grammar” was an incredible advancement. Up to the 1980’s most of the NLP systems were based on complex hand-written rules but in the late 1980s by the introduction of machine learning algorithms for language processing revolutionized the field. A steady increase in computational power resulting from Moore’s law and use of statistical models that use probabilistic measures to map the features making up the input data. Watson an artificial intelligence software designed as a question answering system won the Jeopardy contest, defeating the best human players in February 2011.\nDevelopment of famous virtual assistants like Siri in 2011, Amazon Alexa in 2014, and Google Assistant in 2016. The use of deep learning produced better results than the state-of-the-art in many natural language processing tasks, such as machine translation, text classification, and many more. Recent advancements include the use of network architecture of the transformer which is based on the attention mechanism that has produced better results in various NLP tasks.\nWe humans in our daily life overlook the powerful ability of our human brain to read, understand the meaning of a word, it’s context (how does it relate to each other), understand humor, sarcasm, and thousand other things. How do we teach this to a computer?"
  },
  {
    "objectID": "posts/nlp/into-nlp.html#challenges",
    "href": "posts/nlp/into-nlp.html#challenges",
    "title": "What is Natural Language Processing?",
    "section": "Challenges",
    "text": "Challenges\n1. Ambiguity:  In a natural language, words are unique but their meaning may differ based on the context in which it is used. One classical example used is: - The bank is a financial institution where customers can save or borrow money. - Tom was sitting by the banks of the river.\nIn this example, we can see that the word “bank” is used in two different ways. The word is the same but the meaning is different. This is because the context in which the word is used is different.\n2. Co-Referencing: It is a process to find all the phrases in the document that refer to the same entity. Example: Harry kept the paneer on the plate and ate it. Here it refers to the paneer that he ate which was kept on the plate.\n3. Information Extraction: Identifying phrases in a language that refer to specific types of entities and relations in text. Named Entity Recognition (NER) is the task used to identify the names of people, organizations, places, etc, in a text. Example: Tom used to work at FedEx and lives in Mumbai, India. where Person = Tom, organization = FedEx and Place = Mumbai, India\n4. Personality, intention, emotions, and style: Different authors may have different personalities, intentions, emotions, and styles to convey the same idea. Based on these factors the underlying idea can be interpreted in different ways. Use of humor or sarcasm may convey a meaning that is opposite of the literal one."
  },
  {
    "objectID": "posts/nlp/into-nlp.html#applications",
    "href": "posts/nlp/into-nlp.html#applications",
    "title": "What is Natural Language Processing?",
    "section": "Applications",
    "text": "Applications\n1. Machine Translation:  The idea behind machine translation is to develop a system that is capable of translating text from one language to another without any human intervention. Only translating the text from one language to another is not the key. Understanding the meaning behind the text and translating it to some other language is the crux of it. Example: Google Translate\n2. Automatic summarization:  We all love to read storybooks and always a good storybook will have a summary at the end that highlights the important things about the story. Likewise take any piece of text, a story, a news article, etc, and develop a system that can automatically summary the piece of text. Example: Inshorts – an app that summarizes each news article in 60 words.\n3. Sentiment Analysis:  It deals with the study of extracting opinions and sentiment that are not always explicitly expressed. For instance it helps the company to understand the level of consumer satisfaction for its goods and services. Example: “I love the new iPhone and it has a great camera.”.\nAnother branch of sentiment analysis is “Aspect based Sentiment Analysis” where it tries to extract opinions for each aspect from the customer review. Example: “The new iPhone is great it has a great camera but the battery life is not that good.” Here the customer is satisfied with the camera aspect of the iPhone but not the battery life.\n4. Text Classification:  Organizing text documents into predefined categories enables to classify the information or any activity. Examples: Classifying an email as spam or not spam.\n5. Question Answering:  Question answering deals with a system that can answer questions posed by humans in natural language. Sounds simple yet building the knowledge base, understanding the text, and to answer in natural language is altogether a thing in itself.\n6. Chatbots:  Chatbots are a piece of software that can simulate a conversation (or chat) with a user in natural language through websites, apps, messaging applications, etc. Chatbots are a natural evolution of question answering system but are one step further with their ability to understand the text and engage in a conversation.\n7. Speech Recognition:  Using our voice to interact with our phones has become a common phenomenon. For example to ask questions to our voice assistants like Google Assistant/Siri/Cortana, use of voice to type a piece of text. Recognizing speech has replaced the method by which we interact with our devices and made it so convenient.\nRecent advancements in NLP have deepened our knowledge on how to tackle the various challenges in NLP. Also, this new decade will be filled with excitement and breakthroughs that awaits us. Stay tunned to deep dive into the world of NLP.\nShare if you like it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "posts/spark/movielens.html",
    "href": "posts/spark/movielens.html",
    "title": "Movie Lens - Collaborative Filtering based Recommendation System",
    "section": "",
    "text": "Recommendation System are build by analyzing the user and product interation data. It can be used to give item suggestion to a user or predict how a user would rate the an item.\nRecommendation System have mainly have three approaches:\nIn this notebook, Alternating Least Squares (ALS) matrix factorization algorithm with the use of Apache Spark APIs to predict the ratings of movies in the MovieLens Dataset.\nALS attempts to estimate the ratings matrix R as the product of two lower-rank matrices, X and Y, i.e. X * Yt = R. Typically these approximations are called ‘factor’ matrices. The general approach is iterative. During each iteration, one of the factor matrices is held constant, while the other is solved for using least squares. The newly-solved factor matrix is then held constant while solving for the other factor matrix (source)."
  },
  {
    "objectID": "posts/spark/movielens.html#data-description",
    "href": "posts/spark/movielens.html#data-description",
    "title": "Movie Lens - Collaborative Filtering based Recommendation System",
    "section": "1. Data Description",
    "text": "1. Data Description\nMovieLens 25M movie ratings. Stable benchmark dataset. 25 million ratings and one million tag applications applied to 62,000 movies by 162,000 users.\n\nmovies.csv: Movie information is contained in the file movies.csv. Each line of this file after the header row represents one movie, and has the following format: movieId,title,genres\n\nGenres are a pipe-separated list, and are selected from the following:\n\nAction\nAdventure\nAnimation\nChildren’s\nComedy\nCrime\nDocumentary\nDrama\nFantasy\nFilm-Noir\nHorror\nMusical\nMystery\nRomance\nSci-Fi\nThriller\nWar\nWestern\n(no genres listed)\n\n\nrating.csv: All ratings are contained in the file rating.csv. Each line of this file after the header row represents one rating of one movie by one user, and has the following format: userId,movieId,rating,timestamp\n\nThe lines within this file are ordered first by userId, then, within user, by movieId.\nRatings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\nTimestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970."
  },
  {
    "objectID": "posts/spark/movielens.html#overview-structure-and-data-content",
    "href": "posts/spark/movielens.html#overview-structure-and-data-content",
    "title": "Movie Lens - Collaborative Filtering based Recommendation System",
    "section": "2. Overview Structure and Data Content",
    "text": "2. Overview Structure and Data Content\n\n# Create a spark session\nspark = SparkSession.builder.appName(\n    \"movie-lens-recommendation\").config(\"spark.driver.memory\", \"16g\").getOrCreate()\nspark.sparkContext.setLogLevel(\"ERROR\")\n\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n\n\n23/01/21 01:45:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n\n\n\n# Read dataset\n__dirname = '../input/movielens-20m-dataset/'\nmovies = spark.read.csv(__dirname + 'movie.csv', header=True, inferSchema=True)\nratings = spark.read.csv(__dirname + 'rating.csv',\n                         header=True, inferSchema=True)\n\n                                                                                \n\n\n\n# Shape of the datasets\nprint(f'Movies has {movies.count()} rows and {len(movies.columns)} columns')\nprint(f'Ratings has {ratings.count()} rows and {len(ratings.columns)} colunms')\n\nMovies has 27278 rows and 3 columns\n\n\n[Stage 7:=================================================>         (5 + 1) / 6]\n\n\nRatings has 20000263 rows and 4 colunms\n\n\n                                                                                \n\n\n\n# Display top five rows of each dataframe\nmovies.show(n=5, truncate=False)\nratings.show(n=5, truncate=False)\n\n+-------+----------------------------------+-------------------------------------------+\n|movieId|title                             |genres                                     |\n+-------+----------------------------------+-------------------------------------------+\n|1      |Toy Story (1995)                  |Adventure|Animation|Children|Comedy|Fantasy|\n|2      |Jumanji (1995)                    |Adventure|Children|Fantasy                 |\n|3      |Grumpier Old Men (1995)           |Comedy|Romance                             |\n|4      |Waiting to Exhale (1995)          |Comedy|Drama|Romance                       |\n|5      |Father of the Bride Part II (1995)|Comedy                                     |\n+-------+----------------------------------+-------------------------------------------+\nonly showing top 5 rows\n\n+------+-------+------+-------------------+\n|userId|movieId|rating|timestamp          |\n+------+-------+------+-------------------+\n|1     |2      |3.5   |2005-04-02 23:53:47|\n|1     |29     |3.5   |2005-04-02 23:31:16|\n|1     |32     |3.5   |2005-04-02 23:33:39|\n|1     |47     |3.5   |2005-04-02 23:32:07|\n|1     |50     |3.5   |2005-04-02 23:29:40|\n+------+-------+------+-------------------+\nonly showing top 5 rows\n\n\n\n\n2.1. Data Structure and Statistics\nLet’s go over each dataframe and check it’s schema.\nRun the describe() method to see the count, mean, standard deviation, minimum, and maximum values for the data in each column:\n\n# Movies DataFrame\nmovies.printSchema()\nmovies.describe().show()\n\nroot\n |-- movieId: integer (nullable = true)\n |-- title: string (nullable = true)\n |-- genres: string (nullable = true)\n\n\n\n[Stage 14:>                                                         (0 + 1) / 1]\n\n\n+-------+-----------------+--------------------+------------------+\n|summary|          movieId|               title|            genres|\n+-------+-----------------+--------------------+------------------+\n|  count|            27278|               27278|             27278|\n|   mean|59855.48057042305|                null|              null|\n| stddev|44429.31469707313|                null|              null|\n|    min|                1|\"\"Great Performan...|(no genres listed)|\n|    max|           131262|       貞子3D (2012)|           Western|\n+-------+-----------------+--------------------+------------------+\n\n\n\n                                                                                \n\n\n\n# Ratings DataFrame\nratings.printSchema()\nratings.summary().show()\n\nroot\n |-- userId: integer (nullable = true)\n |-- movieId: integer (nullable = true)\n |-- rating: double (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n\n\n\n[Stage 17:================================================>         (5 + 1) / 6]\n\n\n+-------+-----------------+------------------+------------------+\n|summary|           userId|           movieId|            rating|\n+-------+-----------------+------------------+------------------+\n|  count|         20000263|          20000263|          20000263|\n|   mean|69045.87258292554| 9041.567330339605|3.5255285642993797|\n| stddev|40038.62665316267|19789.477445413264| 1.051988919294229|\n|    min|                1|                 1|               0.5|\n|    25%|            34395|               903|               3.0|\n|    50%|            69133|              2167|               3.5|\n|    75%|           103637|              4771|               4.0|\n|    max|           138493|            131262|               5.0|\n+-------+-----------------+------------------+------------------+\n\n\n\n                                                                                \n\n\nNot all of these statistics are actually meaningful! You can use specific methods from the DataFrame API to compute any statistic:\n\nprint(\n    f\"Number of distinct users: {ratings.select('userId').distinct().count()}\")\nprint(\n    f\"Number of distinct movies: {ratings.select('movieId').distinct().count()}\")\n\n                                                                                \n\n\nNumber of distinct users: 138493\n\n\n[Stage 26:================================================>         (5 + 1) / 6]\n\n\nNumber of distinct movies: 26744\n\n\n                                                                                \n\n\nYou can also leverage your SQL knowledge to query the data.\nExample, Find the number of movies with ratings higher than 4 with and without SQL:\n\n# Without SQL\nprint(\n    f\"Number of distinct movies with rating greater than 4: {ratings.filter('rating > 4').select('movieId').distinct().count()}\")\n\n# With SQL\nratings.createOrReplaceTempView('ratings')\nspark.sql('SELECT COUNT(DISTINCT(movieId)) AS movie_count FROM ratings WHERE rating > 4').show()\n\n                                                                                \n\n\nNumber of distinct movies with rating greater than 4: 17218\n\n\n[Stage 44:================================================>         (5 + 1) / 6]\n\n\n+-----------+\n|movie_count|\n+-----------+\n|      17218|\n+-----------+\n\n\n\n                                                                                \n\n\n\n\n2.2. Missing Values\nCheck if any column contains missing values.\n\n# Check for missing values\n# 1. Movies Dataframe\nmovies.select([F.count(F.when(F.col(c).contains('NULL') |\n                              F.col(c).isNull() |\n                              F.isnan(c), c)).alias(c) for c in movies.columns]).show()\n\n# 2. Ratings Dataframe\nratings.select(*[(\n    F.count(\n        F.when(\n            (F.col(c).contains('NULL') | F.col(c).isNull() | F.isnan(c)), c)) if t not in ('timestamp', 'data')\n    else F.count(F.when(F.col(c).contains('NULL') | F.col(c).isNull(), c))).alias(c)\n    for c, t in ratings.dtypes]).show()\n\n+-------+-----+------+\n|movieId|title|genres|\n+-------+-----+------+\n|      0|    0|     0|\n+-------+-----+------+\n\n\n\n[Stage 53:================================================>         (5 + 1) / 6]\n\n\n+------+-------+------+---------+\n|userId|movieId|rating|timestamp|\n+------+-------+------+---------+\n|     0|      0|     0|        0|\n+------+-------+------+---------+\n\n\n\n                                                                                \n\n\n\n\n2.3. Merge DataFrame\nWe’ll merge the movies and ratings dataframe for further analysis and model building process.\n\n# Merge the movies and ratings dataframes\ndf = ratings.join(movies, on=[\"movieId\"], how=\"left\")\n\n\n# Look at df structure\ndf.printSchema()\nprint(f'Merged DataFrame has {df.count()} rows and {len(df.columns)} colunms')\n\nroot\n |-- movieId: integer (nullable = true)\n |-- userId: integer (nullable = true)\n |-- rating: double (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n |-- title: string (nullable = true)\n |-- genres: string (nullable = true)\n\n\n\n[Stage 57:================================================>         (5 + 1) / 6]\n\n\nMerged DataFrame has 20000263 rows and 6 colunms\n\n\n                                                                                \n\n\n\n# Display top five rows of merged dataframe\ndf.show(n=5, truncate=False)\n\n+-------+------+------+-------------------+---------------------------------------------------------------+--------------------------------------+\n|movieId|userId|rating|timestamp          |title                                                          |genres                                |\n+-------+------+------+-------------------+---------------------------------------------------------------+--------------------------------------+\n|2      |1     |3.5   |2005-04-02 23:53:47|Jumanji (1995)                                                 |Adventure|Children|Fantasy            |\n|29     |1     |3.5   |2005-04-02 23:31:16|City of Lost Children, The (Cité des enfants perdus, La) (1995)|Adventure|Drama|Fantasy|Mystery|Sci-Fi|\n|32     |1     |3.5   |2005-04-02 23:33:39|Twelve Monkeys (a.k.a. 12 Monkeys) (1995)                      |Mystery|Sci-Fi|Thriller               |\n|47     |1     |3.5   |2005-04-02 23:32:07|Seven (a.k.a. Se7en) (1995)                                    |Mystery|Thriller                      |\n|50     |1     |3.5   |2005-04-02 23:29:40|Usual Suspects, The (1995)                                     |Crime|Mystery|Thriller                |\n+-------+------+------+-------------------+---------------------------------------------------------------+--------------------------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "posts/spark/movielens.html#visualizations",
    "href": "posts/spark/movielens.html#visualizations",
    "title": "Movie Lens - Collaborative Filtering based Recommendation System",
    "section": "3. Visualizations",
    "text": "3. Visualizations\n\n# Distribution of User Ratings\nrating_count_df = (df.groupBy(['rating']).count()).toPandas()\n\nfig = plt.figure(figsize=(8, 5))\nsns.barplot(x='rating', y='count', data=rating_count_df)\nplt.title(\"Count of each rating\", fontsize=14)\nplt.show()\n\n                                                                                \n\n\n\n\n\n\nrating_values = df.select(['rating']).toPandas()\n\nfig = plt.figure(figsize=(8, 5))\nsns.violinplot(rating_values['rating'])\nplt.title(\"Distribution of rating\", fontsize=14)\nplt.show()\n\n\n\n\nWe find:\n\nMost of the users have rated 4.0 followed by 3.0.\n\n\ngenre_rating = (df\n                .select(\"movieId\", \"userId\", \"genres\", \"rating\")\n                .withColumn(\"genres_array\", F.split(\"genres\", \"\\|\"))\n                .withColumn(\"genre\", F.explode(\"genres_array\"))\n                .groupBy(\"genre\").agg(F.mean(F.col(\"rating\")).alias(\"genre_rating\"),\n                                      F.countDistinct(\"movieId\").alias(\n                                          \"num_movies\"),\n                                      F.countDistinct(\"movieId\", \"userId\").alias(\"num_ratings\"))\n                ).toPandas()\n\n                                                                                \n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 15))\n\ngenre_rating.plot.barh(\"genre\", \"genre_rating\", ax=axes[0, 0])\naxes[0, 0].set_title(\"Visualizing average rating for each genre\")\n\ngenre_rating.plot.barh(\"genre\", \"num_ratings\", ax=axes[0, 1])\naxes[0, 1].set_title(\"Visualizing number of ratings for each genre\")\n\ngenre_rating.plot.barh(\"genre\", \"num_movies\", ax=axes[1, 0])\naxes[1, 0].set_title(\"Visualizing number of movies in each genre\")\n\nfig.delaxes(axes[1, 1])\nfig.tight_layout()\n\nplt.show()\n\n\n\n\n\n# Analyzing day of the month - Timestamp of rating\nday_month_rating = (df\n                    .withColumnRenamed(\"timestamp\", \"date\")\n                    .withColumn(\"day\", F.dayofmonth(F.col(\"date\")))\n                    .groupBy(\"day\").agg(F.mean(F.col(\"rating\")).alias(\"avg_rating\"),\n                                        F.countDistinct(\"movieId\").alias(\n                                            \"num_movies\"),\n                                        F.countDistinct(\"movieId\", \"userId\").alias(\"num_ratings\"))\n                    ).toPandas()\n\n                                                                                \n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 12))\n\nday_month_rating.plot.scatter(\"day\", \"avg_rating\", ax=axes[0, 0])\naxes[0, 0].set_title(\"Visualizing average rating rated each day\")\n\nday_month_rating.plot.scatter(\"day\", \"num_ratings\", ax=axes[0, 1])\naxes[0, 1].set_title(\"Visualizing number of ratings rated each day\")\n\nday_month_rating.plot.scatter(\"day\", \"num_movies\", ax=axes[1, 0])\naxes[1, 0].set_title(\"Visualizing number of movies rated each day\")\n\nfig.delaxes(axes[1, 1])\nfig.tight_layout()\n\nplt.show()\n\n\n\n\nWe find:\n\nThere is no clear pattern or relation between the day of the month with other features.\n\n\n# Analyzing day of the week - Timestamp of rating\nday_week_rating = (df\n                   .withColumnRenamed(\"timestamp\", \"date\")\n                   .withColumn(\"day\", F.dayofweek(F.col(\"date\")))\n                   .groupBy(\"day\").agg(F.mean(F.col(\"rating\")).alias(\"avg_rating\"),\n                                       F.countDistinct(\"movieId\").alias(\n                                           \"num_movies\"),\n                                       F.countDistinct(\"movieId\", \"userId\").alias(\"num_ratings\"))\n                   ).toPandas()\n\n                                                                                \n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 12))\n\naxes[0, 0].scatter(day_week_rating.day.astype(\n    'int64'), day_week_rating.avg_rating)\naxes[0, 0].set_title(\"Visualizing average rating rated each day\")\n\naxes[0, 1].scatter(day_week_rating.day.astype(\n    'int64'), day_week_rating.num_ratings)\naxes[0, 1].set_title(\"Visualizing number of ratings rated each day\")\n\naxes[1, 0].scatter(day_week_rating.day.astype(\n    'int64'), day_week_rating.num_movies)\naxes[1, 0].set_title(\"Visualizing number of movies rated each day\")\n\nfig.delaxes(axes[1, 1])\nfig.tight_layout()\n\nplt.show()\n\n\n\n\n\nrelease_year_rating = (df\n                       .select(\"title\", \"movieId\", \"userId\", \"rating\")\n                       .withColumn(\"releaseyear\", F.substring('title', -5, 4))\n                       .filter(F.col(\"releaseyear\") > 1900)\n                       .groupBy(\"releaseyear\").agg(F.mean(F.col(\"rating\")).alias(\"avg_rating\"),\n                                                   F.countDistinct(\"movieId\").alias(\n                                                       \"num_movies\"),\n                                                   F.countDistinct(\"movieId\", \"userId\").alias(\"num_ratings\"))\n                       ).toPandas()\n\n                                                                                \n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 12))\n\naxes[0, 0].scatter(release_year_rating.releaseyear.astype(\n    'int64'), release_year_rating.avg_rating)\naxes[0, 0].set_title(\"Visualizing average rating vs Release Year\")\n\naxes[0, 1].scatter(release_year_rating.releaseyear.astype(\n    'int64'), release_year_rating.num_ratings)\naxes[0, 1].set_title(\"Visualizing number of ratings vs Release Year\")\n\naxes[1, 0].scatter(release_year_rating.releaseyear.astype(\n    'int64'), release_year_rating.num_movies)\naxes[1, 0].set_title(\"Visualizing number of movies vs Release Year\")\n\nfig.delaxes(axes[1, 1])\nfig.tight_layout()\n\nplt.show()\n\n\n\n\nWe find:\n\nNumber of movies have increased significantly over the years.\nNumber of rating has a weird uprise and drop."
  },
  {
    "objectID": "posts/spark/movielens.html#data-preparation",
    "href": "posts/spark/movielens.html#data-preparation",
    "title": "Movie Lens - Collaborative Filtering based Recommendation System",
    "section": "4. Data Preparation",
    "text": "4. Data Preparation\n\n# Train Test split\n(train, test) = df.select(\n    ['userId', 'movieId', 'rating']).randomSplit([0.8, 0.2])"
  },
  {
    "objectID": "posts/spark/movielens.html#model-building",
    "href": "posts/spark/movielens.html#model-building",
    "title": "Movie Lens - Collaborative Filtering based Recommendation System",
    "section": "5. Model Building",
    "text": "5. Model Building\nParameter Description:\n\nrank: rank of the factorization\nmaxIter: max number of iterations (>= 0)\nregParam: regularization parameter (>= 0)\nuserCol: column name for user ids. Ids must be within the integer value range\nitemCol: column name for item ids. Ids must be within the integer value range\nratingCol: column name for ratings\ncoldStartStrategy: strategy for dealing with unknown or new users/items at prediction time. This may be useful in cross-validation or production scenarios, for handling user/item ids the model has not seen in the training data. Supported values: ‘nan’, ‘drop’\n\n\n# Basic Model building\nals = ALS(rank=10, maxIter=4, regParam=0.1, userCol='userId',\n          itemCol='movieId', ratingCol='rating', coldStartStrategy=\"drop\")\n\n# Define evaluator as RMSE\nevaluator = RegressionEvaluator(\n    metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n\n\n% % time\n# Fit the model\nmodel = als.fit(train)\n\n                                                                                \n\n\nCPU times: user 168 ms, sys: 40.7 ms, total: 209 ms\nWall time: 3min 24s\n\n\n\n# Evaluate the model\npredictions = model.transform(test)\nrmse = evaluator.evaluate(predictions)\n\nprint('The RMSE for our model is: {}'.format(rmse))\n\n[Stage 170:============================>                            (2 + 2) / 4]\n\n\nThe RMSE for our model is: 0.8176288680275328\n\n\n                                                                                \n\n\n\n5.1. Improving the model\nOne way to improve the model is to tune the hyperparameter of the model. CrossValidator is familiar with sklearn’s cross_val_score and ParamGridBuilder is a builder for a param grid used in grid search-based model selection.\nThe cross validation consumes enormouse amout of time and hence, below you can find the skeleton code to perform hyper parameter tuning.\n\n# Define the model parameter grid\nparam_grid = ParamGridBuilder()\\\n    .addGrid(als.rank, [12, 13, 14])\\\n    .addGrid(als.maxIter, [18, 19, 20])\\\n    .addGrid(als.regParam, [0.05, 0.5, 0.1])\\\n    .build()\n\n# Initialize the cross validator\ncrossVal = CrossValidator(\n    estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=10)\n\n# Fit the model and perform cross validation\ncvModel = crossVal.fit(train)\n\n# Evaluate the model\ncvPredictions = cvModel.transform(test)\ncvRmse = evaluator.evaluate(cvPredictions)\n\nprint('The RMSE for our model is: {}'.format(cvRmse))"
  },
  {
    "objectID": "posts/spark/movielens.html#recommend-movies",
    "href": "posts/spark/movielens.html#recommend-movies",
    "title": "Movie Lens - Collaborative Filtering based Recommendation System",
    "section": "6. Recommend Movies",
    "text": "6. Recommend Movies\nTo recommend movies for a specific user, below is a function that applies the trained model, ALSModel, on the list of movies that the user hasn’t yet rated\n\ndef recommendMovies(model, user, nRecommendation):\n    # Create a Spark DataFrame with the specified user and all the movies listed in the ratings DataFrame\n    dataSet = ratings.select('movieId').distinct(\n    ).withColumn('userId', F.lit(user))\n\n    # Create a Spark DataFrame with the movies that have already been rated by this user\n    moviesAlreadyRated = ratings.filter(\n        ratings.userId == user).select('movieId', 'userId')\n\n    # Apply the recommender system to the data set without the already rated movies to predict ratings\n    predictions = model.transform(dataSet.subtract(moviesAlreadyRated)).dropna().orderBy(\n        'prediction', ascending=False).limit(nRecommendation).select('movieId', 'prediction')\n\n    # Join with the movies DataFrame to get the movies titles and genres\n    recommendations = predictions.join(movies, predictions.movieId == movies.movieId).select(\n        predictions.movieId, movies.title, movies.genres, predictions.prediction)\n\n    return recommendations\n\nNow run this function to recommend 10 movies for different users:\n\nprint('Recommendations for user 153:')\nrecommendMovies(model, 153, 10).toPandas()\n\nRecommendations for user 153:\n\n\n                                                                                \n\n\n\n\n\n\n  \n    \n      \n      movieId\n      title\n      genres\n      prediction\n    \n  \n  \n    \n      0\n      81117\n      Moth, The (Cma) (1980)\n      Drama\n      6.064673\n    \n    \n      1\n      82328\n      One Fine Spring Day (Bomnaleun ganda) (2001)\n      Drama\n      5.925478\n    \n    \n      2\n      109887\n      Great Passage, The (Fune wo amu) (2013)\n      Drama\n      5.818318\n    \n    \n      3\n      116183\n      It's Love I'm After (1937)\n      Comedy\n      5.911806\n    \n    \n      4\n      117907\n      My Brother Tom (2001)\n      Drama\n      6.311569\n    \n    \n      5\n      120134\n      Doggiewoggiez! Poochiewoochiez! (2012)\n      Comedy\n      6.050812\n    \n    \n      6\n      120821\n      The War at Home (1979)\n      Documentary|War\n      6.015426\n    \n    \n      7\n      121029\n      No Distance Left to Run (2010)\n      Documentary\n      6.553770\n    \n    \n      8\n      129536\n      Code Name Coq Rouge (1989)\n      (no genres listed)\n      6.092913\n    \n    \n      9\n      130347\n      Bill Hicks: Sane Man (1989)\n      Comedy\n      5.887615\n    \n  \n\n\n\n\n[Stage 420:=======================================>                (7 + 3) / 10]\n\n\n\nprint('Recommendations for user 250:')\nrecommendMovies(model, 250, 10).toPandas()\n\nRecommendations for user 250:\n\n\n                                                                                \n\n\n\n\n\n\n  \n    \n      \n      movieId\n      title\n      genres\n      prediction\n    \n  \n  \n    \n      0\n      26793\n      Tito and Me (Tito i ja) (1992)\n      Comedy\n      4.924314\n    \n    \n      1\n      103593\n      Taming the Fire (Ukroshcheniye ognya) (1972)\n      Documentary|Drama|Sci-Fi\n      4.850996\n    \n    \n      2\n      104317\n      Flight of the Conchords: A Texan Odyssey (2006)\n      Comedy\n      4.919813\n    \n    \n      3\n      107434\n      Diplomatic Immunity (2009– )\n      Comedy\n      4.923127\n    \n    \n      4\n      110669\n      Honest Liar, An (2014)\n      Comedy|Documentary\n      4.900411\n    \n    \n      5\n      118338\n      Hard to Be a God (2013)\n      Sci-Fi\n      4.911441\n    \n    \n      6\n      120134\n      Doggiewoggiez! Poochiewoochiez! (2012)\n      Comedy\n      5.056443\n    \n    \n      7\n      121029\n      No Distance Left to Run (2010)\n      Documentary\n      5.534087\n    \n    \n      8\n      128091\n      Craig Ferguson: A Wee Bit o' Revolution (2009)\n      Comedy\n      5.608799\n    \n    \n      9\n      130347\n      Bill Hicks: Sane Man (1989)\n      Comedy\n      5.114568\n    \n  \n\n\n\n\n[Stage 666:>                                                       (0 + 4) / 10]"
  },
  {
    "objectID": "posts/spark/movielens.html#thank-you",
    "href": "posts/spark/movielens.html#thank-you",
    "title": "Movie Lens - Collaborative Filtering based Recommendation System",
    "section": "Thank you",
    "text": "Thank you\nShare if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "posts/spark/movielens.html#resources",
    "href": "posts/spark/movielens.html#resources",
    "title": "Movie Lens - Collaborative Filtering based Recommendation System",
    "section": "Resources",
    "text": "Resources\n\nML Tuning: Model Selection and Hyperparameter Tuning Using PySpark\nALS\nPySpark Documentation"
  },
  {
    "objectID": "posts/spark/movielens.html#data-citation",
    "href": "posts/spark/movielens.html#data-citation",
    "title": "Movie Lens - Collaborative Filtering based Recommendation System",
    "section": "Data Citation",
    "text": "Data Citation\nF. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=http://dx.doi.org/10.1145/2827872"
  },
  {
    "objectID": "drafts/01-knapsack.html",
    "href": "drafts/01-knapsack.html",
    "title": "Sagar Thacker",
    "section": "",
    "text": "Given weights and values of n items, put these items in a knapsack of capacity W to get the maximum total value in the knapsack.\nKnapsack problems are divided into three types:\n\nFractional Knapsack (Greedy)\n0/1 Knapsack\nUnbounded Knapsack\n\n\n\n\nChoice:\n\nWe choose to put or not to put item in the bag\n\nOptimize:\n\nMaximize the profit\n\n\n\n\n\n\nBase Case: Always think of the smallest possible valid input\nRecursive function: Call the function recursively with smaller inputs\n\n\n# Import required libraries\nimport math\n\n\n# Top-Down: Recusive-Memoized Implementation\ndef knapsack(weight: list, value: list, capacity: int, n: int) -> int:\n\n    def helper(capacity: int, idx: int):\n        # Base Condition\n        if (idx == 0) or (capacity == 0):\n            return 0\n        if (capacity, idx) in memo:\n            return memo[(capacity, idx)]\n        \n        # Choice diagram\n        if weight[idx-1] <= capacity:\n            memo[(capacity, idx)] = max(value[idx-1] + helper(capacity - weight[idx-1], idx-1), helper(capacity, idx-1))\n        else:\n            memo[(capacity, idx)] = knapsack(capacity, idx-1)\n        return memo[(capacity, idx)]\n    \n    memo = dict()\n    return helper(capacity, n)\n\nweight = [1,2,3,4]\nvalue = [1,3,4,5]\ncapacity = 7\nn = 4\nknapsack(weight, value, capacity, n)\n\n9\n\n\n\n# Bottom-Up: Iterative Implmentation\ndef knapsack_iter(weight: list, value: list, capacity: int, n: int) -> int:\n    length = len(weight)\n    # Convert base case to initialization\n    memo = [[0] * (capacity + 1) for _ in range(length + 1)]\n\n    # Choice diagram\n    for i in range(1, length + 1):\n        for j in range(1, capacity + 1):\n            if weight[i-1] <= j:\n                memo[i][j] = max(value[i-1] + memo[i-1][j - weight[i-1]], memo[i-1][j])\n            else:\n                memo[i][j] = memo[i-1][j]\n    return memo[-1][-1]\n\nweight = [1,2,3,4]\nvalue = [1,3,4,5]\ncapacity = 7\nn = 4\nknapsack_iter(weight, value, capacity, n)\n\n9\n\n\nComplexity Analysis: - Time Complexity: \\(T(n) = O(N*W)\\) where N is the number of weight elements and W is the capacity. - Space Complexity: \\(O(N*W)\\) to store the itermediatary results in the dictionary.\n\n\n\n\nSubset Sum\nEqual Subset Sum Partition\nCount of Subset Sum of a given sum\nMinimum Subset Sum Difference\nNumber of subset of given difference\nTarget Sum\n\n\n\nGiven a set of non-negative integers, and a value sum, determine if there is a subset of the given set with sum equal to given sum.\n\n\n\n\n# Top-Down: Recusive-Memoized Implementation\ndef subsetSum(weight: list, target: int) -> bool:\n    if len(weight) == 0 and target == 0:\n        return True\n    \n    def helper(idx, target):\n        # Base Condition\n        if target == 0:\n            return True\n        if idx == 0:\n            return False\n        if (idx, target) in memo:\n            return memo[(idx, target)]\n\n        # Choice Diagram\n        if weight[idx - 1] <= target:\n            memo[(idx, target)] = helper(idx-1, target - weight[idx-1]) or helper(idx-1, target)\n        else:\n            memo[(idx, target)] = helper(idx-1, target)\n        \n        return memo[(idx, target)]\n    \n    n = len(weight)\n    memo = dict()\n    return helper(n, target)\n    \n\nweight = [2, 3, 7, 8, 10]\ntarget = 11\nprint(subsetSum(weight, target))\n\nweight = [2, 3, 7, 8, 10]\ntarget = 0\nprint(subsetSum(weight, target))\n\nweight = []\ntarget = 0\nprint(subsetSum(weight, target))\n\nweight = []\ntarget = 1\nprint(subsetSum(weight, target))\n\nTrue\nTrue\nTrue\nFalse\n\n\n\n# Bottom-Up: Iterative Implmentation\ndef subsetSum_iter(weight: list, target: int) -> bool:\n    length = len(weight)\n    \n    # Convert base case to initialization\n    memo = [[False] * (target + 1) for _ in range(length + 1)]\n    for _ in range(length + 1):\n        memo[_][0] = True\n\n    # Choice diagram\n    for i in range(1, length + 1):\n        for j in range(1, target + 1):\n            if weight[i-1] <= j:\n                memo[i][j] = memo[i-1][j - weight[i-1]] or memo[i-1][j]\n            else:\n                memo[i][j] = memo[i-1][j]\n    return memo[-1][-1]\n\nweight = [2, 3, 7, 8, 10]\ntarget = 11\nprint(subsetSum(weight, target))\n\nweight = [2, 3, 7, 8, 10]\ntarget = 0\nprint(subsetSum(weight, target))\n\nweight = []\ntarget = 0\nprint(subsetSum(weight, target))\n\nweight = []\ntarget = 1\nprint(subsetSum(weight, target))\n\nTrue\nTrue\nTrue\nFalse\n\n\n\n\n\nGiven a non-empty array nums containing only positive integers, find if the array can be partitioned into two subsets such that the sum of elements in both subsets is equal.\n\ndef equalSubsetSum(arr: list) -> bool:\n    totalSum = sum(arr)\n    if totalSum % 2:\n        return False\n    \n    target = totalSum // 2\n    return subsetSum(arr, target)\n\narr = [1, 5, 11, 5]\nprint(equalSubsetSum(arr))\n\narr = [2, 1, 1, 2, 3]\nprint(equalSubsetSum(arr))\n\nTrue\nFalse\n\n\n\n\n\n\ndef countSubsetSum(weight: list, target: int) -> int:\n    if len(weight) == 0 and target == 0:\n        return 1\n    \n    def helper(idx, target):\n        # Base Condition\n        if target == 0:\n            return 1\n        if idx == 0:\n            return 0\n        \n        if (idx, target) in memo:\n            return memo[(idx, target)]\n\n        # Choice Diagram\n        if weight[idx - 1] <= target:\n            memo[(idx, target)] = helper(idx-1, target - weight[idx-1]) + helper(idx-1, target)\n        else:\n            memo[(idx, target)] = helper(idx-1, target)\n        \n        return memo[(idx, target)]\n    \n    n = len(weight)\n    memo = dict()\n    return helper(n, target)\n\nweight = [2, 3, 5, 6, 8, 10]\ntarget = 10\nprint(countSubsetSum(weight, target))\n\nweight = [3, 3, 3, 3]\ntarget = 6\nprint(countSubsetSum(weight, target))\n\n3\n6\n\n\n\n# Bottom-Up: Iterative Implmentation\ndef countSubsetSum_iter(weight: list, target: int) -> bool:\n    length = len(weight)\n    \n    # Convert base case to initialization\n    memo = [[0] * (target + 1) for _ in range(length + 1)]\n    for _ in range(length + 1):\n        memo[_][0] = 1\n\n    # Choice diagram\n    for i in range(1, length + 1):\n        for j in range(1, target + 1):\n            if weight[i-1] <= j:\n                memo[i][j] = memo[i-1][j - weight[i-1]] + memo[i-1][j]\n            else:\n                memo[i][j] = memo[i-1][j]\n    return memo[-1][-1]\n\nweight = [2, 3, 7, 8, 10]\ntarget = 11\nprint(countSubsetSum_iter(weight, target))\n\nweight = [2, 3, 7, 8, 10]\ntarget = 0\nprint(countSubsetSum_iter(weight, target))\n\nweight = []\ntarget = 0\nprint(countSubsetSum_iter(weight, target))\n\nweight = []\ntarget = 1\nprint(countSubsetSum_iter(weight, target))\n\nweight = [2, 3, 5, 6, 8, 10]\ntarget = 10\nprint(countSubsetSum_iter(weight, target))\n\nweight = [3, 3, 3, 3]\ntarget = 6\nprint(countSubsetSum_iter(weight, target))\n\n1\n1\n1\n0\n3\n6\n\n\n\n\n\nGiven a set of integers, the task is to divide it into two sets S1 and S2 such that the absolute difference between their sums is minimum.\n\ndef minSubsetSumDiff(weight: list) -> int:\n    if len(weight) == 0 and target == 0:\n        return 0\n    \n    total_range = sum(weight)\n\n    def helper(idx, target):\n        # Base Condition\n        if idx == 0:\n            return abs(total_range - 2 * target)\n        if (idx, target) in memo:\n            return memo[(idx, target)]\n\n        # Choice Diagram\n        if weight[idx - 1] <= target:\n            memo[(idx, target)] = min(helper(idx-1, target - weight[idx-1]), helper(idx-1, target))\n        else:\n            memo[(idx, target)] = helper(idx-1, target)\n        \n        return memo[(idx, target)]\n    \n    n = len(weight)\n    memo = dict()\n    return helper(n, total_range)\n\narr = [1,6,11,5]\nprint(minSubsetSumDiff(arr))\n\n1\n\n\n\ndef minSubsetSumDiff_iter(weight: list) -> int:\n    length = len(weight)\n    total_range = sum(weight)\n    \n    # Convert base case to initialization\n    memo = [[False] * (total_range + 1) for _ in range(length + 1)]\n    for _ in range(length + 1):\n        memo[_][0] = True\n\n    # Choice diagram\n    for i in range(1, length + 1):\n        for j in range(1, total_range + 1):\n            if weight[i-1] <= j:\n                memo[i][j] = memo[i-1][j - weight[i-1]] or memo[i-1][j]\n            else:\n                memo[i][j] = memo[i-1][j]\n    \n    half = total_range // 2\n    minimum = math.inf\n    for col in range(half + 1):\n        if memo[-1][col]:\n            minimum = min(minimum, total_range - 2 * col)\n    return minimum\n\n# arr = [1,2,7]\n# arr = [1,6,11,5]\narr = []\nprint(minSubsetSumDiff_iter(arr))\n\n0\n\n\n\n\n\n\ndef countSubsetDiff(arr: list, diff: int) -> int:\n    target = (sum(arr) + diff) // 2\n    return countSubsetSum(arr, target)\n\narr = [1, 1, 2, 3]\ndiff = 1\ncountSubsetDiff(arr, diff)\n\n3\n\n\n\n\n\n\ndef targetSum(arr, target):\n    return countSubsetDiff(arr, target)\n\narr = [1, 1, 2, 3]\ntarget_sum = 1\ntargetSum(arr, target_sum)\n\n3\n\n\n\nfrom pprint import pprint\nexpense = {\n    \"splitwise\": 50.15,\n    \"credit card\": 498.33,\n    \"nov rent\": 325,\n    \"electricity\": 20,\n    \"gas\": 30,\n    \"gifts\": 21,\n    \"food + misc\": 100,\n    \"OPT\": 410,\n    \"I-20\": 55,\n    \"birthday\": 50\n}\nincome = {\n    \"splitwise\": 138.92,\n    \"bank\": 156.97\n}\npprint(expense)\npprint(income)\nprint(sum(expense.values()))\nprint(sum(income.values()))\nprint(sum(income.values()) - sum(expense.values()))\n\n{'I-20': 55,\n 'OPT': 410,\n 'birthday': 50,\n 'credit card': 498.33,\n 'electricity': 20,\n 'food + misc': 100,\n 'gas': 30,\n 'gifts': 21,\n 'nov rent': 325,\n 'splitwise': 50.15}\n{'bank': 156.97, 'splitwise': 138.92}\n1559.48\n295.89\n-1263.5900000000001"
  },
  {
    "objectID": "drafts/python-basics.html",
    "href": "drafts/python-basics.html",
    "title": "Sagar Thacker",
    "section": "",
    "text": "1. Basic Syntax\n\n\n2. Variables and Data Types\n\n\n3. Conditionals\n\n\n4. Type Casting, Exceptions\n\n\n5. Functions, Builtin Functions\n\n\n6. List, Tuples, Sets, Dictionaries"
  },
  {
    "objectID": "drafts/pyspark_learn.html",
    "href": "drafts/pyspark_learn.html",
    "title": "Sagar Thacker",
    "section": "",
    "text": "PySpark is an interface for Apache Spark in Python, often used for large scale data processing and machine learning.\nWhy Apache Spark is good?\n\nIf we have huge amoung of data and max size of ram is 32/64 gb.\nWhat if data is of 128gb and ram is of 32 gb. Hence we would process the data in some distributed manner or distributed system.\nThis is where apache spark helps us.\n\nIt runs in a cluster mode i.e. distributed mode."
  },
  {
    "objectID": "drafts/pyspark_learn.html#installation",
    "href": "drafts/pyspark_learn.html#installation",
    "title": "Sagar Thacker",
    "section": "Installation",
    "text": "Installation\n\n!pip install pyspark\nBefore starting to work with pyspark we need to start a Spark Session.\n\n\n# Creating a spark session\nfrom pyspark.sql import SparkSession\n\n# 'practice' is the name of the session\nspark = SparkSession.builder.appName('practice').getOrCreate()\n\n## Basics of Dataframe\n# Read the dataset\ndf_pyspark = spark.read.csv('test1.csv')\ndf_pyspark\ndf_pyspark.show()\n\ndf_pyspark = spark.read.option('header', 'true').csv('test1.csv').show()\n\n# See more information about columns\n# i.e. check the schema\n# Similar to df.info() in pandas\ndf_pyspark.printSchema()\n\n# By default it read each value in the file as string\n# To read values with proper datatypes, we need to\n# set `inferSchema=True` while reading\ndf_pyspark = spark.read.option('header', 'true').csv('test1.csv', inferSchema=True)\ndf_pyspark.printSchema()\n\n# Another way to read\ndf_pyspark = spark.read.csv('test1.csv', header=True, inferSchema=True)\ndf_pyspark.show()\n\n\n# Column name\ndf_pyspark.columns\n\ndf_pyspark.head(3)\n\n# Get a column and see all ites elements\ndf_pyspark.select('Name') # Gives the return type\ndf_pyspark.select('Name').show() # Dsiplay's the dataframe\n\ndf_pyspark.select(['Name', 'Experience']) # Multiple Columns\ndf_pyspark.select(['Name', 'Experience']).show()\n\n#### Slicing does not work in PySpark\n\ndf_pyspark['Name'] # Return type - Column, has no show function\n\n# Check datatypes\ndf_pyspark.dtypes\n\n# Describe option similar to pandas\ndf_pyspark.describe()\ndf_pyspark.describe().show() # In table format\n\n# Adding columns in dataframe\ndf_pyspark.withColumn('Experience After 2 years', df_pyspark['Experience'] + 2).show()\n\n# Need to assign if you want to save the changes\ndf_pyspark = df_pyspark.withColumn('Experience After 2 years', df_pyspark['Experience'] + 2)\n\n# Drop columns from dataframe\ndf_pyspark = df_pyspark.drop('Experience After 2 years') # By default takes a column name\n\n# Rename a column\ndf_pyspark.withColumnRenamed('Name', 'New Name')\n\n\n# PySpark Handling Missing values\n\nspark = SparkSession.builder.appName('Practice').getOrCreate()\n\ndf_pyspark = spark.read.csv('test2.csv', header=True, inferSchema=True)\ndf_pyspark.show()\n\n# Dropping Columns\ndf_pyspark.drop('Name')\n\n# Whereever there is a null value it will drop that row\ndf_pyspark.na.drop().show()\n\n## Filling missing values\n# Where ever there is null, it will fill it\ndf_pyspark.na.fill('Missing value').show()\ndf_pyspark.na.fill('Missing value', ['Experience', 'Age']).show() # Subset also\n\n# Dropping Rows\n\n# Various Parameters in dropping functionalities\n## Drop paramters\n# any==how\ndf_pyspark.na.drop(how=\"all\").show() # Drop those record that have all null values\n# threshold\n# At least two non null values should be persent, less than that dropped\ndf_pyspark.na.drop(how=\"all\", threshold=2).show()\n# subset - some specific subset of columns\ndf_pyspark.na.drop(how=\"all\", subset=['Experience']).show()\n\n# Handling Missing value by mean, median, and mode.\n# Using an imputer to impute values\nfrom pyspark.ml.feature import Imputer\n\nimputer = Imputer(\n    inputCols=['age', 'Experience', 'Salary'],\n    outputCols = [\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']]\n).setStratergy(\"mean\")\n\n# Fit and transform\nimputer.fit(df_pyspark).transform(df_pyspark).show()\n\n\n# PySpark Dataframes\n'''\n- Filter Operation\n- & | ==\n- ~\n'''\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('dataframe').getOrCreate()\n\ndf_pyspark = spark.read.csv('test1.csv', header=True, inferSchema=True)\n\ndf_pyspark.show()\n\n# Filter Operations\n## Way 1: Salary of people less than or equal to 20000\ndf_pyspark.filter(\"Salary<=20000\").show()\ndf_pyspark.filter(\"Salary<=20000\").select(['Name', 'age']).show()\n\n# Way 2:\ndf_pyspark.filter(df_pyspark['Salary'] <= 20000).show()\ndf_pyspark.filter((df_pyspark['Salary'] <= 20000) & (df_pyspark['Salary'] >= 15000)).show()\n\ndf_pyspark.filter(~(df_pyspark['Salary'] <= 20000)).show()\n\n\n# PySpark GroupBy and Aggregate functions\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('Agg').getOrCreate()\n\ndf_pyspark = spark.read.csv('test3.csv', header=True, inferSchema=True)\n\ndf_pyspark.show()\n\ndf_pyspark.printSchema()\n\n## Group By\n### Grouped to find the maximum salary\ndf_pyspark.groupBy('Name').sum().show()\n\n### Groupby department which gives maximum salary\ndf_pyspark.groupBy('Departments').sum().show()\ndf_pyspark.groupBy('Departments').mean().show()\ndf_pyspark.groupBy('Departments').count().show()\n\n### Direct aggregate functions\ndf_pyspark.agg({'Salary': 'sum'}).show()\n\n\n# Example of PySpark ML\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('ML').getOrCreate()\n\ntraining = spark.read.csv('test1.csv', header=True, inferSchema=True)\n\ntraining.show()\ntraining.printSchema()\ntraining.columns\n\n# VectorAssembler to group features\nfrom pyspark.ml.feature import VectorAssembler\nassembler = VectorAssembler(inputCols=['age','Experience'],outputCol='Independent Features')\n\noutput = assembler.transform(training)\n\noutput.show()\n\nfinalized_data = output.select('Independent Features', \"Salary\")\n\nfinalized_data.show()\n\n# Train Test Split\nfrom pyspark.ml.regression import LinearRegression\ntrain_data, test_data = finalized_data.randomSplit([0.75, 0.25])\nregressor = LinearRegression(featuresCol='Independent Features', labelCol='Salary')\nregressor=regressor.fit(train_data)\n\nregressor.coefficients\n\nregressor.intercept\n\n# Prediction\npred_results = regressor.evaluate(test_data)\npred_results.predictions.show()\n\npred_results.meanAbsoluteError\npred_results.meanSquaredError\n\n## Handling Categorical Features\nfrom pyspark.ml.feature import StringIndexer\nindexer = StringIndexer(inputCol='sex',outputCol='sex_indexed')\n# multiple columns\nindexer = StringIndexer(inputCols=['smoker','day','time'],outputCols=['smoker_indexed','day_indexed','time_indexed'])\ndf_r=indexer.fit(df_pyspark).transform(df_pyspark)\ndf_r.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sagar Thacker",
    "section": "",
    "text": "Blog Series:\n\nNatural Language Processing\nKaggle Competitions\n\n\n“Don’t focus on having a great blog. Focus on producing a blog that’s great for your readers.” - Brian Clark\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nAWS Instance Setup\n\n\n\n\n\n\n\nAWS\n\n\n\n\nStep-by-step guide to setup AWS EC2 Instance and setup environment\n\n\n\n\n\n\nMay 19, 2023\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to MLOps\n\n\n\n\n\n\n\nMLOPs\n\n\n\n\nLearn how to combine machine learning with software engineering to develop, deploy & maintain production ML applications.\n\n\n\n\n\n\nMay 18, 2023\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nEDA - Playground Series Season 3, Episode 4\n\n\n\n\n\n\n\nKaggle\n\n\n\n\nTabular Classification with a Credit Card Fraud Dataset\n\n\n\n\n\n\nJan 30, 2023\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nModeling - Playground Series Season 3, Episode 4\n\n\n\n\n\n\n\nKaggle\n\n\n\n\nTabular Classification with a Credit Card Fraud Dataset\n\n\n\n\n\n\nJan 30, 2023\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nMovie Lens - Collaborative Filtering based Recommendation System\n\n\n\n\n\n\n\nPySpark\n\n\n\n\nMovie Recommendation Using PySpark\n\n\n\n\n\n\nJan 20, 2022\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nSong Popularity EDA\n\n\n\n\n\n\n\nKaggle\n\n\n\n\nSong Popularity Prediction is a competition on Kaggle. This post is all about performing exploratory data analysis following the coding session by Martin Henze.\n\n\n\n\n\n\nMay 31, 2021\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nText Preprocessing\n\n\n\n\n\n\n\nText Preprocessing\n\n\nNatural Language Processing\n\n\nNLP\n\n\n\n\nVairous preprocessing steps required to clean and prepare your data in NLP\n\n\n\n\n\n\nApr 19, 2021\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nWhat is Natural Language Processing?\n\n\n\n\n\n\n\nNatural Language Processing\n\n\nNLP\n\n\n\n\nA brief summary of what is natural language processing, it’s challenges and applications.\n\n\n\n\n\n\nApr 12, 2021\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nPath to become a Machine Learning Expert\n\n\n\n\n\n\n\nMachine Learning\n\n\nLearning Path\n\n\n\n\nComprehensive learning path to become an expert in Machine Learning\n\n\n\n\n\n\nApr 5, 2021\n\n\n7 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Hello,  I’m Sagar Thacker",
    "section": "",
    "text": "I’m a research scientist at University at Buffalo where I focus on identifying white supremacist extremist language in online hate speech. I am fortunate to be advised by Professor Kenneth Joseph.\nMy expertise lies in conducting research and development and creating software solutions using Natural Language Processing (NLP), Applied Machine Learning (ML), and Information Access. I have a strong interest in understanding entire ML lifecycle end-to-end to evaluate the business requirements; run experiments; build data and machine learning (ML) models; evaluate performance and business impact; deploy the models; setup long-term monitoring.\nI recently completed my Master’s degree in Computer Science from the University at Buffalo, The State University of New York in December 2022. During that period, one of my most exciting and rewarding project I worked upon involved various shared tasks from Social Media Mining for Health Applications (SMM4H) 2020 & 2021 to tackle problems in the health care domain. You can read more about this project here, along with my report.\nBefore moving to the USA, I worked remotely as a Data Scientist at FedEx Express. My responsibility included End-to-End Research, Design, Developement, Deployment of NLP, and classical ML classification tasks . I worked on projects such as identifying customer sentiments towards our services, classifying customer tickets to relevant departments for efficient resolution, building Extract-Transform-Load (ETL) pipelines, and developing dashboards to improve communication, productivity, and save time and money.\nIn 2018, I got the opportunity to work as a Assistant System Engineer at Tata Consultancy Services. I worked on developing automated test scripts to simulate testing process in web apps, model customer/software system interaction in ARD for automatic tests scripts generation.\nDuring my undergraduate studies at Ramrao Adik Institute of Technology (RAIT), I majored in Computer Science and actively participated in cultural events and competitions. I also served as the Administrator of Social Wing RAIT, a non-profit organization dedicated to creating a better society. In this role, I managed various events focused on making a positive impact, such as tree planting, teaching at orphanages, donation drives, and clean-up efforts.\nOne of our most successful events was a marathon fundraiser, with all proceeds going towards providing education for underprivileged children in our community for an entire academic year. Thanks to the overwhelming support of our community, we were able to provide education for more than 20 children, making a tangible and meaningful difference in their lives.\nOutside of work, I have a keen interest in learning about our galaxy, black holes, and physics. I also enjoy exploring different cuisines, watching football, and playing Dota 2. Connecting with others is something I truly enjoy, so if you share similar interests or would like to connect, please feel free to reach out. I look forward to hearing from you! 🙌\n\nMy Journey\n\n\n“Stay hungry. Stay foolish.” - Steve Job\n\n\n\n\n\n\n\n2021 - 2023\n\n\n\n\n\nUniversity at Buffalo, The State University of New York\n\n\nMaster of Science, Computer Science\n\n\n\n\n\n\n2014 - 2018\n\n\n\n\n\nUniversity of Mumbai\n\n\nBachelors of Engineering, Computer Engineering\n\n\n\n\n\n\n\n\n\n\n\n2020 - 2021\n\n\n\n\n\nData Scientist\n\n\nFedEx Express\n\n\n\n\n\n\n2018 - 2019\n\n\n\n\n\nAssistant System Engineer\n\n\nTata Consultancy Services\n\n\n\n\n\n\n\n\n\nAchievements\n\n\n\nSecured second place in the H2O World India Hackathon held in April 2023."
  },
  {
    "objectID": "nlp_series.html",
    "href": "nlp_series.html",
    "title": "Natural Language Processing",
    "section": "",
    "text": "What is Natural Language Processing\nText Preprocessing"
  }
]