{
  "hash": "54690c15adf1e38cd56de7adbc37bcab",
  "result": {
    "markdown": "---\ntitle: Prefect Blocks\ndescription: 'Prefect Blocks are reusable, composable building blocks for orchestrating data workflows.'\nauthor: Sagar Thacker\ndate: '2023-06-07'\ncategories:\n  - MLOps\n  - Prefect\nformat:\n  html:\n    toc: true\n    code-summary: Show the code\n    code-line-numbers: true\n    code-block-background: true\nexecute:\n  echo: false\n---\n\nIn this post, we'll be building up on the concepts we discussed in the previous post. If you haven't read that, I highly recommend checking it out before you continue reading this. You can find the post [here](https://sagarthacker.com/posts/mlops/intro_workflow_orchestration.html).\n\nIn the previous post, we build a simple prefect flow that downloads a file from a URL and saves it to a local directory. We used MLflow to track our experiment - train a XGBoost model on the downloaded data. In this post, we'll be implementing the same workflow but this time we'll fetch the data from S3 instead of downloading it from a URL. \n\nWe'll setup the S3 bucket and also create a new IAM user and attach the AmazonS3FullAccess policy to it. We'll use the Access key and Secret key to access the S3 bucket from our code.\n\n## Setup S3 Bucket\n\n1. Log into your AWS account and click on `Services` on the top left corner. Search for `S3` and click on it. \n2. Click on `Create bucket` and give it a name. Note: Bucket names must be unique across all AWS accounts.\n3. Keep the default settings and click on `Create bucket`.\n\n## Upload the data to the S3 bucket\n\nDownload the data on your local machine and upload it to the S3 bucket. To keep it simple we'll use the GUI to upload the data to the S3 bucket. Use the following command in your terminal / command prompt to download the data.\n\n```bash\n# Download dataset\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-01.parquet # January 2022\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-02.parquet # February 2022\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-03.parquet # March 2022\n```\n\nNow, let's upload the data to the S3 bucket. Follow the steps below to upload the data to the S3 bucket.\n\n1. Click on the bucket you just created.\n2. Create a new folder by clicking on `Create folder` and give it a name `data`.\n3. Click on `Upload` and select the file you want to upload.\n4. Click on `Upload` again and wait for the upload to complete.\n\nAwesome! We now have the data in the S3 bucket.\n\n## Create IAM user\n\nWe'll now create a new IAM user and attach the AmazonS3FullAccess policy to it. We'll use the Access key and Secret key to access the S3 bucket from our code. Follow the steps below to create a new IAM user.\n\n1. Click on `Services` on the top left corner. Search for `IAM` and click on it.\n2. Click on `Users` on the left sidebar and then click on `Add user`.\n3. Give the user a name and click on `Next`.\n4. It is recommended to use groups to manage user's permissions. Click on `Create group` and give it a name. Search for `AmazonS3FullAccess` in the Permissions policies table and select it. Click on `Create user group`.\n5. Now, select the group you just created and click on `Next`. \n6. Click on `Create User`.\n\n### Generate Access key and Secret key\n\n1. Click on the user you just created.\n2. Click on `Security credentials` tab and under the Access keys section click on `Create access key`.\n3. Select the `Other` option and click on `Next` and then click on `Create access key`.\n4. Click on `Show` to view the Access key and Secret key. Note: You won't be able to view the Secret key again so make sure you save it somewhere safe. Alternatively, you can also download the CSV file using the `Download .csv file` button.\n\nWe're all setup now. Let's get started with Prefect Blocks.\n\n## Prefect Blocks\n\nBlocks serve as a fundamental element in Prefect, allowing the storage of configuration information and providing an interface for seamless interaction with external systems. \n\nBlocks offer a secure way to store authentication credentials for various services such as AWS, GitHub, Slack, and more, enabling seamless integration with Prefect.\n\nBy utilizing blocks, you gain access to convenient methods that facilitate interactions with external systems. For instance, you can effortlessly download or upload data from/to an S3 bucket, query or write data in a database, or send messages to Slack channels.\n\nConfiguring blocks can be done either programmatically or through the user-friendly interfaces of Prefect Cloud and the Prefect server UI. This flexibility allows you to manage blocks based on your preferred approach.\n\nYou can find blocks on the Prefect UI under the `Blocks` section. Click on `Add Block` to see the different blocks available. \n\n![](https://docs.prefect.io/img/ui/block-library.png)\n\n::: {.callout-note}\nDon't worry if you don't see the different blocks in your UI. We'll be adding them in the next section. You can skip this section if you find `S3 Bucket` and `AWS Credentials` Blocks in the UI.\n:::\n\n### Adding blocks to Prefect UI\n\n::: {.callout-note}\nWe'll be using the same environment we created in the previous post. If you haven't created the environment, you can find the instructions [here](https://sagarthacker.com/posts/mlops/intro_workflow_orchestration.html#setup).\n:::\n\nTo see the different blocks Prefect has to offer you can find them [here](https://docs.prefect.io/integrations/). Prefect also allows you to create your own custom blocks. You can find the documentation [here](https://docs.prefect.io/2.10.12/concepts/blocks/#creating-new-block-types).\n\nFor this post, we'll be using the AWS integration `prefect-aws` to use the following blocks:\n\n- `S3Bucket`\n- `AwsCredentials`\n\nTo use these blocks you can pip install the package, then register the blocks you want to use with Prefect Cloud or a Prefect server. Follow the steps below:\n\n1. Install the `prefect_aws` package in your environment.\n\n```bash\npip install prefect_aws\n```\n\n2. Register the blocks using the `prefect register` command. `-m` flag is used to specify the module name.\n\n```bash\nprefect block register -m prefect_aws\n```\n\nNow you should be able to see the blocks in the Prefect UI.\n\n### Create S3 Bucket block\n\nWe'll now create a new file `create_s3_bucket.py` to store the credentials and the S3 bucket configurations. We'll be using the `AwsCredentials` and `S3Bucket` blocks to create the S3 bucket.\n\n```{.python filename=\"create_s3_bucket.py\"}\nfrom time import sleep\nfrom prefect_aws import S3Bucket, AwsCredentials\n\n# Create and save AWS credentials block\ndef create_aws_creds_block():\n    # Create AWS credentials object\n    my_aws_creds_obj = AwsCredentials(\n        aws_access_key_id = \"<ACCESS_KEY>\", # Replace with your access key\n        aws_secret_access_key = \"<SECRET_KEY>\" # Replace with your secret key\n    )\n    \n    # Save the AWS credentials block\n    my_aws_creds_obj.save(name=\"my-aws-creds\", overwrite=True)\n\n# Create and save S3 bucket block\ndef create_s3_bucket_block():\n    # Load AWS credentials block\n    aws_creds = AwsCredentials.load(\"my-aws-creds\")\n    \n    # Create S3 bucket object\n    my_s3_bucket_obj = S3Bucket(\n        bucket_name = \"<BUCKET_NAME>\", # Replace with your bucket name\n        credentials = aws_creds\n    )\n    \n    # Save the S3 bucket block\n    my_s3_bucket_obj.save(name=\"s3-bucket-example\", overwrite=True)\n\n\nif __name__ == \"__main__\":\n    # Create and save AWS credentials block\n    create_aws_creds_block()\n    \n    # Sleep for 5 seconds to allow time for the AWS credentials to be saved\n    sleep(5)\n    \n    # Create and save S3 bucket block\n    create_s3_bucket_block()\n```\n\nLet's understand what's happening in the code above.\n\n1. We first create an `AwsCredentials` object and pass the Access key and Secret key to it. We then save the object as a block using the `save` method. We also pass the `overwrite` argument as `True` to overwrite the block if it already exists. (Note: Use the Access key and Secret key you created in the previous section)\n2. We then create an `S3Bucket` object and pass the bucket name and the `AwsCredentials` object to it. We then save the object as a block using the `save` method. We also pass the `overwrite` argument as `True` to overwrite the block if it already exists.\n\nThis allows to store the credentials and configurations in a secure way, and use them in our workflows.\n\n`prefect block ls` command can be used to list all the blocks. However, since we haven't registered the above below in Prefect, we won't be able to see them.\n\nWe'll run the script to create and save the blocks in Prefect. Before running the script, make sure you start the prefect server.\n\n```bash\n# Make sure to start the prefect server before running the script\nprefect server start\n\n# Run the script (In new terminal window)\npython create_s3_bucket.py\n```\n\nIf you run the command `prefect block ls` you should be able to see the blocks. Similarly if you go to the Prefect UI you should be able to see the blocks under the `Blocks` section.\n\n![](./images/prefect_blocks/prefect_block_ls.png)\n\n![](./images/prefect_blocks/prefect_block_ui.png)\n\nNow that we have the blocks setup, we can use them in our workflow.\n\n## Workflow\n\nWe'll be using the same workflow we created in the previous post with some modifications. We'll be using the `S3Bucket` block to download the data from the S3 bucket.\n\nWe import the S3Bucket Block using the command `from prefect_aws import S3Bucket`.\n\n```{.python filename=\"orchestration_s3.py\"}\nimport os\nimport pickle\nimport pathlib\nimport scipy\nimport mlflow\nimport sklearn\nimport argparse\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.metrics import mean_squared_error\nfrom prefect import flow, task\nfrom prefect_aws import S3Bucket\n\n@task(name=\"Read a Parquet file\")\ndef read_data(filename: str) -> pd.DataFrame:\n    \"\"\"Read data into DataFrame\"\"\"\n    df = pd.read_parquet(filename)\n\n    df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n    df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)\n\n    df[\"duration\"] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n\n    df = df[(df.duration >= 1) & (df.duration <= 60)]\n\n    categorical = [\"PULocationID\", \"DOLocationID\"]\n    df[categorical] = df[categorical].astype(str)\n\n    return df\n\n@task(name=\"Add Features\")\ndef add_features(df_train: pd.DataFrame, df_val: pd.DataFrame) -> tuple([\n        scipy.sparse._csr.csr_matrix,\n        scipy.sparse._csr.csr_matrix,\n        np.ndarray,\n        np.ndarray,\n        sklearn.feature_extraction.DictVectorizer,\n    ]):\n    \"\"\"Add features to the model\"\"\"\n    df_train[\"PU_DO\"] = df_train[\"PULocationID\"] + \"_\" + df_train[\"DOLocationID\"]\n    df_val[\"PU_DO\"] = df_val[\"PULocationID\"] + \"_\" + df_val[\"DOLocationID\"]\n\n    categorical = [\"PU_DO\"]  #'PULocationID', 'DOLocationID']\n    numerical = [\"trip_distance\"]\n\n    dv = DictVectorizer()\n\n    train_dicts = df_train[categorical + numerical].to_dict(orient=\"records\")\n    X_train = dv.fit_transform(train_dicts)\n\n    val_dicts = df_val[categorical + numerical].to_dict(orient=\"records\")\n    X_val = dv.transform(val_dicts)\n\n    y_train = df_train[\"duration\"].values\n    y_val = df_val[\"duration\"].values\n    return X_train, X_val, y_train, y_val, dv\n\n@task(name=\"Train Model\", log_prints=True)\ndef train_best_model(\n    X_train: scipy.sparse._csr.csr_matrix,\n    X_val: scipy.sparse._csr.csr_matrix,\n    y_train: np.ndarray,\n    y_val: np.ndarray,\n    dv: sklearn.feature_extraction.DictVectorizer,\n) -> None:\n    \"\"\"train a model with best hyperparams and write everything out\"\"\"\n\n    with mlflow.start_run():\n        train = xgb.DMatrix(X_train, label=y_train)\n        valid = xgb.DMatrix(X_val, label=y_val)\n\n        best_params = {\n            \"learning_rate\": 0.09585355369315604,\n            \"max_depth\": 30,\n            \"min_child_weight\": 1.060597050922164,\n            \"objective\": \"reg:linear\",\n            \"reg_alpha\": 0.018060244040060163,\n            \"reg_lambda\": 0.011658731377413597,\n            \"seed\": 42,\n        }\n\n        mlflow.log_params(best_params)\n\n        booster = xgb.train(\n            params=best_params,\n            dtrain=train,\n            num_boost_round=100,\n            evals=[(valid, \"validation\")],\n            early_stopping_rounds=20,\n        )\n\n        y_pred = booster.predict(valid)\n        rmse = mean_squared_error(y_val, y_pred, squared=False)\n        mlflow.log_metric(\"rmse\", rmse)\n\n        pathlib.Path(\"models\").mkdir(exist_ok=True)\n        with open(\"models/preprocessor.b\", \"wb\") as f_out:\n            pickle.dump(dv, f_out)\n        mlflow.log_artifact(\"models/preprocessor.b\", artifact_path=\"preprocessor\")\n\n        mlflow.xgboost.log_model(booster, artifact_path=\"models_mlflow\")\n    return None\n\n@flow(name=\"Main Flow\")\ndef main_flow():\n    \"\"\"Main flow of the program\"\"\"\n    \n    # MLflow settings\n    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n    mlflow.set_experiment(\"nyc-taxi-experiment\")\n\n    # Load\n    s3_bucket_block = S3Bucket.load(\"s3-bucket-example\")\n    s3_bucket_block.download_folder_to_path(from_folder=\"data\", to_folder=\"data\")\n\n    df_train = read_data(\"./data/green_tripdata_2021-01.parquet\")\n    df_val = read_data(\"./data/green_tripdata_2021-02.parquet\")\n\n    # Transform\n    X_train, X_val, y_train, y_val, dv = add_features(df_train, df_val)\n\n    # Train\n    train_best_model(X_train, X_val, y_train, y_val, dv)\n\nif __name__ == \"__main__\":\n    main_flow()\n```\n\nTo keep the workflow simple, we have removed the `fetch` and `download_and_read` (Subflow) functions. We have also made some changes to how the main_flow is called. The main change is that we are using the `S3Bucket` block to download the data from the S3 bucket.\n\nTo better understand the above script you can read my previous post [here](https://sagarthacker.com/posts/mlops/intro_workflow_orchestration.html#prefect-flow).\n\n```python\n# Loads the configuration of the S3 Bucket from the S3Bucket Block\ns3_bucket_block = S3Bucket.load(\"s3-bucket-block\")\n\n# Downloads the data from the S3 bucket folder name \"data\" to the local folder \"data\"\ns3_bucket_block.download_folder_to_path(from_folder=\"data\", to_folder=\"data\")\n```\n\nRun the above script using the command `python orchestration_s3.py`\n\n![](./images/prefect_blocks/terminal_output.png)\n\n# Further Reading\n\n- [Prefect Blocks](https://docs.prefect.io/2.10.12/concepts/blocks/)\n- [prefect-aws](https://prefecthq.github.io/prefect-aws/)\n- [prefect-aws.s3](https://prefecthq.github.io/prefect-aws/s3/)\n- [prefect-aws.credentials](https://prefecthq.github.io/prefect-aws/credentials/)\n\n## Conclusion\n\nPrefect Blocks are a great way to reuse code and configurations, and to share them with your team. They also help secure your credentials and other sensitive information. In this tutorial, we have seen how to create a Prefect Block for the S3 Bucket. We have also seen how to use the S3 Bucket Block in a Prefect Flow.\n\nThank you for reading and I hope you found this notebook helpful. Upvote if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace!\n\n",
    "supporting": [
      "prefect-blocks_files"
    ],
    "filters": [],
    "includes": {}
  }
}