{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Introduction to MLflow\"\n",
    "description: \"Learn how MLflow simplifies experiment tracking, model versioning, and deployment for efficient machine learning development.\"\n",
    "author: \"Sagar Thacker\"\n",
    "date: \"2023-05-28\"\n",
    "categories: [MLOps, MLflow]\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to our latest blog post, where we delve into the fascinating world of experiment tracking and its powerful implementation using MLflow. ðŸ“š We'll explore the concept of experiment tracking and learn how MLflow can revolutionize the way we track and manage our machine learning experiments.\n",
    "\n",
    "In this comprehensive guide, we will walk you through the process of leveraging MLflow to effectively track your experiments, enabling you to gain valuable insights into model performance, parameter tuning, and results. Discover how MLflow's model registry feature empowers you to version your models, ensuring reproducibility and seamless collaboration."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Experiment Tracking?\n",
    "\n",
    "In a nutshell, it is the process of building an ML model. More formally, Experiment tracking is the process of keeping track of all the relevant information from an ML experiment, which includes:\n",
    "\n",
    "- Source code\n",
    "- Environment\n",
    "- Data\n",
    "- Model\n",
    "- Hyperparameters\n",
    "- Metrics\n",
    "\n",
    "Terminologies associated with ML Experiments:\n",
    "\n",
    "1. **Experiment Run**: Each trail while building an ML model is called an experiment run. Whether it's training a vanilla Linear Regression model or conducting hyperparameter tuning with various combinations, each distinct attempt is considered an experiment run. It allows for systematic organization and analysis of different approaches and variations.\n",
    "2. **Run Artifact**: Any file that is associated with an ML run is an Artifact. Example: Model weights, Model predictions, Model metrics, etc.\n",
    "3. **Experiment Metadata**: Additional information such as Model Signature - description of a modelâ€™s inputs and outputs, Model Input Example - example of a valid model input.\n",
    "\n",
    "## Why is Experiment Tracking Important?\n",
    "\n",
    "- **Reproducibility**: It is important to be able to reproduce the results of an experiment. This is especially true in the case of ML experiments where the results are not deterministic. Experiment tracking helps us to reproduce the results of an experiment by keeping track of all the relevant information.\n",
    "- **Organization**: Any file associated with an ML run is considered an artifact. These artifacts can include crucial components such as model weights, model predictions, model metrics, or any other files generated during the experiment. They serve as valuable resources for evaluating and understanding the outcomes of each run.\n",
    "- **Optimization**: In addition to the core elements of an ML experiment, there is additional metadata that provides valuable context and insights. This metadata includes the model signature, which describes the inputs and outputs of the model, and the model input example, which provides an example of a valid input for the model. These details contribute to a comprehensive understanding of the experiment and aid in reproducibility.\n",
    "\n",
    "In the past, before the advent of dedicated experiment tracking tools, researchers and data scientists relied on spreadsheets to keep track of their experiments. However, this manual approach presented several challenges and limitations. Let's explore the major points of concern:\n",
    "\n",
    "- **Error Prone**: The process of manually copying and pasting the results of an experiment from a Jupyter notebook or other sources into a spreadsheet was tedious and error-prone. It often led to mistakes or inaccuracies in recording the data, undermining the reliability and integrity of the experiment records.\n",
    "- **No standard format**: Without a standardized format for organizing and documenting experiment results, researchers faced difficulties in comparing and analyzing the outcomes of different experiments. The lack of consistency hindered the ability to draw meaningful insights and make informed decisions based on the collected data.\n",
    "- **Visibility & Collaboration**: Sharing experiment results with other team members was a cumbersome task. Spreadsheet-based tracking offered limited visibility and collaboration capabilities, impeding effective teamwork and knowledge sharing. It was challenging to provide access, gather feedback, or collaborate on an experiment in a seamless manner."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with MLflow\n",
    "\n",
    "MLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It provides a comprehensive set of tools for tracking experiments, packaging ML code, and deploying models. MLflow is designed to work with any ML library and language, and it is built on an open API, enabling seamless integration with other platforms and tools.\n",
    "\n",
    "To see MLflow in action, let's walk through a simple example of training a Linear Regression model on NY Green Taxi Trips dataset. We will use MLflow to track the experiment and record the results.\n",
    "\n",
    "### Install MLflow and Setup Environment\n",
    "\n",
    "Create a new virtual environment and install MLflow and other libraries using the following command:\n",
    "\n",
    "```{.bash filename=\"requirements.txt\"}\n",
    "mlflow\n",
    "jupyter\n",
    "scikit-learn\n",
    "pandas\n",
    "xgboost\n",
    "fastparquet\n",
    "hyperopt\n",
    "optuna\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Create conda environment\n",
    "conda create -p venv python=3.9 -y\n",
    "\n",
    "# Activate conda environment\n",
    "conda activate venv/\n",
    "\n",
    "# Install required libraries\n",
    "pip install -r requirements.txt --no-cache-dir\n",
    "```\n",
    "\n",
    "Download the dataset using the following command:\n",
    "\n",
    "```bash\n",
    "# Create data directory\n",
    "mkdir data\n",
    "\n",
    "# Move to data directory\n",
    "cd data\n",
    "\n",
    "# Download dataset\n",
    "wget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-01.parquet # January 2022\n",
    "wget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-02.parquet # February 2022\n",
    "wget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-03.parquet # March 2022\n",
    "```\n",
    "\n",
    "We'll also create a jupyter notebook named `mlflow.ipynb` to run our experiment. After following the above steps, you should have the following directory structure:\n",
    "\n",
    "```bash\n",
    ".\n",
    "â”œâ”€â”€ data\n",
    "â”‚   â”œâ”€â”€ green_tripdata_2022-01.parquet\n",
    "â”‚   â”œâ”€â”€ green_tripdata_2022-02.parquet\n",
    "â”‚   â””â”€â”€ green_tripdata_2022-03.parquet\n",
    "â”œâ”€â”€ mlflow.ipynb\n",
    "â”œâ”€â”€ requirements.txt\n",
    "â””â”€â”€ venv\n",
    "```\n",
    "\n",
    "To see MLflow UI in action, we'll run the following command while you're in the root directory in the terminal:\n",
    "\n",
    "```bash\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "This will start the MLflow server on port 5000. You can access the MLflow UI at http://127.0.0.1:5000. Shown below:\n",
    "\n",
    "![](../images/introduction-to-mlflow/mlflow_ui.png)\n",
    "\n",
    "::: {.callout-note}\n",
    "## Note\n",
    "If you face an **Access to 127.0.0.1 was denied. You don't have authorization to view this page. HTTP ERROR 403** error while accessing the MLflow UI, you can resolve it by clearing the browser cache and cookies.\n",
    ":::\n",
    "\n",
    "One interesting thing to note here is that MLflow will create a new directory named `mlruns` in the current working directory. This directory will contain all the experiment runs and artifacts. This configuration of backend and artifact storage is called `MLflow on localhost`. There are other configurations available as well, which we will explore in another post.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://mlflow.org/docs/latest/_images/scenario_1.png\" alt=\"mlflow localhost tracking\" width=400 height=450/>\n",
    "<p>Source: <a href=\"https://mlflow.org/docs/latest/tracking.html#scenario-1-mlflow-on-localhost\">MLflow Documentation</a></p>\n",
    "</p>\n",
    "\n",
    "### Load Dataset\n",
    "Let's start by importing the required libraries and loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: true\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import mlflow\n",
    "import optuna\n",
    "\n",
    "from mlflow.entities import ViewType\n",
    "from mlflow.tracking import MlflowClient\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a Parquet file into a pandas DataFrame, performs data transformations, and returns the resulting DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): The path to the Parquet file to be read.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The processed DataFrame containing the data from the Parquet file.\n",
    "\n",
    "    Raises:\n",
    "        [Any exceptions raised by pandas.read_parquet()]\n",
    "\n",
    "    Notes:\n",
    "        - The function performs the following transformations on the DataFrame:\n",
    "            - Converts 'lpep_dropoff_datetime' and 'lpep_pickup_datetime' columns to pandas datetime objects.\n",
    "            - Computes the 'duration' column by subtracting 'lpep_pickup_datetime' from 'lpep_dropoff_datetime'\n",
    "              and converting the result to minutes.\n",
    "            - Filters the DataFrame to include rows where the 'duration' is between 1 and 60 minutes (inclusive).\n",
    "            - Converts 'PULocationID' and 'DOLocationID' columns to string type.\n",
    "\n",
    "    Example:\n",
    "        filename = 'data.parquet'\n",
    "        df = read_dataframe(filename)\n",
    "    \"\"\"\n",
    "    # Read the Parquet file into a DataFrame\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    # Convert 'lpep_dropoff_datetime' and 'lpep_pickup_datetime' columns to pandas datetime objects\n",
    "    df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n",
    "    df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "\n",
    "    # Compute the 'duration' column by subtracting 'lpep_pickup_datetime' from 'lpep_dropoff_datetime' and converting to minutes\n",
    "    df['duration'] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n",
    "    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "    # Filter the DataFrame to include rows where the 'duration' is between 1 and 60 minutes (inclusive)\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    # Convert 'PULocationID' and 'DOLocationID' columns to string type\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "    \n",
    "    # Return the processed DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Parquet file for training data into a DataFrame\n",
    "df_train = read_dataframe('./data/green_tripdata_2022-01.parquet')\n",
    "\n",
    "# Read the Parquet file for validation data into a DataFrame\n",
    "df_val = read_dataframe('./data/green_tripdata_2022-02.parquet')\n",
    "\n",
    "# Read the Parquet file for testing data into a DataFrame\n",
    "df_test = read_dataframe('./data/green_tripdata_2022-03.parquet')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and Transform Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame, dv: DictVectorizer, fit_dv: bool = False):\n",
    "    \"\"\"\n",
    "    Preprocesses a pandas DataFrame by creating new features, transforming categorical features into a numerical format,\n",
    "    and returning the transformed data along with the DictVectorizer.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): The input DataFrame to be preprocessed.\n",
    "        dv (sklearn.feature_extraction.DictVectorizer): The DictVectorizer instance to be used for transforming categorical features.\n",
    "        fit_dv (bool, optional): Indicates whether to fit the DictVectorizer on the data. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the transformed feature matrix and the DictVectorizer instance.\n",
    "\n",
    "    Notes:\n",
    "        - The function assumes that the DataFrame contains the columns 'PULocationID' and 'DOLocationID'.\n",
    "        - The function creates a new feature 'PU_DO' by concatenating 'PULocationID' and 'DOLocationID'.\n",
    "        - The categorical feature 'PU_DO' and numerical feature 'trip_distance' are selected for transformation.\n",
    "        - The function transforms the selected features into a dictionary representation and applies the DictVectorizer.\n",
    "        - If fit_dv is True, the DictVectorizer is fitted on the data. Otherwise, the existing fitted DictVectorizer is used.\n",
    "\n",
    "    Example:\n",
    "        df = read_dataframe('data.parquet')\n",
    "        dv = DictVectorizer()\n",
    "        X, dv = preprocess(df, dv, fit_dv=True)\n",
    "    \"\"\"\n",
    "    # Create a new feature 'PU_DO' by concatenating 'PULocationID' and 'DOLocationID'\n",
    "    df['PU_DO'] = df['PULocationID'] + '_' + df['DOLocationID']\n",
    "\n",
    "    # Select categorical and numerical features for transformation\n",
    "    categorical = ['PU_DO']\n",
    "    numerical = ['trip_distance']\n",
    "\n",
    "    # Convert the selected features into a dictionary representation\n",
    "    dicts = df[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "    # Apply DictVectorizer for transforming categorical features\n",
    "    if fit_dv:\n",
    "        # Fit the DictVectorizer on the data\n",
    "        X = dv.fit_transform(dicts)\n",
    "    else:\n",
    "        # Transform using the existing fitted DictVectorizer\n",
    "        X = dv.transform(dicts)\n",
    "\n",
    "    # Return the transformed feature matrix and DictVectorizer\n",
    "    return X, dv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the target variable\n",
    "target = 'duration'\n",
    "\n",
    "# Extract the target variable from the training, validation and testing datasets\n",
    "y_train = df_train[target].values\n",
    "y_val = df_val[target].values\n",
    "y_test = df_test[target].values\n",
    "\n",
    "# Initialize a DictVectorizer for preprocessing\n",
    "dv = DictVectorizer()\n",
    "\n",
    "# Preprocess the training data\n",
    "X_train, dv = preprocess(df_train, dv, fit_dv=True)\n",
    "\n",
    "# Preprocess the validation data using the fitted DictVectorizer from the training data\n",
    "X_val, _ = preprocess(df_val, dv, fit_dv=False)\n",
    "\n",
    "# Preprocess the testing data using the fitted DictVectorizer from the training data\n",
    "X_test, _ = preprocess(df_test, dv, fit_dv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_pickle(obj, filename: str):\n",
    "    \"\"\"\n",
    "    Pickles (serializes) an object and saves it to a file.\n",
    "\n",
    "    Parameters:\n",
    "        obj (Any): The object to be pickled.\n",
    "        filename (str): The path and filename to save the pickled object.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Notes:\n",
    "        - The function uses the 'pickle' module to serialize the object and save it to a file.\n",
    "        - The file is opened in binary mode for writing using the \"wb\" mode.\n",
    "    \"\"\"\n",
    "    with open(filename, \"wb\") as f_out:\n",
    "        return pickle.dump(obj, f_out)\n",
    "\n",
    "\n",
    "# Specify the destination path for saving files\n",
    "dest_path = './outputs'\n",
    "\n",
    "# Create dest_path folder unless it already exists\n",
    "os.makedirs(dest_path, exist_ok=True)\n",
    "\n",
    "# Save DictVectorizer object as a pickle file\n",
    "dump_pickle(dv, os.path.join(dest_path, \"dv.pkl\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Linear Regression Model with MLflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mlflow you create one experiment and all the runs are stored under that experiment. You can create a new experiment using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new experiment\n",
    "mlflow.set_experiment(\"nyc-taxi-experiment\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you check your MLflow UI, you'll notice a new experiment named `ny-taxi-experiment` has been created under Experiments section. All the runs and artifacts will be stored under this experiment.\n",
    "\n",
    "![](../images/introduction-to-mlflow/mlflow_new_exp.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the destination path for saving model files\n",
    "model_path = \"./outputs/models\"\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Set a tag for the developer\n",
    "    mlflow.set_tag(\"developer\", \"Sagar\")\n",
    "\n",
    "    # Initialize and train a LinearRegression model\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the validation data\n",
    "    yhat = lr.predict(X_val)\n",
    "\n",
    "    # Calculate the root mean squared error (RMSE)\n",
    "    rmse = mean_squared_error(y_val, yhat, squared=False)\n",
    "\n",
    "    # Log the RMSE metric to MLflow\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    # Create dest_path folder unless it already exists\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "    # Save the trained model as a pickle file\n",
    "    dump_pickle(lr, os.path.join(model_path, \"lin_reg.pkl\"))\n",
    "\n",
    "    # Log the trained model as an artifact to MLflow\n",
    "    mlflow.log_artifact(local_path=f\"{model_path}/lin_reg.pkl\", artifact_path=\"models_pickle\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running an experiment run, go to your MLflow UI and there you can see an entry of the run. If you click on the run you have observer other details associated with the run such as `Metrics`, `Tags`, `Artifacts`. In the `Artifacts` section you can see the `model_pickle` folder which contains the model file. You can download the model file from there and use it for prediction. \n",
    "\n",
    "::: {.callout-tip}\n",
    "If you don't see your run on the UI, you can refresh using `Refresh` button on the right side of the page above the table.\n",
    ":::\n",
    "\n",
    "![](../images/introduction-to-mlflow/linear_reg.png)\n",
    "\n",
    "![](../images/introduction-to-mlflow/lr_exp.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila ðŸŽ‰! You have successfully trained a Linear Regression model and tracked the experiment using MLflow. You can find the complete code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traing a Lasso Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the destination path for saving model files\n",
    "model_path = \"./outputs/models\"\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Set a tag for the developer\n",
    "    mlflow.set_tag(\"developer\", \"Sagar\")\n",
    "\n",
    "    # Set the value for the regularization parameter (alpha)\n",
    "    alpha = 0.1\n",
    "\n",
    "    # Log the regularization parameter (alpha) as a parameter in MLflow\n",
    "    mlflow.log_param(\"alpha\", alpha)\n",
    "\n",
    "    # Initialize and train a Lasso Regression model\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the validation data\n",
    "    yhat = lasso.predict(X_val)\n",
    "\n",
    "    # Calculate the root mean squared error (RMSE)\n",
    "    rmse = mean_squared_error(y_val, yhat, squared=False)\n",
    "\n",
    "    # Log the RMSE metric to MLflow\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    # Create the destination folder for saving model files if it doesn't exist\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "    # Save the trained Lasso Regression model as a pickle file\n",
    "    dump_pickle(lr, os.path.join(model_path, \"lasso_reg.pkl\"))\n",
    "\n",
    "    # Log the trained model as an artifact to MLflow\n",
    "    mlflow.log_artifact(local_path=f\"{model_path}/lasso_reg.pkl\", artifact_path=\"models_pickle\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Linear Regression, we can train a Lasso Regression model and track the experiment using MLflow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Tools such as MLflow shine when it comes to hyperparameter tuning and model selection. Let's see how we can leverage MLflow to tune the hyperparameters of our model.\n",
    "\n",
    "We'll setup a new experiment for hyperparameter tuning and use MLflow to track the results. We'll use `Hyperopt` and `Optuna` libraries for hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new experiment\n",
    "mlflow.set_experiment(\"random-forest-hyperopt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-26 12:23:03,233]\u001b[0m A new study created in memory with name: no-name-19f4f970-eafc-402c-89b3-2aa461ab17fc\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 12:23:04,744]\u001b[0m Trial 0 finished with value: 6.012747224033297 and parameters: {'n_estimators': 25, 'max_depth': 20, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 0 with value: 6.012747224033297.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 12:23:04,872]\u001b[0m Trial 1 finished with value: 6.249433998787504 and parameters: {'n_estimators': 16, 'max_depth': 4, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 0 with value: 6.012747224033297.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 12:23:06,223]\u001b[0m Trial 2 finished with value: 6.039045655830305 and parameters: {'n_estimators': 34, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 0 with value: 6.012747224033297.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 12:23:06,636]\u001b[0m Trial 3 finished with value: 6.179387143797025 and parameters: {'n_estimators': 44, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 0 with value: 6.012747224033297.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 12:23:07,140]\u001b[0m Trial 4 finished with value: 6.075505898039151 and parameters: {'n_estimators': 22, 'max_depth': 11, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 0 with value: 6.012747224033297.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 12:23:07,316]\u001b[0m Trial 5 finished with value: 6.441117537172997 and parameters: {'n_estimators': 35, 'max_depth': 3, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 0 with value: 6.012747224033297.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 12:23:08,432]\u001b[0m Trial 6 finished with value: 6.0285791267371165 and parameters: {'n_estimators': 28, 'max_depth': 16, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 0 with value: 6.012747224033297.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 12:23:08,532]\u001b[0m Trial 7 finished with value: 7.881244954282265 and parameters: {'n_estimators': 34, 'max_depth': 1, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 0 with value: 6.012747224033297.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 12:23:09,146]\u001b[0m Trial 8 finished with value: 6.025608014492215 and parameters: {'n_estimators': 12, 'max_depth': 19, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 0 with value: 6.012747224033297.\u001b[0m\n",
      "\u001b[32m[I 2023-05-26 12:23:09,239]\u001b[0m Trial 9 finished with value: 7.071070856187059 and parameters: {'n_estimators': 22, 'max_depth': 2, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 0 with value: 6.012747224033297.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def run_optimization(num_trials: int = 10):\n",
    "    \"\"\"\n",
    "    Runs the optimization process using Optuna library to find the optimal hyperparameters for RandomForestRegressor.\n",
    "\n",
    "    Parameters:\n",
    "        num_trials (int): The number of optimization trials to perform. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Notes:\n",
    "        - The function defines an objective function for Optuna to minimize the root mean squared error (RMSE).\n",
    "        - The objective function samples hyperparameters, trains a RandomForestRegressor model with those hyperparameters,\n",
    "          evaluates the model on the validation data, and logs the RMSE metric to MLflow.\n",
    "        - Optuna performs the optimization process by searching for the set of hyperparameters that minimizes the RMSE.\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(trial):\n",
    "        \"\"\"\n",
    "        Objective function for Optuna optimization.\n",
    "\n",
    "        Parameters:\n",
    "            trial (optuna.Trial): A trial object representing a single optimization trial.\n",
    "\n",
    "        Returns:\n",
    "            float: The value of the objective function (RMSE).\n",
    "\n",
    "        Notes:\n",
    "            - The objective function samples hyperparameters from the defined search space.\n",
    "            - It initializes and trains a RandomForestRegressor model with the sampled hyperparameters.\n",
    "            - The model is evaluated on the validation data, and the RMSE is calculated.\n",
    "            - The RMSE and the sampled hyperparameters are logged to MLflow.\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 10, 50, 1),\n",
    "            'max_depth': trial.suggest_int('max_depth', 1, 20, 1),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 10, 1),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 4, 1),\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        # Start a new MLflow run for each trial\n",
    "        with mlflow.start_run():\n",
    "            # Set a tag for the model type\n",
    "            mlflow.set_tag(\"model\", \"RandomForestRegressor\")\n",
    "            \n",
    "            # Log the sampled hyperparameters to MLflow\n",
    "            mlflow.log_params(params)\n",
    "\n",
    "            # Initialize a RandomForestRegressor model with the sampled hyperparameters\n",
    "            rf = RandomForestRegressor(**params)\n",
    "\n",
    "            # Train the model on the training data\n",
    "            rf.fit(X_train, y_train)\n",
    "\n",
    "            # Make predictions on the validation data\n",
    "            y_pred = rf.predict(X_val)\n",
    "\n",
    "            # Calculate the root mean squared error (RMSE)\n",
    "            rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "\n",
    "            # Log the RMSE metric to MLflow\n",
    "            mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "        return rmse\n",
    "\n",
    "    # Use the Tree-structured Parzen Estimator (TPE) sampler for efficient hyperparameter search\n",
    "    sampler = TPESampler(seed=42)\n",
    "\n",
    "    # Create an Optuna study with the defined objective function and search direction\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "\n",
    "    # Run the optimization process with the specified number of trials\n",
    "    study.optimize(objective, n_trials=num_trials)\n",
    "\n",
    "run_optimization()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the experiment, you can see the results in the MLflow UI. You can also select multiple runs and compare them using the `Compare` button on the top left corner of the table.\n",
    "\n",
    "![](../images/introduction-to-mlflow/hyperparameter_rf.png)\n",
    "\n",
    "![](../images/introduction-to-mlflow/compare_results.png)\n",
    "\n",
    "- One of the most interesting plot in the MLflow UI is the `Parallel Coordinates Plot`. It allows you to visualize the relationship between the hyperparameters and the metrics.\n",
    "- `Scatter plot` allows you to visualize the relationship between a single hyperparameter and the metric.\n",
    "- `Box Plot` are similar to scatter plot in terms of visualization.\n",
    "- `Contour Plot` allows you to visualize the relationship between two hyperparameters and the metric.\n",
    "\n",
    "Apart from the visualizations, they also provide more information of the `Run Duration`, `Parameters`, `Metrics` and `Tags` associated with the run in a tabular format to compare the runs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Registry\n",
    "\n",
    "MLflow Model Registry is a centralized model store, set of APIs, and UI, to collaboratively manage the full lifecycle of an MLflow Model. It provides model lineage (which MLflow Experiment and Run produced the model), model versioning, stage transitions (for example from staging to production), annotations, and other functionality to track the model lifecycle.\n",
    "\n",
    "From our example above for hyperparameter tuning, we can register the best model which performs well on the test dataset. \n",
    "\n",
    "To interact with an MLflow Tracking Server and an MLflow Registry Server, we utilize the Client class. It allows us to create and manage experiments and runs in the Tracking Server, as well as create and manage registered models and model versions in the Registry Server. The mlflow.client module offers a Python interface for performing CRUD operations on MLflow Experiments, Runs, Model Versions, and Registered Models. It serves as a lower-level API that directly corresponds to MLflow's REST API calls.\n",
    "\n",
    "More information about the MLflow Client can be found [here](https://mlflow.org/docs/latest/python_api/mlflow.client.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization experiment name\n",
    "HPO_EXPERIMENT_NAME = \"random-forest-hyperopt\"\n",
    "\n",
    "# Best model experiment name\n",
    "EXPERIMENT_NAME = \"random-forest-best-models\"\n",
    "\n",
    "# Create a new experiment for the best models\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically log parameters and metrics\n",
    "mlflow.sklearn.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest Parameters\n",
    "RF_PARAMS = ['max_depth', 'n_estimators', 'min_samples_split',\n",
    "             'min_samples_leaf', 'random_state', 'n_jobs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_log_model(params):\n",
    "    \"\"\"\n",
    "    Trains a RandomForestRegressor model with the given hyperparameters and logs evaluation metrics to MLflow.\n",
    "\n",
    "    Parameters:\n",
    "        params (dict): Dictionary of hyperparameters for RandomForestRegressor.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Notes:\n",
    "        - The function starts an MLflow run to track the model training and evaluation process.\n",
    "        - It converts certain hyperparameters to integers.\n",
    "        - A RandomForestRegressor model is initialized with the provided hyperparameters.\n",
    "        - The model is trained on the training data.\n",
    "        - The trained model is evaluated on the validation and test sets, and the root mean squared error (RMSE) is calculated and logged to MLflow as evaluation metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        # Convert specific hyperparameters to integers\n",
    "        for param in RF_PARAMS:\n",
    "            params[param] = int(params[param])\n",
    "\n",
    "        # Initialize a RandomForestRegressor model with the given hyperparameters\n",
    "        rf = RandomForestRegressor(**params)\n",
    "\n",
    "        # Train the model on the training data\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the trained model on the validation set\n",
    "        val_rmse = mean_squared_error(y_val, rf.predict(X_val), squared=False)\n",
    "\n",
    "        # Log the validation RMSE metric to MLflow\n",
    "        mlflow.log_metric(\"val_rmse\", val_rmse)\n",
    "\n",
    "        # Evaluate the trained model on the test set\n",
    "        test_rmse = mean_squared_error(y_test, rf.predict(X_test), squared=False)\n",
    "\n",
    "        # Log the test RMSE metric to MLflow\n",
    "        mlflow.log_metric(\"test_rmse\", test_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_register_model(top_n: int):\n",
    "    \"\"\"\n",
    "    Runs the process to register the best model based on the top_n model runs with the lowest test RMSE.\n",
    "\n",
    "    Parameters:\n",
    "        top_n (int): The number of top model runs to consider.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Notes:\n",
    "        - The function interacts with the MLflow tracking server to retrieve and register models.\n",
    "        - It retrieves the top_n model runs based on the lowest validation RMSE.\n",
    "        - For each run, it trains a model using the hyperparameters from the run and logs evaluation metrics to MLflow.\n",
    "        - After evaluating the models, it selects the one with the lowest test RMSE.\n",
    "        - The selected model is registered with a specified name in MLflow.\n",
    "    \"\"\"\n",
    "\n",
    "    # Connect to the MLflow tracking server\n",
    "    client = MlflowClient()\n",
    "\n",
    "    # Retrieve the top_n model runs and log the models\n",
    "    experiment = client.get_experiment_by_name(HPO_EXPERIMENT_NAME)\n",
    "\n",
    "    # Retrieve the top_n model runs based on the lowest validation RMSE\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=experiment.experiment_id,\n",
    "        run_view_type=ViewType.ACTIVE_ONLY,\n",
    "        max_results=top_n,\n",
    "        order_by=[\"metrics.rmse ASC\"]\n",
    "    )\n",
    "\n",
    "    # Train and log the model for each run\n",
    "    for run in runs:\n",
    "        # Train and log the model based on the hyperparameters from the run\n",
    "        train_and_log_model(params=run.data.params)\n",
    "\n",
    "    # Select the model with the lowest test RMSE\n",
    "    experiment = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\n",
    "    # Retrieve model runs based on the lowest test RMSE, and select the first run (with the lowest test RMSE)\n",
    "    best_run = client.search_runs(\n",
    "        experiment_ids=experiment.experiment_id,\n",
    "        run_view_type=ViewType.ACTIVE_ONLY,\n",
    "        order_by=[\"metrics.test_rmse ASC\"]\n",
    "    )[0]\n",
    "\n",
    "    # Register the best model\n",
    "    model_uri = f\"runs:/{best_run.info.run_id}/model\"\n",
    "\n",
    "    # Register the best model with a specified name\n",
    "    mlflow.register_model(\n",
    "        model_uri=model_uri,\n",
    "        name=\"random-forest-best-model\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 13:27:30 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/wizard/Astronaut/Dev/MLOps/week2/venv/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n",
      "Successfully registered model 'random-forest-best-model'.\n",
      "2023/05/26 13:27:40 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: random-forest-best-model, version 1\n",
      "Created version '1' of model 'random-forest-best-model'.\n"
     ]
    }
   ],
   "source": [
    "# The number of top model runs to consider\n",
    "top_n = 5\n",
    "\n",
    "run_register_model(top_n=top_n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By following the mentioned steps, a new experiment called random-forest-best-models will be created, displaying five runs. You may observe that each run provides additional information. This is due to the utilization of MLflow's **autologging** feature.\n",
    "\n",
    "Read more about autologging [here](https://mlflow.org/docs/latest/tracking.html#automatic-logging)\n",
    "\n",
    "Autologging captures more parameters, metrics, and tags, and also saves the model as an artifact along with environment details. This feature proves particularly beneficial when you aim to reproduce the results. You have the option to download the model and environment details, enabling you to replicate the outcomes accurately.\n",
    "\n",
    "![](../images/introduction-to-mlflow/rf_test_runs.png)\n",
    "\n",
    "Click on the run with the best model stored and access additional details about the run.\n",
    "\n",
    "![](../images/introduction-to-mlflow/best_test_run.png)\n",
    "\n",
    "By clicking on the link to model registry in the above image, you will be redirected to the model registry page. \n",
    "\n",
    "![](../images/introduction-to-mlflow/best_model.png)\n",
    "\n",
    "Another way to access the model registry is by clicking on the model registry tab on the left side of the MLflow UI. You will be able to see the model registry page with the best model registered.\n",
    "\n",
    "![](../images/introduction-to-mlflow/mlflow_model_section.png)\n",
    "\n",
    "You also have an option to change the stage of the model to `Staging`, `Production`, or `Archived`. You can do this by clicking on the stage transition button on the right side of the model name.\n",
    "\n",
    "In the MLflow ecosystem, the responsibility of determining which models are ready for production lies with the Data Scientist. Once a model is registered in the model registry, the Deployment Engineer can review important details such as the model's parameters, size, and performance. Based on this information, they can make informed decisions on moving the model across different stages.\n",
    "\n",
    "It's important to note that the model registry itself does not handle the deployment of models. It serves as a centralized repository to list the models that are considered production-ready, with stages acting as labels. To complete the deployment process, it is recommended to complement the model registry with a CI/CD pipeline specifically designed for deploying models. This pipeline would handle the actual deployment process, incorporating the production-ready models identified in the model registry.\n",
    "\n",
    "![](../images/introduction-to-mlflow/change_stage.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow Limitations\n",
    "\n",
    "MLflow, while a powerful tool for experiment tracking and model management, has certain limitations to consider.\n",
    "\n",
    "- **Authentication & Users**:  It lacks built-in authentication and user management capabilities, making it less suitable for environments requiring strict access control.\n",
    "- **Data versioning**: MLflow does not provide a native solution for data versioning, requiring alternative approaches.\n",
    "- **Model/Data Monitoring & Alerting**: Moreover, for model and data monitoring, as well as alerting, other specialized tools may be more appropriate.\n",
    "\n",
    "## Alternatives to MLflow\n",
    "\n",
    "- [Neptune.ai](https://neptune.ai/)\n",
    "- [Comet](https://www.comet.com/site/)\n",
    "- [Weights & Biases](https://wandb.ai/site)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulation ðŸŽ‰! You have successfully trained ML model and tracked the experiment using MLflow.\n",
    "\n",
    "Thank you for reading and I hope you found this notebook helpful. Upvote if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
