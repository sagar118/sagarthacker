[
  {
    "objectID": "pages/mlops.html",
    "href": "pages/mlops.html",
    "title": "MLOps",
    "section": "",
    "text": "This page contains the list of posts related to MLOps. It makes it easier to follow and read topics in a structured format:"
  },
  {
    "objectID": "pages/mlops.html#introduction",
    "href": "pages/mlops.html#introduction",
    "title": "MLOps",
    "section": "Introduction",
    "text": "Introduction\n\nIntroduction to MLOps"
  },
  {
    "objectID": "pages/mlops.html#workflow-orchestration",
    "href": "pages/mlops.html#workflow-orchestration",
    "title": "MLOps",
    "section": "Workflow Orchestration",
    "text": "Workflow Orchestration\n\nIntroduction to Workflow Orchestration\nPrefect Blocks\nPrefect Deployments\nPrefect Cloud"
  },
  {
    "objectID": "pages/mlops.html#experiment-tracking",
    "href": "pages/mlops.html#experiment-tracking",
    "title": "MLOps",
    "section": "Experiment Tracking",
    "text": "Experiment Tracking\n\nIntroduction to MLflow\nMLflow on AWS"
  },
  {
    "objectID": "pages/mlops.html#deployments",
    "href": "pages/mlops.html#deployments",
    "title": "MLOps",
    "section": "Deployments",
    "text": "Deployments\n\nDeployment of Dockerized ML Models with AWS Kinesis and Lambda\nDeployment of ML models on Hugging Face Spaces"
  },
  {
    "objectID": "pages/kaggle.html",
    "href": "pages/kaggle.html",
    "title": "Kaggle Competitions",
    "section": "",
    "text": "This page contains the list of posts related to kaggle competition:\n\nPlayground Series Season 3, Episode 4\n\nExploratory Data Analysis\nEnsemble Model"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Beyond the Dataset",
    "section": "",
    "text": "Hello and welcome to my blog. You‚Äôll find all my posts and links to projects below. Happy reading!\n\nüìÆ Blog Series\n\nNatural Language Processing\nKaggle Competitions\nMLOps\n\n\n\nüì¨ Subscribe\n\n\n‚ÄúDon‚Äôt focus on having a great blog. Focus on producing a blog that‚Äôs great for your readers.‚Äù - Brian Clark\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nDeployment of ML models on Hugging Face Spaces\n\n\n\n\n\n\n\nMLOps\n\n\nHugging Face\n\n\nGradio\n\n\nHF Spaces\n\n\nDeployment\n\n\n\n\nBuild a simple web app with Gradio and deploy it on Hugging Face Spaces\n\n\n\n\n\n\nJun 26, 2023\n\n\n21 min\n\n\n\n\n\n\n  \n\n\n\n\nDeployment of Dockerized ML Models with AWS Kinesis and Lambda\n\n\n\n\n\n\n\nMLOps\n\n\nAWS Lambda\n\n\nAWS Kinesis\n\n\nAWS ECR\n\n\nDocker\n\n\nDeployment\n\n\n\n\nA comprehensive guide to deploying ml model with AWS Lambda, Kenisis and ECR\n\n\n\n\n\n\nJun 21, 2023\n\n\n26 min\n\n\n\n\n\n\n  \n\n\n\n\nPrefect Cloud Deployment\n\n\n\n\n\n\n\nMLOps\n\n\nPrefect\n\n\nDeployment\n\n\nPrefect-Cloud\n\n\n\n\nA comprehensive guide to deploying Prefect Flows on Prefect Cloud\n\n\n\n\n\n\nJun 16, 2023\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nPrefect Deployment\n\n\n\n\n\n\n\nMLOps\n\n\nPrefect\n\n\nDeployment\n\n\n\n\nA comprehensive guide to deploying Prefect Flows\n\n\n\n\n\n\nJun 11, 2023\n\n\n19 min\n\n\n\n\n\n\n  \n\n\n\n\nPrefect Blocks\n\n\n\n\n\n\n\nMLOps\n\n\nPrefect\n\n\n\n\nPrefect Blocks are reusable, composable building blocks for orchestrating data workflows.\n\n\n\n\n\n\nJun 7, 2023\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\nPrefect: An Workflow Orchestration Tool\n\n\n\n\n\n\n\nMLOps\n\n\nPrefect\n\n\n\n\nPrefect is an open-source workflow orchestration tool that helps you automate and manage the flow of work across your data stack.\n\n\n\n\n\n\nJun 5, 2023\n\n\n20 min\n\n\n\n\n\n\n  \n\n\n\n\nMLflow on AWS\n\n\n\n\n\n\n\nMLOps\n\n\nMLflow\n\n\nAWS\n\n\n\n\nDiscover the implementation of MLflow on AWS, leveraging EC2 to host MLFlow Server, S3 for artifact storage and RDS-PostgreSQL for backend entity storager.\n\n\n\n\n\n\nMay 30, 2023\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to MLflow\n\n\n\n\n\n\n\nMLOps\n\n\nMLflow\n\n\n\n\nLearn how MLflow simplifies experiment tracking, model versioning, and deployment for efficient machine learning development.\n\n\n\n\n\n\nMay 28, 2023\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nAWS Instance Setup\n\n\n\n\n\n\n\nAWS\n\n\nPython\n\n\nDocker\n\n\n\n\nStep-by-step guide to setup AWS EC2 Instance and setup environment\n\n\n\n\n\n\nMay 19, 2023\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to MLOps\n\n\n\n\n\n\n\nMLOps\n\n\n\n\nLearn how to combine machine learning with software engineering to develop, deploy & maintain production ML applications.\n\n\n\n\n\n\nMay 18, 2023\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nModeling - Playground Series Season 3, Episode 4\n\n\n\n\n\n\n\nKaggle\n\n\n\n\nTabular Classification with a Credit Card Fraud Dataset\n\n\n\n\n\n\nJan 30, 2023\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nEDA - Playground Series Season 3, Episode 4\n\n\n\n\n\n\n\nKaggle\n\n\n\n\nTabular Classification with a Credit Card Fraud Dataset\n\n\n\n\n\n\nJan 30, 2023\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nMovie Lens - Collaborative Filtering based Recommendation System\n\n\n\n\n\n\n\nPySpark\n\n\n\n\nMovie Recommendation Using PySpark\n\n\n\n\n\n\nJan 20, 2022\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nSong Popularity EDA\n\n\n\n\n\n\n\nKaggle\n\n\n\n\nSong Popularity Prediction is a competition on Kaggle. This post is all about performing exploratory data analysis following the coding session by Martin Henze.\n\n\n\n\n\n\nMay 31, 2021\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nText Preprocessing\n\n\n\n\n\n\n\nText Preprocessing\n\n\nNatural Language Processing\n\n\nNLP\n\n\n\n\nVairous preprocessing steps required to clean and prepare your data in NLP\n\n\n\n\n\n\nApr 19, 2021\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nWhat is Natural Language Processing?\n\n\n\n\n\n\n\nNatural Language Processing\n\n\nNLP\n\n\n\n\nA brief summary of what is natural language processing, it‚Äôs challenges and applications.\n\n\n\n\n\n\nApr 12, 2021\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nPath to become a Machine Learning Expert\n\n\n\n\n\n\n\nMachine Learning\n\n\nLearning Path\n\n\n\n\nComprehensive learning path to become an expert in Machine Learning\n\n\n\n\n\n\nApr 5, 2021\n\n\n9 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/spark/movielens-recommendation-system.html",
    "href": "posts/spark/movielens-recommendation-system.html",
    "title": "Movie Lens - Collaborative Filtering based Recommendation System",
    "section": "",
    "text": "Recommendation System are build by analyzing the user and product interation data. It can be used to give item suggestion to a user or predict how a user would rate the an item.\nRecommendation System have mainly have three approaches:\nIn this notebook, Alternating Least Squares (ALS) matrix factorization algorithm with the use of Apache Spark APIs to predict the ratings of movies in the MovieLens Dataset.\nALS attempts to estimate the ratings matrix R as the product of two lower-rank matrices, X and Y, i.e.¬†X * Yt = R. Typically these approximations are called ‚Äòfactor‚Äô matrices. The general approach is iterative. During each iteration, one of the factor matrices is held constant, while the other is solved for using least squares. The newly-solved factor matrix is then held constant while solving for the other factor matrix (source).\nCode\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport warnings\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\n%matplotlib inline\nos.environ[\"PYSPARK_PYTHON\"] = \"python3\"\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "posts/spark/movielens-recommendation-system.html#data-description",
    "href": "posts/spark/movielens-recommendation-system.html#data-description",
    "title": "Movie Lens - Collaborative Filtering based Recommendation System",
    "section": "1. Data Description",
    "text": "1. Data Description\nMovieLens 25M movie ratings. Stable benchmark dataset. 25 million ratings and one million tag applications applied to 62,000 movies by 162,000 users.\n\nmovies.csv: Movie information is contained in the file movies.csv. Each line of this file after the header row represents one movie, and has the following format: movieId,title,genres\n\nGenres are a pipe-separated list, and are selected from the following:\n\nAction\nAdventure\nAnimation\nChildren‚Äôs\nComedy\nCrime\nDocumentary\nDrama\nFantasy\nFilm-Noir\nHorror\nMusical\nMystery\nRomance\nSci-Fi\nThriller\nWar\nWestern\n(no genres listed)\n\n\nrating.csv: All ratings are contained in the file rating.csv. Each line of this file after the header row represents one rating of one movie by one user, and has the following format: userId,movieId,rating,timestamp\n\nThe lines within this file are ordered first by userId, then, within user, by movieId.\nRatings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\nTimestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970."
  },
  {
    "objectID": "posts/spark/movielens-recommendation-system.html#overview-structure-and-data-content",
    "href": "posts/spark/movielens-recommendation-system.html#overview-structure-and-data-content",
    "title": "Movie Lens - Collaborative Filtering based Recommendation System",
    "section": "2. Overview Structure and Data Content",
    "text": "2. Overview Structure and Data Content\n\n# Create a spark session\nspark = SparkSession.builder.appName(\n    \"movie-lens-recommendation\").config(\"spark.driver.memory\", \"16g\").getOrCreate()\nspark.sparkContext.setLogLevel(\"ERROR\")\n\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n\n\n23/01/21 01:45:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n\n\n\n# Read dataset\n__dirname = '../input/movielens-20m-dataset/'\nmovies = spark.read.csv(__dirname + 'movie.csv', header=True, inferSchema=True)\nratings = spark.read.csv(__dirname + 'rating.csv',\n                         header=True, inferSchema=True)\n\n                                                                                \n\n\n\n# Shape of the datasets\nprint(f'Movies has {movies.count()} rows and {len(movies.columns)} columns')\nprint(f'Ratings has {ratings.count()} rows and {len(ratings.columns)} colunms')\n\nMovies has 27278 rows and 3 columns\nRatings has 20000263 rows and 4 colunms\n\n\n[Stage 7:=================================================&gt;         (5 + 1) / 6]                                                                                \n\n\n\n# Display top five rows of each dataframe\nmovies.show(n=5, truncate=False)\nratings.show(n=5, truncate=False)\n\n+-------+----------------------------------+-------------------------------------------+\n|movieId|title                             |genres                                     |\n+-------+----------------------------------+-------------------------------------------+\n|1      |Toy Story (1995)                  |Adventure|Animation|Children|Comedy|Fantasy|\n|2      |Jumanji (1995)                    |Adventure|Children|Fantasy                 |\n|3      |Grumpier Old Men (1995)           |Comedy|Romance                             |\n|4      |Waiting to Exhale (1995)          |Comedy|Drama|Romance                       |\n|5      |Father of the Bride Part II (1995)|Comedy                                     |\n+-------+----------------------------------+-------------------------------------------+\nonly showing top 5 rows\n\n+------+-------+------+-------------------+\n|userId|movieId|rating|timestamp          |\n+------+-------+------+-------------------+\n|1     |2      |3.5   |2005-04-02 23:53:47|\n|1     |29     |3.5   |2005-04-02 23:31:16|\n|1     |32     |3.5   |2005-04-02 23:33:39|\n|1     |47     |3.5   |2005-04-02 23:32:07|\n|1     |50     |3.5   |2005-04-02 23:29:40|\n+------+-------+------+-------------------+\nonly showing top 5 rows\n\n\n\n\n2.1. Data Structure and Statistics\nLet‚Äôs go over each dataframe and check it‚Äôs schema.\nRun the describe() method to see the count, mean, standard deviation, minimum, and maximum values for the data in each column:\n\n# Movies DataFrame\nmovies.printSchema()\nmovies.describe().show()\n\nroot\n |-- movieId: integer (nullable = true)\n |-- title: string (nullable = true)\n |-- genres: string (nullable = true)\n\n+-------+-----------------+--------------------+------------------+\n|summary|          movieId|               title|            genres|\n+-------+-----------------+--------------------+------------------+\n|  count|            27278|               27278|             27278|\n|   mean|59855.48057042305|                null|              null|\n| stddev|44429.31469707313|                null|              null|\n|    min|                1|\"\"Great Performan...|(no genres listed)|\n|    max|           131262|       Ë≤ûÂ≠ê3D (2012)|           Western|\n+-------+-----------------+--------------------+------------------+\n\n\n\n[Stage 14:&gt;                                                         (0 + 1) / 1]                                                                                \n\n\n\n# Ratings DataFrame\nratings.printSchema()\nratings.summary().show()\n\nroot\n |-- userId: integer (nullable = true)\n |-- movieId: integer (nullable = true)\n |-- rating: double (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n\n+-------+-----------------+------------------+------------------+\n|summary|           userId|           movieId|            rating|\n+-------+-----------------+------------------+------------------+\n|  count|         20000263|          20000263|          20000263|\n|   mean|69045.87258292554| 9041.567330339605|3.5255285642993797|\n| stddev|40038.62665316267|19789.477445413264| 1.051988919294229|\n|    min|                1|                 1|               0.5|\n|    25%|            34395|               903|               3.0|\n|    50%|            69133|              2167|               3.5|\n|    75%|           103637|              4771|               4.0|\n|    max|           138493|            131262|               5.0|\n+-------+-----------------+------------------+------------------+\n\n\n\n[Stage 17:================================================&gt;         (5 + 1) / 6]                                                                                \n\n\nNot all of these statistics are actually meaningful! You can use specific methods from the DataFrame API to compute any statistic:\n\nprint(\n    f\"Number of distinct users: {ratings.select('userId').distinct().count()}\")\nprint(\n    f\"Number of distinct movies: {ratings.select('movieId').distinct().count()}\")\n\n                                                                                [Stage 26:================================================&gt;         (5 + 1) / 6]                                                                                \n\n\nNumber of distinct users: 138493\nNumber of distinct movies: 26744\n\n\nYou can also leverage your SQL knowledge to query the data.\nExample, Find the number of movies with ratings higher than 4 with and without SQL:\n\n# Without SQL\nprint(\n    f\"Number of distinct movies with rating greater than 4: {ratings.filter('rating &gt; 4').select('movieId').distinct().count()}\")\n\n# With SQL\nratings.createOrReplaceTempView('ratings')\nspark.sql('SELECT COUNT(DISTINCT(movieId)) AS movie_count FROM ratings WHERE rating &gt; 4').show()\n\n                                                                                [Stage 44:================================================&gt;         (5 + 1) / 6]                                                                                \n\n\nNumber of distinct movies with rating greater than 4: 17218\n+-----------+\n|movie_count|\n+-----------+\n|      17218|\n+-----------+\n\n\n\n\n\n2.2. Missing Values\nCheck if any column contains missing values.\n\n# Check for missing values\n# 1. Movies Dataframe\nmovies.select([F.count(F.when(F.col(c).contains('NULL') |\n                              F.col(c).isNull() |\n                              F.isnan(c), c)).alias(c) for c in movies.columns]).show()\n\n# 2. Ratings Dataframe\nratings.select(*[(\n    F.count(\n        F.when(\n            (F.col(c).contains('NULL') | F.col(c).isNull() | F.isnan(c)), c)) if t not in ('timestamp', 'data')\n    else F.count(F.when(F.col(c).contains('NULL') | F.col(c).isNull(), c))).alias(c)\n    for c, t in ratings.dtypes]).show()\n\n+-------+-----+------+\n|movieId|title|genres|\n+-------+-----+------+\n|      0|    0|     0|\n+-------+-----+------+\n\n+------+-------+------+---------+\n|userId|movieId|rating|timestamp|\n+------+-------+------+---------+\n|     0|      0|     0|        0|\n+------+-------+------+---------+\n\n\n\n[Stage 53:================================================&gt;         (5 + 1) / 6]                                                                                \n\n\n\n\n2.3. Merge DataFrame\nWe‚Äôll merge the movies and ratings dataframe for further analysis and model building process.\n\n# Merge the movies and ratings dataframes\ndf = ratings.join(movies, on=[\"movieId\"], how=\"left\")\n\n\n# Look at df structure\ndf.printSchema()\nprint(f'Merged DataFrame has {df.count()} rows and {len(df.columns)} colunms')\n\nroot\n |-- movieId: integer (nullable = true)\n |-- userId: integer (nullable = true)\n |-- rating: double (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n |-- title: string (nullable = true)\n |-- genres: string (nullable = true)\n\nMerged DataFrame has 20000263 rows and 6 colunms\n\n\n[Stage 57:================================================&gt;         (5 + 1) / 6]                                                                                \n\n\n\n# Display top five rows of merged dataframe\ndf.show(n=5, truncate=False)\n\n+-------+------+------+-------------------+---------------------------------------------------------------+--------------------------------------+\n|movieId|userId|rating|timestamp          |title                                                          |genres                                |\n+-------+------+------+-------------------+---------------------------------------------------------------+--------------------------------------+\n|2      |1     |3.5   |2005-04-02 23:53:47|Jumanji (1995)                                                 |Adventure|Children|Fantasy            |\n|29     |1     |3.5   |2005-04-02 23:31:16|City of Lost Children, The (Cit√© des enfants perdus, La) (1995)|Adventure|Drama|Fantasy|Mystery|Sci-Fi|\n|32     |1     |3.5   |2005-04-02 23:33:39|Twelve Monkeys (a.k.a. 12 Monkeys) (1995)                      |Mystery|Sci-Fi|Thriller               |\n|47     |1     |3.5   |2005-04-02 23:32:07|Seven (a.k.a. Se7en) (1995)                                    |Mystery|Thriller                      |\n|50     |1     |3.5   |2005-04-02 23:29:40|Usual Suspects, The (1995)                                     |Crime|Mystery|Thriller                |\n+-------+------+------+-------------------+---------------------------------------------------------------+--------------------------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "posts/spark/movielens-recommendation-system.html#visualizations",
    "href": "posts/spark/movielens-recommendation-system.html#visualizations",
    "title": "Movie Lens - Collaborative Filtering based Recommendation System",
    "section": "3. Visualizations",
    "text": "3. Visualizations\n\n# Distribution of User Ratings\nrating_count_df = (df.groupBy(['rating']).count()).toPandas()\n\nfig = plt.figure(figsize=(8, 5))\nsns.barplot(x='rating', y='count', data=rating_count_df)\nplt.title(\"Count of each rating\", fontsize=14)\nplt.show()\n\n                                                                                \n\n\n\n\n\n\nrating_values = df.select(['rating']).toPandas()\n\nfig = plt.figure(figsize=(8, 5))\nsns.violinplot(rating_values['rating'])\nplt.title(\"Distribution of rating\", fontsize=14)\nplt.show()\n\n\n\n\nWe find:\n\nMost of the users have rated 4.0 followed by 3.0.\n\n\ngenre_rating = (df\n                .select(\"movieId\", \"userId\", \"genres\", \"rating\")\n                .withColumn(\"genres_array\", F.split(\"genres\", \"\\|\"))\n                .withColumn(\"genre\", F.explode(\"genres_array\"))\n                .groupBy(\"genre\").agg(F.mean(F.col(\"rating\")).alias(\"genre_rating\"),\n                                      F.countDistinct(\"movieId\").alias(\n                                          \"num_movies\"),\n                                      F.countDistinct(\"movieId\", \"userId\").alias(\"num_ratings\"))\n                ).toPandas()\n\n                                                                                \n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 15))\n\ngenre_rating.plot.barh(\"genre\", \"genre_rating\", ax=axes[0, 0])\naxes[0, 0].set_title(\"Visualizing average rating for each genre\")\n\ngenre_rating.plot.barh(\"genre\", \"num_ratings\", ax=axes[0, 1])\naxes[0, 1].set_title(\"Visualizing number of ratings for each genre\")\n\ngenre_rating.plot.barh(\"genre\", \"num_movies\", ax=axes[1, 0])\naxes[1, 0].set_title(\"Visualizing number of movies in each genre\")\n\nfig.delaxes(axes[1, 1])\nfig.tight_layout()\n\nplt.show()\n\n\n\n\n\n# Analyzing day of the month - Timestamp of rating\nday_month_rating = (df\n                    .withColumnRenamed(\"timestamp\", \"date\")\n                    .withColumn(\"day\", F.dayofmonth(F.col(\"date\")))\n                    .groupBy(\"day\").agg(F.mean(F.col(\"rating\")).alias(\"avg_rating\"),\n                                        F.countDistinct(\"movieId\").alias(\n                                            \"num_movies\"),\n                                        F.countDistinct(\"movieId\", \"userId\").alias(\"num_ratings\"))\n                    ).toPandas()\n\n                                                                                \n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 12))\n\nday_month_rating.plot.scatter(\"day\", \"avg_rating\", ax=axes[0, 0])\naxes[0, 0].set_title(\"Visualizing average rating rated each day\")\n\nday_month_rating.plot.scatter(\"day\", \"num_ratings\", ax=axes[0, 1])\naxes[0, 1].set_title(\"Visualizing number of ratings rated each day\")\n\nday_month_rating.plot.scatter(\"day\", \"num_movies\", ax=axes[1, 0])\naxes[1, 0].set_title(\"Visualizing number of movies rated each day\")\n\nfig.delaxes(axes[1, 1])\nfig.tight_layout()\n\nplt.show()\n\n\n\n\nWe find:\n\nThere is no clear pattern or relation between the day of the month with other features.\n\n\n# Analyzing day of the week - Timestamp of rating\nday_week_rating = (df\n                   .withColumnRenamed(\"timestamp\", \"date\")\n                   .withColumn(\"day\", F.dayofweek(F.col(\"date\")))\n                   .groupBy(\"day\").agg(F.mean(F.col(\"rating\")).alias(\"avg_rating\"),\n                                       F.countDistinct(\"movieId\").alias(\n                                           \"num_movies\"),\n                                       F.countDistinct(\"movieId\", \"userId\").alias(\"num_ratings\"))\n                   ).toPandas()\n\n                                                                                \n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 12))\n\naxes[0, 0].scatter(day_week_rating.day.astype(\n    'int64'), day_week_rating.avg_rating)\naxes[0, 0].set_title(\"Visualizing average rating rated each day\")\n\naxes[0, 1].scatter(day_week_rating.day.astype(\n    'int64'), day_week_rating.num_ratings)\naxes[0, 1].set_title(\"Visualizing number of ratings rated each day\")\n\naxes[1, 0].scatter(day_week_rating.day.astype(\n    'int64'), day_week_rating.num_movies)\naxes[1, 0].set_title(\"Visualizing number of movies rated each day\")\n\nfig.delaxes(axes[1, 1])\nfig.tight_layout()\n\nplt.show()\n\n\n\n\n\nrelease_year_rating = (df\n                       .select(\"title\", \"movieId\", \"userId\", \"rating\")\n                       .withColumn(\"releaseyear\", F.substring('title', -5, 4))\n                       .filter(F.col(\"releaseyear\") &gt; 1900)\n                       .groupBy(\"releaseyear\").agg(F.mean(F.col(\"rating\")).alias(\"avg_rating\"),\n                                                   F.countDistinct(\"movieId\").alias(\n                                                       \"num_movies\"),\n                                                   F.countDistinct(\"movieId\", \"userId\").alias(\"num_ratings\"))\n                       ).toPandas()\n\n                                                                                \n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 12))\n\naxes[0, 0].scatter(release_year_rating.releaseyear.astype(\n    'int64'), release_year_rating.avg_rating)\naxes[0, 0].set_title(\"Visualizing average rating vs Release Year\")\n\naxes[0, 1].scatter(release_year_rating.releaseyear.astype(\n    'int64'), release_year_rating.num_ratings)\naxes[0, 1].set_title(\"Visualizing number of ratings vs Release Year\")\n\naxes[1, 0].scatter(release_year_rating.releaseyear.astype(\n    'int64'), release_year_rating.num_movies)\naxes[1, 0].set_title(\"Visualizing number of movies vs Release Year\")\n\nfig.delaxes(axes[1, 1])\nfig.tight_layout()\n\nplt.show()\n\n\n\n\nWe find:\n\nNumber of movies have increased significantly over the years.\nNumber of rating has a weird uprise and drop."
  },
  {
    "objectID": "posts/spark/movielens-recommendation-system.html#data-preparation",
    "href": "posts/spark/movielens-recommendation-system.html#data-preparation",
    "title": "Movie Lens - Collaborative Filtering based Recommendation System",
    "section": "4. Data Preparation",
    "text": "4. Data Preparation\n\n# Train Test split\n(train, test) = df.select(\n    ['userId', 'movieId', 'rating']).randomSplit([0.8, 0.2])"
  },
  {
    "objectID": "posts/spark/movielens-recommendation-system.html#model-building",
    "href": "posts/spark/movielens-recommendation-system.html#model-building",
    "title": "Movie Lens - Collaborative Filtering based Recommendation System",
    "section": "5. Model Building",
    "text": "5. Model Building\nParameter Description:\n\nrank: rank of the factorization\nmaxIter: max number of iterations (&gt;= 0)\nregParam: regularization parameter (&gt;= 0)\nuserCol: column name for user ids. Ids must be within the integer value range\nitemCol: column name for item ids. Ids must be within the integer value range\nratingCol: column name for ratings\ncoldStartStrategy: strategy for dealing with unknown or new users/items at prediction time. This may be useful in cross-validation or production scenarios, for handling user/item ids the model has not seen in the training data. Supported values: ‚Äònan‚Äô, ‚Äòdrop‚Äô\n\n\n# Basic Model building\nals = ALS(rank=10, maxIter=4, regParam=0.1, userCol='userId',\n          itemCol='movieId', ratingCol='rating', coldStartStrategy=\"drop\")\n\n# Define evaluator as RMSE\nevaluator = RegressionEvaluator(\n    metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n\n\n% % time\n# Fit the model\nmodel = als.fit(train)\n\n                                                                                \n\n\nCPU times: user 168 ms, sys: 40.7 ms, total: 209 ms\nWall time: 3min 24s\n\n\n\n# Evaluate the model\npredictions = model.transform(test)\nrmse = evaluator.evaluate(predictions)\n\nprint('The RMSE for our model is: {}'.format(rmse))\n\n[Stage 170:============================&gt;                            (2 + 2) / 4]                                                                                \n\n\nThe RMSE for our model is: 0.8176288680275328\n\n\n\n5.1. Improving the model\nOne way to improve the model is to tune the hyperparameter of the model. CrossValidator is familiar with sklearn‚Äôs cross_val_score and ParamGridBuilder is a builder for a param grid used in grid search-based model selection.\nThe cross validation consumes enormouse amout of time and hence, below you can find the skeleton code to perform hyper parameter tuning.\n\n# Define the model parameter grid\nparam_grid = ParamGridBuilder()\\\n    .addGrid(als.rank, [12, 13, 14])\\\n    .addGrid(als.maxIter, [18, 19, 20])\\\n    .addGrid(als.regParam, [0.05, 0.5, 0.1])\\\n    .build()\n\n# Initialize the cross validator\ncrossVal = CrossValidator(\n    estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=10)\n\n# Fit the model and perform cross validation\ncvModel = crossVal.fit(train)\n\n# Evaluate the model\ncvPredictions = cvModel.transform(test)\ncvRmse = evaluator.evaluate(cvPredictions)\n\nprint('The RMSE for our model is: {}'.format(cvRmse))"
  },
  {
    "objectID": "posts/spark/movielens-recommendation-system.html#recommend-movies",
    "href": "posts/spark/movielens-recommendation-system.html#recommend-movies",
    "title": "Movie Lens - Collaborative Filtering based Recommendation System",
    "section": "6. Recommend Movies",
    "text": "6. Recommend Movies\nTo recommend movies for a specific user, below is a function that applies the trained model, ALSModel, on the list of movies that the user hasn‚Äôt yet rated\n\ndef recommendMovies(model, user, nRecommendation):\n    # Create a Spark DataFrame with the specified user and all the movies listed in the ratings DataFrame\n    dataSet = ratings.select('movieId').distinct(\n    ).withColumn('userId', F.lit(user))\n\n    # Create a Spark DataFrame with the movies that have already been rated by this user\n    moviesAlreadyRated = ratings.filter(\n        ratings.userId == user).select('movieId', 'userId')\n\n    # Apply the recommender system to the data set without the already rated movies to predict ratings\n    predictions = model.transform(dataSet.subtract(moviesAlreadyRated)).dropna().orderBy(\n        'prediction', ascending=False).limit(nRecommendation).select('movieId', 'prediction')\n\n    # Join with the movies DataFrame to get the movies titles and genres\n    recommendations = predictions.join(movies, predictions.movieId == movies.movieId).select(\n        predictions.movieId, movies.title, movies.genres, predictions.prediction)\n\n    return recommendations\n\nNow run this function to recommend 10 movies for different users:\n\nprint('Recommendations for user 153:')\nrecommendMovies(model, 153, 10).toPandas()\n\nRecommendations for user 153:\n\n\n                                                                                [Stage 420:=======================================&gt;                (7 + 3) / 10]\n\n\n\n\n\n\n\n\n\nmovieId\ntitle\ngenres\nprediction\n\n\n\n\n0\n81117\nMoth, The (Cma) (1980)\nDrama\n6.064673\n\n\n1\n82328\nOne Fine Spring Day (Bomnaleun ganda) (2001)\nDrama\n5.925478\n\n\n2\n109887\nGreat Passage, The (Fune wo amu) (2013)\nDrama\n5.818318\n\n\n3\n116183\nIt's Love I'm After (1937)\nComedy\n5.911806\n\n\n4\n117907\nMy Brother Tom (2001)\nDrama\n6.311569\n\n\n5\n120134\nDoggiewoggiez! Poochiewoochiez! (2012)\nComedy\n6.050812\n\n\n6\n120821\nThe War at Home (1979)\nDocumentary|War\n6.015426\n\n\n7\n121029\nNo Distance Left to Run (2010)\nDocumentary\n6.553770\n\n\n8\n129536\nCode Name Coq Rouge (1989)\n(no genres listed)\n6.092913\n\n\n9\n130347\nBill Hicks: Sane Man (1989)\nComedy\n5.887615\n\n\n\n\n\n\n\n\nprint('Recommendations for user 250:')\nrecommendMovies(model, 250, 10).toPandas()\n\nRecommendations for user 250:\n\n\n                                                                                [Stage 666:&gt;                                                       (0 + 4) / 10]\n\n\n\n\n\n\n\n\n\nmovieId\ntitle\ngenres\nprediction\n\n\n\n\n0\n26793\nTito and Me (Tito i ja) (1992)\nComedy\n4.924314\n\n\n1\n103593\nTaming the Fire (Ukroshcheniye ognya) (1972)\nDocumentary|Drama|Sci-Fi\n4.850996\n\n\n2\n104317\nFlight of the Conchords: A Texan Odyssey (2006)\nComedy\n4.919813\n\n\n3\n107434\nDiplomatic Immunity (2009‚Äì )\nComedy\n4.923127\n\n\n4\n110669\nHonest Liar, An (2014)\nComedy|Documentary\n4.900411\n\n\n5\n118338\nHard to Be a God (2013)\nSci-Fi\n4.911441\n\n\n6\n120134\nDoggiewoggiez! Poochiewoochiez! (2012)\nComedy\n5.056443\n\n\n7\n121029\nNo Distance Left to Run (2010)\nDocumentary\n5.534087\n\n\n8\n128091\nCraig Ferguson: A Wee Bit o' Revolution (2009)\nComedy\n5.608799\n\n\n9\n130347\nBill Hicks: Sane Man (1989)\nComedy\n5.114568"
  },
  {
    "objectID": "posts/spark/movielens-recommendation-system.html#thank-you",
    "href": "posts/spark/movielens-recommendation-system.html#thank-you",
    "title": "Movie Lens - Collaborative Filtering based Recommendation System",
    "section": "Thank you",
    "text": "Thank you\nShare if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "posts/spark/movielens-recommendation-system.html#resources",
    "href": "posts/spark/movielens-recommendation-system.html#resources",
    "title": "Movie Lens - Collaborative Filtering based Recommendation System",
    "section": "Resources",
    "text": "Resources\n\nML Tuning: Model Selection and Hyperparameter Tuning Using PySpark\nALS\nPySpark Documentation"
  },
  {
    "objectID": "posts/spark/movielens-recommendation-system.html#data-citation",
    "href": "posts/spark/movielens-recommendation-system.html#data-citation",
    "title": "Movie Lens - Collaborative Filtering based Recommendation System",
    "section": "Data Citation",
    "text": "Data Citation\nF. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=http://dx.doi.org/10.1145/2827872"
  },
  {
    "objectID": "posts/nlp/natural-language-processing.html",
    "href": "posts/nlp/natural-language-processing.html",
    "title": "What is Natural Language Processing?",
    "section": "",
    "text": "Natural language processing is a branch of Artificial Intelligence which aims to bridge the gap between how a computer and human communicate with each other. The two major handles used for communication are speech and written i.e.¬†text."
  },
  {
    "objectID": "posts/nlp/natural-language-processing.html#history-of-natural-language-processing",
    "href": "posts/nlp/natural-language-processing.html#history-of-natural-language-processing",
    "title": "What is Natural Language Processing?",
    "section": "History of Natural Language Processing",
    "text": "History of Natural Language Processing\nThe dawn of NLP can be dated back to the early 1900s. In 1950, Alan Turing published his famous article ‚ÄúComputing Machinery and Intelligence‚Äù which proposed what is now called the Turing test as a criterion of intelligence. It tests the ability of the computer program to impersonate a human in a real-time conversation with a human judge where the judge is unable to distinguish the human from the computer program. In 1954, the Georgetown experiment automatically translated more than sixty Russian words into English.\nIn 1957, Noam Chomsky‚Äôs Syntactic Structures a rule-based system of syntactic structures with ‚Äúuniversal grammar‚Äù was an incredible advancement. Up to the 1980‚Äôs most of the NLP systems were based on complex hand-written rules but in the late 1980s by the introduction of machine learning algorithms for language processing revolutionized the field. A steady increase in computational power resulting from Moore‚Äôs law and use of statistical models that use probabilistic measures to map the features making up the input data. Watson an artificial intelligence software designed as a question answering system won the Jeopardy contest, defeating the best human players in February 2011.\nDevelopment of famous virtual assistants like Siri in 2011, Amazon Alexa in 2014, and Google Assistant in 2016. The use of deep learning produced better results than the state-of-the-art in many natural language processing tasks, such as machine translation, text classification, and many more. Recent advancements include the use of network architecture of the transformer which is based on the attention mechanism that has produced better results in various NLP tasks.\nWe humans in our daily life overlook the powerful ability of our human brain to read, understand the meaning of a word, it‚Äôs context (how does it relate to each other), understand humor, sarcasm, and thousand other things. How do we teach this to a computer?"
  },
  {
    "objectID": "posts/nlp/natural-language-processing.html#challenges",
    "href": "posts/nlp/natural-language-processing.html#challenges",
    "title": "What is Natural Language Processing?",
    "section": "Challenges",
    "text": "Challenges\n1. Ambiguity:  In a natural language, words are unique but their meaning may differ based on the context in which it is used. One classical example used is: - The bank is a financial institution where customers can save or borrow money. - Tom was sitting by the banks of the river.\nIn this example, we can see that the word ‚Äúbank‚Äù is used in two different ways. The word is the same but the meaning is different. This is because the context in which the word is used is different.\n2. Co-Referencing: It is a process to find all the phrases in the document that refer to the same entity. Example: Harry kept the paneer on the plate and ate it. Here it refers to the paneer that he ate which was kept on the plate.\n3. Information Extraction: Identifying phrases in a language that refer to specific types of entities and relations in text. Named Entity Recognition (NER) is the task used to identify the names of people, organizations, places, etc, in a text. Example: Tom used to work at FedEx and lives in Mumbai, India. where Person = Tom, organization = FedEx and Place = Mumbai, India\n4. Personality, intention, emotions, and style: Different authors may have different personalities, intentions, emotions, and styles to convey the same idea. Based on these factors the underlying idea can be interpreted in different ways. Use of humor or sarcasm may convey a meaning that is opposite of the literal one."
  },
  {
    "objectID": "posts/nlp/natural-language-processing.html#applications",
    "href": "posts/nlp/natural-language-processing.html#applications",
    "title": "What is Natural Language Processing?",
    "section": "Applications",
    "text": "Applications\n1. Machine Translation:  The idea behind machine translation is to develop a system that is capable of translating text from one language to another without any human intervention. Only translating the text from one language to another is not the key. Understanding the meaning behind the text and translating it to some other language is the crux of it. Example: Google Translate\n2. Automatic summarization:  We all love to read storybooks and always a good storybook will have a summary at the end that highlights the important things about the story. Likewise take any piece of text, a story, a news article, etc, and develop a system that can automatically summary the piece of text. Example: Inshorts ‚Äì an app that summarizes each news article in 60 words.\n3. Sentiment Analysis:  It deals with the study of extracting opinions and sentiment that are not always explicitly expressed. For instance it helps the company to understand the level of consumer satisfaction for its goods and services. Example: ‚ÄúI love the new iPhone and it has a great camera.‚Äù.\nAnother branch of sentiment analysis is ‚ÄúAspect based Sentiment Analysis‚Äù where it tries to extract opinions for each aspect from the customer review. Example: ‚ÄúThe new iPhone is great it has a great camera but the battery life is not that good.‚Äù Here the customer is satisfied with the camera aspect of the iPhone but not the battery life.\n4. Text Classification:  Organizing text documents into predefined categories enables to classify the information or any activity. Examples: Classifying an email as spam or not spam.\n5. Question Answering:  Question answering deals with a system that can answer questions posed by humans in natural language. Sounds simple yet building the knowledge base, understanding the text, and to answer in natural language is altogether a thing in itself.\n6. Chatbots:  Chatbots are a piece of software that can simulate a conversation (or chat) with a user in natural language through websites, apps, messaging applications, etc. Chatbots are a natural evolution of question answering system but are one step further with their ability to understand the text and engage in a conversation.\n7. Speech Recognition:  Using our voice to interact with our phones has become a common phenomenon. For example to ask questions to our voice assistants like Google Assistant/Siri/Cortana, use of voice to type a piece of text. Recognizing speech has replaced the method by which we interact with our devices and made it so convenient.\nRecent advancements in NLP have deepened our knowledge on how to tackle the various challenges in NLP. Also, this new decade will be filled with excitement and breakthroughs that awaits us. Stay tunned to deep dive into the world of NLP.\nShare if you like it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "posts/mlops/introduction-workflow-orchestration.html",
    "href": "posts/mlops/introduction-workflow-orchestration.html",
    "title": "Prefect: An Workflow Orchestration Tool",
    "section": "",
    "text": "Prefect is an open-source workflow orchestration tool that helps you automate and manage the flow of work across your data stack.\nPrefect is built on Python and uses a modular architecture that makes it easy to build and deploy complex workflows. Prefect also includes a rich set of features for monitoring, debugging, and managing your workflows.\nBefore we dive into Prefect, let‚Äôs first understand what is workflow orchestration, why do we need it, and where does Prefect come into play."
  },
  {
    "objectID": "posts/mlops/introduction-workflow-orchestration.html#what-is-workflow-orchestration",
    "href": "posts/mlops/introduction-workflow-orchestration.html#what-is-workflow-orchestration",
    "title": "Prefect: An Workflow Orchestration Tool",
    "section": "What is Workflow Orchestration?",
    "text": "What is Workflow Orchestration?\nBuilding a ML system has a lot of moving parts. We have to deal with data collection, data preprocessing, model training, model serving, etc. Each of these steps can be further broken down into sub-steps. For example, data preprocessing can be broken down into feature engineering, feature selection, etc.\nWorkflow Orchestration is the process of automating and managing the flow of work across these steps. It helps us build complex workflows by combining multiple steps together. It also helps us manage the dependencies between these steps."
  },
  {
    "objectID": "posts/mlops/introduction-workflow-orchestration.html#negative-engineering",
    "href": "posts/mlops/introduction-workflow-orchestration.html#negative-engineering",
    "title": "Prefect: An Workflow Orchestration Tool",
    "section": "Negative Engineering",
    "text": "Negative Engineering\nInspite of building a robust system, there are chances that something might go wrong. For example, the data might not be available, the model might fail to train, etc.\n\nOften time while developing any application, we find ourselves spending a lot of time fixing bugs and making sure that everything works as expected. We spend time writing code to handle edge cases and make sure that our application is robust enough to handle any unexpected situation. Exmaples of such situations are:\n\nWriting Retries logic when APIs go down\nBuild Notifications for when a job fails\nRecord logs for observability and debugging\nWrite conditional logic to handle edge cases\nHandle cases when requests timeouts or fails\n\nNegative Engineering refers to ideology that we spend most of our time writing code to handle edge cases rather than writing code that actually solves the problem. This is where Workflow Orchestration comes into play.\nWorflow Orchestration tools provide set of features off the shelf that aim to eliminate the need for negative engineering. These tools provide features like retries, notifications, logging, lineage tracking, etc. out of the box."
  },
  {
    "objectID": "posts/mlops/introduction-workflow-orchestration.html#prefect",
    "href": "posts/mlops/introduction-workflow-orchestration.html#prefect",
    "title": "Prefect: An Workflow Orchestration Tool",
    "section": "Prefect",
    "text": "Prefect\nPrefect aims to eliminate the need for negative engineering by providing a rich set of features out of the box.\n\nSetup\nIt is a good practice to create a virtual environment for each project. This helps us keep our dependencies separate and avoid any version conflicts.\n\n\nrequirements.txt\n\nfastparquet==2023.4.0\nmlflow==2.3.1\npandas==2.0.1\nprefect==2.10.8\nscikit_learn==1.2.2\nxgboost==1.7.5\npsycopg2-binary==2.9.5\n\nLet‚Äôs create a virtual environment named venv and install all the dependencies.\n# Create a virtual environment\nconda create -p venv python=3.10 -y\n\n# Activate the virtual environment\nconda activate venv/\n\n# Install all the dependencies\npip install -r requirements.txt --no-cache-dir\n\n\nPrefect Flow\nWe will be using the NYC Taxi Dataset for this tutorial. The dataset contains information about green taxi trips in New York City. The dataset is available in parquet format.\nSteps that we‚Äôll cover:\n\nWe‚Äôll download the dataset from the NY Taxi website and load it into a pandas dataframe.\nPreprocess the data.\nTrain a model on the preprocessed data.\nLog the model and the metrics to MLflow.\n\nYou‚Äôll wonder where do we use Prefect in this? Answer is in each step.\nWe‚Äôll go through each step in detail. At the end you‚Äôll find the complete code for the workflow.\n\n\nImport Libraries\nimport os\nimport pickle\nimport pathlib\nimport scipy\nimport mlflow\nimport sklearn\nimport argparse\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.metrics import mean_squared_error\nfrom prefect import flow, task\nfrom prefect.tasks import task_input_hash\nfrom datetime import timedelta\n\n\nTwo most important building blocks in Prefect are Task and Flow. We‚Äôll start by importing these two. More on these later.\nfrom prefect import task, flow\n\n\nDownload the dataset\nWe have created a python function named fetch what will download the data from the NY Taxi website and save it to the data directory.\n@task(name=\"Fetch Data\", log_prints=True, retries=3, cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1))\ndef fetch(year: int, month: int, color: str) -&gt; None:\n    \"\"\"Fetches data from the NYC Taxi dataset and saves it locally\"\"\"\n\n    # Download the data from the NYC Taxi dataset\n\n    url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{color}_tripdata_{year}-{month:0&gt;2}.parquet\"\n    file_name = f\"{color}_tripdata_{year}-{month:0&gt;2}.parquet\"\n\n    pathlib.Path(\"data\").mkdir(exist_ok=True)\n\n    os.system(f\"wget {url} -O ./data/{file_name}\")\nLet‚Äôs understand what is happening here.\nThe function is defined in the same way as any other python function. The only difference is that we have added a decorator @task to the function. This decorator converts the python function into a Prefect Task. The decorator takes a few arguments:\n\n@task is a decorator that converts a python function into a Prefect Task.\nname is the name of the task. This is used to identify the task in the Prefect UI.\nlog_prints is a boolean flag that tells Prefect to log the output of the task.\nretries is the number of times the task should be retried in case of failure.\ncache_key_fn is a function that returns a unique key for the task. This is used to cache the output of the task.\ncache_expiration is the time after which the cache should expire.\n\n\n\n\n\n\n\nTerminology Alert: Task\n\n\n\nA task is a unit of work that needs to be done. It can be anything from downloading a file to training a model. Prefect provides a decorator @task that converts a python function into a Prefect Task.\nImagine you are preparing a sandwich. To prepare a delicious sandwich you need to perform a few tasks like: get the bread, apply butter, add cheese, cut down the veggies, etc. Each of these tasks is a unit of work that needs to be done to prepare the sandwich. In Prefect, each of these tasks can be represented as a Prefect Task.\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo enable caching we specified a cache_key_fn which is a function that returns a cache key on our task. We cachec our task based on its input by using task_input_hash which is a function that returns a unique hash for the input of the task.\nIt hashes all inputs to the task and returns a unique hash. If the task inputs do not change, the hash will remain the same and the cachec results are used instead of running the task again until cache expires.\nWe also specified a cache_expiration of 1 day. This means that the cache will expire after 1 day.\n\n\nThis is a good practice while working with large datasets. It helps us avoid downloading the same dataset again and again.\nNext up, we have function named read_data that takes a filename and load the data into a pandas dataframe.\n@task(name=\"Read a Parquet file\")\ndef read_data(filename: str) -&gt; pd.DataFrame:\n    \"\"\"Read data into DataFrame\"\"\"\n    df = pd.read_parquet(filename)\n\n    df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n    df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)\n\n    df[\"duration\"] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n\n    df = df[(df.duration &gt;= 1) & (df.duration &lt;= 60)]\n\n    categorical = [\"PULocationID\", \"DOLocationID\"]\n    df[categorical] = df[categorical].astype(str)\n\n    return df\nThis function is also decorated with @task decorator. Nothing new here.\n\n\nPreprocess the data\nNow that we have the data, we need to preprocess it before we can train a model on it. We‚Äôll create a function named add_features that combines PULocationID and DOLocationID. Also, we‚Äôll represent our features using the DictVectorizer. We‚Äôll perform these steps for both training and validation data.\nAgain, we‚Äôll decorate the function with @task decorator.\n@task(name=\"Add Features\")\ndef add_features(df_train: pd.DataFrame, df_val: pd.DataFrame) -&gt; tuple([\n        scipy.sparse._csr.csr_matrix,\n        scipy.sparse._csr.csr_matrix,\n        np.ndarray,\n        np.ndarray,\n        sklearn.feature_extraction.DictVectorizer,\n    ]):\n    \"\"\"Add features to the model\"\"\"\n    df_train[\"PU_DO\"] = df_train[\"PULocationID\"] + \"_\" + df_train[\"DOLocationID\"]\n    df_val[\"PU_DO\"] = df_val[\"PULocationID\"] + \"_\" + df_val[\"DOLocationID\"]\n\n    categorical = [\"PU_DO\"]  #['PULocationID', 'DOLocationID']\n    numerical = [\"trip_distance\"]\n\n    dv = DictVectorizer()\n\n    train_dicts = df_train[categorical + numerical].to_dict(orient=\"records\")\n    X_train = dv.fit_transform(train_dicts)\n\n    val_dicts = df_val[categorical + numerical].to_dict(orient=\"records\")\n    X_val = dv.transform(val_dicts)\n\n    y_train = df_train[\"duration\"].values\n    y_val = df_val[\"duration\"].values\n    return X_train, X_val, y_train, y_val, dv\n\n\nTrain the model\nWe have an XGBoost model that we want to train. We‚Äôll create a function named train_best_model that takes the training and validation data along with the DictVectorizer and trains the model. We‚Äôll use the best hyperparameters that we found seperately.\nOne awesome thing you‚Äôll observe is that we are also using mlflow to log the model and the hyperparameters. This is a great way to track the model performance and the hyperparameters that were used to train the model.\nDue to the log_prints flag, Prefect will log the output of the task including the output logs from mlflow. This is a great way to visualize the logs in the Prefect UI.\n@task(name=\"Train Model\", log_prints=True)\ndef train_best_model(\n    X_train: scipy.sparse._csr.csr_matrix,\n    X_val: scipy.sparse._csr.csr_matrix,\n    y_train: np.ndarray,\n    y_val: np.ndarray,\n    dv: sklearn.feature_extraction.DictVectorizer,\n) -&gt; None:\n    \"\"\"train a model with best hyperparams and write everything out\"\"\"\n\n    with mlflow.start_run():\n        train = xgb.DMatrix(X_train, label=y_train)\n        valid = xgb.DMatrix(X_val, label=y_val)\n\n        best_params = {\n            \"learning_rate\": 0.09585355369315604,\n            \"max_depth\": 30,\n            \"min_child_weight\": 1.060597050922164,\n            \"objective\": \"reg:linear\",\n            \"reg_alpha\": 0.018060244040060163,\n            \"reg_lambda\": 0.011658731377413597,\n            \"seed\": 42,\n        }\n\n        mlflow.log_params(best_params)\n\n        booster = xgb.train(\n            params=best_params,\n            dtrain=train,\n            num_boost_round=100,\n            evals=[(valid, \"validation\")],\n            early_stopping_rounds=20,\n        )\n\n        y_pred = booster.predict(valid)\n        rmse = mean_squared_error(y_val, y_pred, squared=False)\n        mlflow.log_metric(\"rmse\", rmse)\n\n        pathlib.Path(\"models\").mkdir(exist_ok=True)\n        with open(\"models/preprocessor.b\", \"wb\") as f_out:\n            pickle.dump(dv, f_out)\n        mlflow.log_artifact(\"models/preprocessor.b\", artifact_path=\"preprocessor\")\n\n        mlflow.xgboost.log_model(booster, artifact_path=\"models_mlflow\")\n    return None\nNow, we have our indenpendent tasks ready. Let‚Äôs create a flow that will run these tasks in the order we want.\n\n\nCreate a flow\nWe‚Äôll create a flow named main_flow that will run the tasks in the order we want. We‚Äôll also set the name of the flow to ‚ÄúMain Flow‚Äù. This is a great way to identify the flow in the Prefect UI.\nObserve that yet again the flow is simply a function decorated with @flow decorator. We‚Äôll also set the params argument to the flow. These paramarater also get recorded in the Prefect UI which can be very useful to track the flow runs.\n\n\n\n\n\n\nTerminology Alert: Flow\n\n\n\nA flow is a collection of tasks that are executed in a particular order. A flow is a function decorated with @flow decorator. The flow function can take arguments and return values.\nImagine flow is a container that holds all the tasks and the order in which they are executed. Making a delicious sandwich is a flow and the actions you take to make the sandwich are the tasks.\n\n\n@flow(name=\"Main Flow\")\ndef main_flow(params):\n    \"\"\"Main flow of the program\"\"\"\n    \n    # MLflow settings\n    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n    mlflow.set_experiment(\"nyc-taxi-experiment\")\n\n    # Download and read data\n    df_train, df_val = download_and_read(params.years, params.months, params.color)\n\n    # Transform\n    X_train, X_val, y_train, y_val, dv = add_features(df_train, df_val)\n\n    # Train\n    train_best_model(X_train, X_val, y_train, y_val, dv)\nIf you observe closely, you‚Äôll notice that we have a download_and_read function but we didn‚Äôt define that anywhere to avoid confusion until now. We‚Äôll define that function next but that function will be a Subflow in our main flow.\n\n\nCreate a subflow\nImage a subflow as a flow within a flow. A subflow is a function decorated with the same @flow decorator but it is called from within another flow. A subflow can also take arguments and return values.\nWe‚Äôll create a subflow named download_and_read that will call the previously defined functions (tasks) to download the data and read it into a pandas dataframe. We‚Äôll also set the name of the subflow to ‚ÄúDownload and Read‚Äù. This is a great way to identify the subflow in the Prefect UI.\n@flow(name=\"Subflow - Download and Read Data\", log_prints=True)\ndef download_and_read(years: list, months: list, color: str):\n    # Download the data from the NYC Taxi dataset\n    for year in years:\n        for month in months:\n            print(f\"Download: Year-Month: {year}-{month} ({color})\")\n            fetch(year, month, color)\n\n    # Read the data into a DataFrame\n    track_data =[[], []]\n    df_train = pd.DataFrame()\n    for month in months[:-1]:\n        print(f\"Read: Year-Month: {year}-{month:0&gt;2} ({color})\")\n        df = read_data(f\"./data/{color}_tripdata_{year}-{month:0&gt;2}.parquet\")\n        df_train = pd.concat([df_train, df], ignore_index=True)\n        track_data[0].append(month)\n\n    print(f\"Read: Year-Month: {year}-{months[-1]:0&gt;2} ({color})\")\n    df_val = read_data(f\"./data/{color}_tripdata_{year}-{months[-1]:0&gt;2}.parquet\")\n    track_data[1].append(months[-1])\n\n    print(f\"Training data consists of months: {track_data[0]}\")\n    print(f\"Validation data consists of months: {track_data[1]}\")\n\n    return df_train, df_val\n\n\n\n\n\n\nTerminology Alert: Subflow\n\n\n\nA subflow is a flow within a flow. A subflow is a function decorated with @flow decorator. The subflow function can take arguments and return values.\nImagine you want to make a spicy sauce to have with your sandwich. To make a sauce you need to follow a recipe with multiple steps. Consider this as a subflow within the flow of making a sandwich.\n\n\n\n\nRun the flow\nWe‚Äôll run the script by calling the main_flow with the necessary arguments.\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Ingest CSV data to Postgres')\n\n    parser.add_argument(\"--years\", nargs=\"+\", required=True, help=\"Data from year\")\n    parser.add_argument(\"--months\", nargs=\"+\", required=True, help=\"Data from months\")\n    parser.add_argument(\"--color\", required=True, help=\"Taxi color\", default=\"green\")\n\n    args = parser.parse_args()\n\n    main_flow(args)\nExample, to run the flow for the months of January, February and March of 2021 for green taxis, we‚Äôll run the following command:\npython orchestration.py --years 2021 --months 1 2 3 --color green\nIllustration of the flow of execution of the tasks in the flow.\nmain_flow                       (Flow)\n    |\n    |---- download_and_read     (Subflow)\n    |           |\n    |           |---- fetch     (Task)\n    |           |---- read_data (Task)\n    |---- add_features          (Task)\n    |---- train_best_model      (Task)\n\nThis will run the flow and log all the tasks in the Prefect UI.\nBelow you‚Äôll find some screenshots of the Prefect UI, sample output on terminal, and MLflow UI.\nSample output on terminal:\n\nPrefect UI:\nYou can start the Prefect server using the command prefect server start and then navigate to http://127.0.0.1:4200 to view the Prefect UI.\n\nYou‚Äôll find both the main flow and the subflow on the page as it records all the flows that are run. You can click on the flow to view the details of the flow. Main flow:\n\nSubflow:\n\nLastly you‚Äôll find your experiment in the MLflow UI:\nRun the mlflow UI using the command mlflow server --backend-store-uri sqlite:///mlflow.db --host 0.0.0.0 --port 8080 and then navigate to http://127.0.0.1:8080 to view the MLflow UI. Note: MLflow runs on port 5000 by default but we are running it on port 8080.\n\nCongratulations on running your first Prefect flow! üéâ\nYou can find the whole code below:\n\n\norchestration.py\n\nimport os\nimport pickle\nimport pathlib\nimport scipy\nimport mlflow\nimport sklearn\nimport argparse\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.metrics import mean_squared_error\nfrom prefect import flow, task\nfrom prefect.tasks import task_input_hash\nfrom datetime import timedelta\n\n@task(name=\"Fetch Data\", log_prints=True, retries=3, cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1))\ndef fetch(year: int, month: int, color: str) -&gt; None:\n    \"\"\"Fetches data from the NYC Taxi dataset and saves it locally\"\"\"\n\n    # Download the data from the NYC Taxi dataset\n\n    url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{color}_tripdata_{year}-{month:0&gt;2}.parquet\"\n    file_name = f\"{color}_tripdata_{year}-{month:0&gt;2}.parquet\"\n\n    pathlib.Path(\"data\").mkdir(exist_ok=True)\n\n    os.system(f\"wget {url} -O ./data/{file_name}\")\n\n@task(name=\"Read a Parquet file\")\ndef read_data(filename: str) -&gt; pd.DataFrame:\n    \"\"\"Read data into DataFrame\"\"\"\n    df = pd.read_parquet(filename)\n\n    df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n    df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)\n\n    df[\"duration\"] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n\n    df = df[(df.duration &gt;= 1) & (df.duration &lt;= 60)]\n\n    categorical = [\"PULocationID\", \"DOLocationID\"]\n    df[categorical] = df[categorical].astype(str)\n\n    return df\n\n@task(name=\"Add Features\")\ndef add_features(df_train: pd.DataFrame, df_val: pd.DataFrame) -&gt; tuple([\n        scipy.sparse._csr.csr_matrix,\n        scipy.sparse._csr.csr_matrix,\n        np.ndarray,\n        np.ndarray,\n        sklearn.feature_extraction.DictVectorizer,\n    ]):\n    \"\"\"Add features to the model\"\"\"\n    df_train[\"PU_DO\"] = df_train[\"PULocationID\"] + \"_\" + df_train[\"DOLocationID\"]\n    df_val[\"PU_DO\"] = df_val[\"PULocationID\"] + \"_\" + df_val[\"DOLocationID\"]\n\n    categorical = [\"PU_DO\"]  #'PULocationID', 'DOLocationID']\n    numerical = [\"trip_distance\"]\n\n    dv = DictVectorizer()\n\n    train_dicts = df_train[categorical + numerical].to_dict(orient=\"records\")\n    X_train = dv.fit_transform(train_dicts)\n\n    val_dicts = df_val[categorical + numerical].to_dict(orient=\"records\")\n    X_val = dv.transform(val_dicts)\n\n    y_train = df_train[\"duration\"].values\n    y_val = df_val[\"duration\"].values\n    return X_train, X_val, y_train, y_val, dv\n\n\n@flow(name=\"Subflow - Download and Read Data\", log_prints=True)\ndef download_and_read(years: list, months: list, color: str):\n    # Download the data from the NYC Taxi dataset\n    for year in years:\n        for month in months:\n            print(f\"Download: Year-Month: {year}-{month} ({color})\")\n            fetch(year, month, color)\n\n    # Read the data into a DataFrame\n    track_data =[[], []]\n    df_train = pd.DataFrame()\n    for month in months[:-1]:\n        print(f\"Read: Year-Month: {year}-{month:0&gt;2} ({color})\")\n        df = read_data(f\"./data/{color}_tripdata_{year}-{month:0&gt;2}.parquet\")\n        df_train = pd.concat([df_train, df], ignore_index=True)\n        track_data[0].append(month)\n\n    print(f\"Read: Year-Month: {year}-{months[-1]:0&gt;2} ({color})\")\n    df_val = read_data(f\"./data/{color}_tripdata_{year}-{months[-1]:0&gt;2}.parquet\")\n    track_data[1].append(months[-1])\n\n    print(f\"Training data consists of months: {track_data[0]}\")\n    print(f\"Validation data consists of months: {track_data[1]}\")\n\n    return df_train, df_val\n\n@task(name=\"Train Model\", log_prints=True)\ndef train_best_model(\n    X_train: scipy.sparse._csr.csr_matrix,\n    X_val: scipy.sparse._csr.csr_matrix,\n    y_train: np.ndarray,\n    y_val: np.ndarray,\n    dv: sklearn.feature_extraction.DictVectorizer,\n) -&gt; None:\n    \"\"\"train a model with best hyperparams and write everything out\"\"\"\n\n    with mlflow.start_run():\n        train = xgb.DMatrix(X_train, label=y_train)\n        valid = xgb.DMatrix(X_val, label=y_val)\n\n        best_params = {\n            \"learning_rate\": 0.09585355369315604,\n            \"max_depth\": 30,\n            \"min_child_weight\": 1.060597050922164,\n            \"objective\": \"reg:linear\",\n            \"reg_alpha\": 0.018060244040060163,\n            \"reg_lambda\": 0.011658731377413597,\n            \"seed\": 42,\n        }\n\n        mlflow.log_params(best_params)\n\n        booster = xgb.train(\n            params=best_params,\n            dtrain=train,\n            num_boost_round=100,\n            evals=[(valid, \"validation\")],\n            early_stopping_rounds=20,\n        )\n\n        y_pred = booster.predict(valid)\n        rmse = mean_squared_error(y_val, y_pred, squared=False)\n        mlflow.log_metric(\"rmse\", rmse)\n\n        pathlib.Path(\"models\").mkdir(exist_ok=True)\n        with open(\"models/preprocessor.b\", \"wb\") as f_out:\n            pickle.dump(dv, f_out)\n        mlflow.log_artifact(\"models/preprocessor.b\", artifact_path=\"preprocessor\")\n\n        mlflow.xgboost.log_model(booster, artifact_path=\"models_mlflow\")\n    return None\n\n@flow(name=\"Main Flow\")\ndef main_flow(params):\n    \"\"\"Main flow of the program\"\"\"\n    \n    # MLflow settings\n    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n    mlflow.set_experiment(\"nyc-taxi-experiment\")\n\n    # Download and read data\n    df_train, df_val = download_and_read(params.years, params.months, params.color)\n\n    # Transform\n    X_train, X_val, y_train, y_val, dv = add_features(df_train, df_val)\n\n    # Train\n    train_best_model(X_train, X_val, y_train, y_val, dv)\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Ingest CSV data to Postgres')\n\n    parser.add_argument(\"--years\", nargs=\"+\", required=True, help=\"Data from year\")\n    parser.add_argument(\"--months\", nargs=\"+\", required=True, help=\"Data from months\")\n    parser.add_argument(\"--color\", required=True, help=\"Taxi color\", default=\"green\")\n\n    args = parser.parse_args()\n\n    main_flow(args)"
  },
  {
    "objectID": "posts/mlops/introduction-workflow-orchestration.html#further-reading",
    "href": "posts/mlops/introduction-workflow-orchestration.html#further-reading",
    "title": "Prefect: An Workflow Orchestration Tool",
    "section": "Further Reading",
    "text": "Further Reading\n\nPrefect Flows\nPrefect Tasks\nPrefect Tutorials\n\nIn the next post, we will see what are prefect blocks and how to use them to build a more complex flow. Also, how to deploy our flows to Prefect Cloud and run them on a schedule.\nThank you for reading and I hope you found this notebook helpful. Upvote if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "posts/mlops/setup-aws-instance.html",
    "href": "posts/mlops/setup-aws-instance.html",
    "title": "AWS Instance Setup",
    "section": "",
    "text": "In this tutorial we will learn how to setup an AWS EC2 instance, setup key-pair and connect to the instance via ssh. We will also install python, docker and docker-compose on the instance.\nLet‚Äôs dive in and get your environment up and running seamlessly!"
  },
  {
    "objectID": "posts/mlops/setup-aws-instance.html#setup-aws-instance",
    "href": "posts/mlops/setup-aws-instance.html#setup-aws-instance",
    "title": "AWS Instance Setup",
    "section": "Setup AWS Instance",
    "text": "Setup AWS Instance\n\nGo to https://aws.amazon.com to Sign in / Create an AWS Account.\nTo launch EC2 instance, click on to services on the left-top corner of the page. Select Compute and EC2.\n\n\n\nTo launch a new instance, click on Launch Instance.\n\n\n\nSet any Name to the instance and select Ubuntu in the Application and OS Images section. Also, choose Ubuntu Server 20.04 LTS (HVM), SSD Volume Type as the Amazon Machine Image (AMI).\n\n\n\nSelect t2.xlarge as the Instance type for our instance. As 16GiB of memory should be ideal for our work.\n\n\n\nIf you don‚Äôt already have a Key pair, you can create a new key pair. You would be asked to download and save your key pair.\n\n\n\n\n\n\n\nTip\n\n\n\nSave your key pair at ~/.ssh/ folder.\n\n\n\n\n\n\nLastly, increase the storage to 30 GiB as we would be working with large file and docker images with would consume some space.\n\n\n\nClick on Launch Instance to create and start the new instance."
  },
  {
    "objectID": "posts/mlops/setup-aws-instance.html#connect-to-instance-via-ssh",
    "href": "posts/mlops/setup-aws-instance.html#connect-to-instance-via-ssh",
    "title": "AWS Instance Setup",
    "section": "Connect to Instance via SSH",
    "text": "Connect to Instance via SSH\nOnce your instance is running, you can go to the instance summary page to checkout your Public IPv4 address which will be used to connect to the instance via ssh.\nTo check whether you can establish a connection to your instance:\nssh -i ~/.ssh/mlops-zc-key.pem ubuntu@&lt;your-public-ipv4-address&gt;\nEg. ssh -i ~/.ssh/mlops-zc-key ubuntu@34.236.146.20\n\n\n\n\n\n\nbad permissions error\n\n\n\nIf you receive an error like:\nIt is required that your private key files are NOT accessible by others. This private key will be ignored.\nChange the file permission using the command:\nchmod go-r ~/.ssh/mlops-zc-key.pem\n\n\nIf asked about ‚ÄúAre you sure you want to continue connecting (yes/no/[fingerprint])?‚Äù, type yes.\nRather than manually entering the whole command, you can save the configurations at ~/.ssh/config file. If you don‚Äôt already have an file name config in the ~/.ssh/ directory then go to your .ssh directory and use the command touch config to create the file.\nTo edit the file use the command vim ~/.ssh/config.\nIn the file add the following details:\nHost mlops-zoomcamp\n    HostName &lt;your-public-ipv4-address&gt;\n    User ubuntu\n    IdentityFile ~/.ssh/mlops-zc-key.pem\n\n\n\n\n\n\nCaution\n\n\n\nEvery time you stop and start your instance you would have to edit the config file and change the public ipv4 address with the new ipv4 address."
  },
  {
    "objectID": "posts/mlops/setup-aws-instance.html#install-softwares",
    "href": "posts/mlops/setup-aws-instance.html#install-softwares",
    "title": "AWS Instance Setup",
    "section": "Install Softwares",
    "text": "Install Softwares\n\nPython\n\nCreate a directory called downloads and move into the directory.\n\nmkdir downloads\ncd downloads\n\nDownload and Install Anaconda\n\nwget https://repo.anaconda.com/archive/Anaconda3-2023.03-1-Linux-x86_64.sh\nbash Anaconda3-2023.03-1-Linux-x86_64.sh\n\nFor the following prompts:\n\nAnaconda3 will now be installed into this location:\n/home/ubuntu/anaconda3\nPress Enter\nDo you wish the installer to initialize Anaconda3\nby running conda init? [yes|no]\n[no] &gt;&gt;&gt;\nType yes\nPython installed üéâ\n\n\nDocker\n\nUpdate packages using the command\n\nsudo apt update\n\nInstall Docker\n\nsudo apt install docker.io\n\nCheck docker is installed by running hello-world program in docker:\n\nsudo docker run hello-world\nWe don‚Äôt want to use sudo everytime we run docker. To do that, create the docker group and add your user:\n\nCreate the docker group.\n\nsudo groupadd docker\n\nAdd your user to the docker group.\n\nsudo usermod -aG docker $USER\n\nLog out and log back in so that your group membership is re-evaluated.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you‚Äôre running Linux in a virtual machine, it may be necessary to restart the virtual machine for changes to take effect.\n\n\n\nTo check if you can run docker without using sudo, use the command\n\ndocker --help\nDocker installed üéâ\n\n\ndocker-compose\n\nMove to the home directory. It will look like\n\nubuntu@ip-172-31-19-228:~/downloads$ cd ..\nubuntu@ip-172-31-19-228:~$ ls\nanaconda3 downloads\n\nCreate a folder named soft and move into the folder.\n\nmkdir soft\ncd soft\n\nDownload docker-compose and make the file executable\n\nwget https://github.com/docker/compose/releases/download/v2.18.1/docker-compose-linux-x86_64 -O docker-compose\n\nchmod +x docker-compose\nTo access docker-compose from any location we need to add the PATH to the bashrc file. Again move to the home directory.\n\nOpen the .bashrc file using the command\n\nvim .bashrc\n\nMove to the end of the file and press i to go into Insert Mode.\nAdd the following peice of code in the file.\n\nexport PATH=\"${HOME}/soft:${PATH}\"\n\nSave and Exit the file:\n\nPress esc key\nType :wq and hit enter\n\n\n\nRun the command to execute the file:\n\nsource .bashrc\n\nCross-check by typing which docker-compose. You‚Äôll get the output as /home/ubuntu/soft/docker-compose\n\ndocker-compose installed üéâ\n\n\n\n\n\n\nSTOP EC2 instance\n\n\n\nPlease remember to stop the EC2 instance after completing your work to avoid incurring any additional charges.\n\n\nThank you for reading and I hope you found this notebook helpful. Upvote if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "posts/mlops/prefect-cloud.html",
    "href": "posts/mlops/prefect-cloud.html",
    "title": "Prefect Cloud Deployment",
    "section": "",
    "text": "Prefect Cloud is a hosted platform for managing your data workflows. That means you don‚Äôt have to own your own servers. We know Prefect follows a hybrid approach, which means the storage of your flow and the execution environment (prefect server) are seperate from each other.\nWhy should this matter to you? Simply, it means you can run your code on your own infrastructure keeping the code and data on your own servers. Only the metadata about the run is sent to Prefect Cloud. This is a great way to keep your data secure.\nTo understand this better, let‚Äôs look at it with an example. We‚Äôll follow through the following steps:\nIn our example, we‚Äôll store our code on GitHub and use the Prefect Cloud to record your runs. We‚Äôll start the prefect worker locally and run the flow locally.\nEverytime you run the deployed flow, the worker will clone the github repo and use the code to run the flow. All the metadata about the run will be sent to Prefect Cloud.\nLet‚Äôs get started!"
  },
  {
    "objectID": "posts/mlops/prefect-cloud.html#deployment-on-prefect-cloud",
    "href": "posts/mlops/prefect-cloud.html#deployment-on-prefect-cloud",
    "title": "Prefect Cloud Deployment",
    "section": "Deployment on Prefect Cloud",
    "text": "Deployment on Prefect Cloud\n\nStep 1: Create a GitHub repo\nLogin to your GitHub account and create a new repository. Name it prefect-cloud-demo for instance.\nLocally, initialize a git repo and add the remote origin to your local repo.\n# Initialize a git repo\ngit init\n# Add the remote origin to your local repo\ngit remote add origin [your-github-repo-url]\n\n\nStep 2: Setup\nIt is a good practice to create a virtual environment for each project. This helps us keep our dependencies separate and avoid any version conflicts.\n\n\nrequirements.txt\n\nfastparquet==2023.4.0\nmlflow==2.3.1\npandas==2.0.1\nprefect==2.10.8\nscikit_learn==1.2.2\nxgboost==1.7.5\npsycopg2-binary==2.9.5\n\nLet‚Äôs create a virtual environment named venv and install all the dependencies.\n# Create a virtual environment\nconda create -p venv python=3.10 -y\n\n# Activate the virtual environment\nconda activate venv/\n\n# Install all the dependencies\npip install -r requirements.txt --no-cache-dir\n\n\nStep 3: Create a prefect project\nCreate a prefect project using the CLI.\nprefect project init\nThe folder structure of your project should look like this:\n.\n‚îú‚îÄ‚îÄ deployment.yaml\n‚îú‚îÄ‚îÄ prefect.yaml\n‚îú‚îÄ‚îÄ requirements.txt\n‚îî‚îÄ‚îÄ venv\n‚îî‚îÄ‚îÄ ./prefect (hidden)\n‚îî‚îÄ‚îÄ .prefectignore (hidden)\n\n\n\n\n\n\nNote\n\n\n\nOne thing to note here is that in the prefect.yaml file you‚Äôll see key named pull with a value for git_clone_project. It will contain the url of your GitHub repo. It automatically gets added when you run the prefect project init command in a git repo.\nEverytime you run a deployment flow it will pull the code from the repo to run it.\n\n\n\n\nStep 3: Write a prefect flow, configure the yaml file, and push it to GitHub\nWe‚Äôll use the same flow we created in the previous section to keep things simple.\n\n\n\"prefect_demo.py\n\nfrom prefect import flow, task\n\n@task(name=\"say_hello\", log_prints=True)\ndef say_hello():\n    print(\"Hello, I'm a Task!\")\n\n@flow(name=\"mlflow-flow\", log_prints=True)\ndef flow_hello():\n    print(\"Hello, I'm a Flow!\")\n    print(\"I'm about to call a Task...\")\n    say_hello()\n\nif __name__ == \"__main__\":\n    flow_hello()\n\nLet‚Äôs now configure the deployment.yaml file. This file tells Prefect Cloud how to deploy your flow.\n\n\ndeployment.yaml\n\ndeployments:\n- # base metadata\n  name: manual-deployment\n  tags: [\"test\"]\n  description: \"Trigger deployment using `run` CLI command or Prefect UI\"\n  \n  # flow-specific fields\n  entrypoint: prefect_demo.py:flow_hello\n  parameters:\n    name: \"Sagar\"\n  \n  # infra-specific fields\n  work_pool:\n    name: test-pool\n    work_queue_name: demo\n\nLet‚Äôs now push the code to GitHub.\n# Add all the files to git\ngit add .\n\n# Add .gitignore\ntouch .gitignore\n# Add the following to .gitignore file from https://github.com/github/gitignore/blob/main/Python.gitignore\n\n# Commit the changes\ngit commit -m \"Initial commit\"\n\n# Push the code to GitHub\ngit push -u origin main\n\n\nStep 4: Log into Prefect Cloud through CLI\nIf you haven‚Äôt already created an account on Prefect Cloud, you can do so here. Enter your email and you‚Äôll receive a link to login to Prefect Cloud.\nNow, we‚Äôll create a workspace in Prefect Cloud. Workspaces are only available on prefect cloud. Workspaces offer the flexibility to organize and compartmentalize your workflows according to your preferences.\nFor instance, you can leverage separate workspaces to isolate development, staging, and production environments. They also provide a means to create clear boundaries between different teams, ensuring better organization and collaboration.\nCreate a workspace named demo.\nLet‚Äôs go ahead and login to Prefect Cloud through CLI. Run the following command to see that which server you‚Äôre connected to by default.\nprefect version\nUnder the Server section, you‚Äôll notice Database as sqlite and SQLite version mentioned. This means when you run the flow, it will create a sqlite database on your local machine and store the metadata there.\nVersion:             2.10.8\nAPI version:         0.8.4\nPython version:      3.10.11\nGit commit:          79093235\nBuilt:               Mon, May 8, 2023 12:23 PM\nOS/Arch:             darwin/x86_64\nProfile:             default\nServer type:         ephemeral\nServer:\n  Database:          sqlite\n  SQLite version:    3.41.2\nLet‚Äôs now login to Prefect Cloud through CLI. Run the following command\nprefect cloud login\nand you‚Äôll be presented with two options:\n\nLog in with a web browser.\nPaste an API key.\n\nLet‚Äôs go ahead and login with a web browser. By default, option 1 will be highlighted and you can press enter to continue. This will open a browser window and you‚Äôll be asked to authorize.\nOnce you‚Äôve authorized, you‚Äôll see a message like Authenticated with Prefect Cloud! Using workspace '[email]/demo'.\nNow if you run prefect version, you‚Äôll see that the server has changed to cloud. This means that when you run the flow, it will upload the metadata to Prefect Cloud.\nVersion:             2.10.8\nAPI version:         0.8.4\nPython version:      3.10.11\nGit commit:          79093235\nBuilt:               Mon, May 8, 2023 12:23 PM\nOS/Arch:             darwin/x86_64\nProfile:             default\nServer type:         cloud\n\n\nStep 5: Deploy the flow to Prefect Cloud\nBefore we deploy the flow, let‚Äôs create a work pool. Follow the steps below:\n\nGo to the Work Pools tab on the left side of the screen.\nClick on the+ icon on the top middle of the screen.\nEnter the name of the work pool as test-pool.\nSelect the Local Subprocess as the Infrastructure Type.\nClick on the Create button.\n\nNow, let‚Äôs deploy the flow to Prefect Cloud. Run the following command:\nprefect deploy --name manual-deployment\n\n\nStep 6: Run the deployed flow\nTo run the deployed flow, we need a worker that will run the flow. Run the following command to start a worker:\nprefect worker start -p test-pool\nOpen a new terminal to run the flow. Now we have two options to run the deployed flow:\n\nThrough CLI\nThrough Prefect UI\n\nPreviously I have mostly used the Prefect UI to run the flow. But in this case, we‚Äôll use the CLI to run the flow. Run the following command to run the flow:\n# Command to run the flow\nprefect deployment run [DEPLOYMENT-NAME]\n\n# In our case, the command will be\nprefect deployment run  hello-flow/manual-deployment\nIf you don‚Äôt know the name of the deployment, you can run the following command to see all the deployments:\nprefect deployment ls\nCongratulations üéâüéâüéâ! You‚Äôve successfully deployed and run your first flow on Prefect Cloud.\nPrefect gives you the flexibility to store your code anywhere and run it anywhere. You can start the worker on local, EC2, or any other machine and run the flow.\nNow, let‚Äôs see how to run the deployment with mlflow to record the metrics."
  },
  {
    "objectID": "posts/mlops/prefect-cloud.html#run-the-deployment-with-mlflow-optional",
    "href": "posts/mlops/prefect-cloud.html#run-the-deployment-with-mlflow-optional",
    "title": "Prefect Cloud Deployment",
    "section": "Run the deployment with mlflow [Optional]",
    "text": "Run the deployment with mlflow [Optional]\nWe‚Äôll tweak the flow a little bit to record the metrics with mlflow. The code is super super simple, however, the main take away is how to set the tracking uri and start the mlflow server to track the metrics.\n\n\nmlflow_demo.py\n\nimport mlflow\nfrom prefect import flow, task\n\n@task(name=\"mlflow-run\", log_prints=True)\ndef mlflow_run():\n  with mlflow.start_run():\n    mlflow.log_metric(\"metric\", 1.0)\n\n@flow(name=\"mlflow-flow\", log_prints=True)\ndef flow_hello():\n    mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n    mlflow.set_experiment(\"demo\")\n    \n    print(\"Hello, I'm a Flow!\")\n    print(\"I'm about to call a Task...\")\n    mlflow_run()\n\nif __name__ == \"__main__\":\n    flow_hello()\n\nLet‚Äôs now configure the deployment.yaml file. Add the following lines to the deployment.yaml file.\n\n\ndeployment.yaml\n\n- # base metadata\n  name: mlflow-deployment\n  description: \"Trigger deployment using `run` CLI command or Prefect UI\"\n  \n  # flow-specific fields\n  entrypoint: mlflow_demo.py:flow_hello\n  \n  # infra-specific fields\n  work_pool:\n    name: test-pool\n    work_queue_name: mlflow-demo\n\nLet‚Äôs now push the code to GitHub.\n# Add all the files to git\ngit add .\n\n# Commit the changes\ngit commit -m \"Add mlflow demo\"\n\n# Push the code to GitHub\ngit push -u origin main\nNow, we‚Äôll deploy the new flow to Prefect Cloud. Run the following command:\n# Deploy the flow\nprefect deploy --name mlflow-deployment\n\n# Checkout the deployment name\nprefect deployment ls\nWe already have a worker running. However, if you don‚Äôt have a worker running, you can start a worker by running the following command:\nprefect worker start -p test-pool\nLet‚Äôs start the mlflow server. Run the following command:\nmlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./artifacts --host 127.0.0.1 --port 5000\nNow, let‚Äôs run the flow. Run the following command:\nprefect deployment run mlflow-flow/mlflow-deployment\nSome pointer to note:\n\nWe set the tracking uri and experiment name in the function where the flow begins.\n\nThis is important because if you set the tracking uri and experiment name in the __main__ function, it will not work.\nWhen you run the flow, it runs the function where the flow begins.\nIn our case, it‚Äôs the flow_hello function.\n\nWe set the tracking uri to http://127.0.0.1:5000 because we ran the server locally, however if you run the server on EC2 or any other machine, you need to set the tracking uri accordingly.\nTo start the mlflow server we used the backend store as sqlite db and artifacts directory to store the artifacts.\nWe also specified the host and post so as to map the mlflow server to the same port where we set the tracking uri.\n\nLastly, if you want to logout from the Prefect Cloud, run the following command:\nprefect cloud logout"
  },
  {
    "objectID": "posts/mlops/prefect-cloud.html#references",
    "href": "posts/mlops/prefect-cloud.html#references",
    "title": "Prefect Cloud Deployment",
    "section": "References",
    "text": "References\n\nPrefect Cloud Docs\nImage Source"
  },
  {
    "objectID": "posts/mlops/prefect-cloud.html#conclusion",
    "href": "posts/mlops/prefect-cloud.html#conclusion",
    "title": "Prefect Cloud Deployment",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we learned how to deploy a flow to Prefect Cloud and run it. We also learned how to run the flow with mlflow to record the metrics.\nI hope you enjoyed this article. If you have any questions, feel free to reach out to me on Twitter or Email.\nüëè Upvote if you liked it, üí¨ comment if you loved it. Hope to see you guys in the next one. ‚úåÔ∏è Peace!"
  },
  {
    "objectID": "posts/mlops/introduction-to-mlops.html",
    "href": "posts/mlops/introduction-to-mlops.html",
    "title": "Introduction to MLOps",
    "section": "",
    "text": "For as long as I can remember, Jupyter Notebooks have been my go-to tool for machine learning (ML) projects. When it comes to experimentation, prototyping, and data exploration, Jupyter Notebooks undoubtedly reign supreme. However, automating and operationalizing ML products presents a significant challenge.\nThat‚Äôs where MLOps (Machine Learning Operations) comes into play. MLOps refers to a set of practices aimed at fostering collaboration and communication between data scientists and operations professionals. It bridges the gap between ML development and deployment, streamlining the process and ensuring efficient and scalable ML product deployment.\nSource: Neptune.ai"
  },
  {
    "objectID": "posts/mlops/introduction-to-mlops.html#drawbacks-on-working-with-jupyter-notebooks",
    "href": "posts/mlops/introduction-to-mlops.html#drawbacks-on-working-with-jupyter-notebooks",
    "title": "Introduction to MLOps",
    "section": "Drawbacks on working with Jupyter Notebooks",
    "text": "Drawbacks on working with Jupyter Notebooks\n\nLack of reproducibility: The flexibility to execute cells independently and out of order can make reproducing experiments challenging. It is crucial to carefully document the order of cell execution and ensure that all necessary cells are run to achieve reproducible results.\nIssues with version control: Jupyter Notebooks are not designed to work seamlessly with version control systems like Git. The JSON-based structure of notebooks makes it difficult to track changes, merge conflicts, and collaborate effectively when multiple team members are working on the same notebook simultaneously.\nLack of scalability: Jupyter Notebooks may face limitations in dealing with large datasets or computationally demanding tasks. Due to their single kernel architecture, they may struggle with memory restrictions and long execution times when working with big data or complex machine learning models.\nLimited code organization: While Jupyter Notebooks allow for code organization using cells and sections, larger notebooks can become challenging to navigate and maintain. As the size of the notebook grows, it can be harder to find specific sections of code, leading to reduced code readability and maintainability.\nPerformance limitations: Jupyter Notebooks excel in providing an interactive and exploratory environment, but they may not be the most performant option for computationally intensive tasks. For tasks requiring high-speed execution or efficient memory utilization, alternative tools like Python scripts or specialized frameworks may be more suitable."
  },
  {
    "objectID": "posts/mlops/introduction-to-mlops.html#devops-vs-mlops",
    "href": "posts/mlops/introduction-to-mlops.html#devops-vs-mlops",
    "title": "Introduction to MLOps",
    "section": "DevOps vs MLOps",
    "text": "DevOps vs MLOps\nDevOps principles have gained widespread acceptance in the software development industry due to their ability to integrate and automate software development and IT operations, leading to iterative improvements, fast feedback, collaboration, and automation. MLOps principles, derived from DevOps, aim to bring these best practices to the realm of machine learning and enable faster deployment of ML models into production.\nHowever, there are notable differences in how MLOps operates compared to DevOps:\n\nExperimental nature: MLOps involves extensive experimentation by data scientists and ML/DL engineers. They need to manage data and code to ensure reproducibility while experimenting with different features such as hyperparameters, parameters, and models. Reproducibility remains a challenge in the ML/DL sector, which sets it apart from the more deterministic nature of traditional software development in DevOps.\nHybrid team composition: MLOps teams have a hybrid structure that includes data scientists or ML researchers alongside software engineers. While data scientists focus on experimentation, model development, and exploratory data analysis, they may lack the expertise of software engineers in building production-ready services. This combination of skill sets is essential for successfully deploying ML models in production. `\nTesting: Testing in MLOps goes beyond conventional code tests like unit testing and integration testing. It encompasses model validation, model training, and other specific tasks associated with testing an ML system. The unique challenges of testing ML models require specialized techniques and frameworks to ensure model accuracy and reliability.\nAutomated deployment: Deploying an offline-trained ML model as a prediction service requires a multi-step pipeline in MLOps. Automating the tasks that data scientists manually perform before model deployment adds complexity to the process. It involves automating model retraining, validation, and deployment steps to ensure efficient and seamless deployment of updated models.\nProduction performance degradation and Training-Serving Skew: ML models in production can experience reduced performance due to changing data profiles or suboptimal coding. Unlike traditional software systems, ML models are sensitive to changes in data and require monitoring and adaptation to maintain optimal performance. Training-Serving Skew refers to discrepancies between how data is handled in the training and serving pipelines, which can further impact model performance.\nMonitoring: Monitoring is essential for ML models in production. It involves tracking the performance of deployed models and monitoring the summary statistics of the data used to build the models. Monitoring helps identify deviations from expected values, triggering alerts or initiating a roll-back process when necessary. Since data profiles and statistics can change over time, ongoing monitoring is critical for maintaining model effectiveness."
  },
  {
    "objectID": "posts/mlops/introduction-to-mlops.html#mlops-principles",
    "href": "posts/mlops/introduction-to-mlops.html#mlops-principles",
    "title": "Introduction to MLOps",
    "section": "MLOps Principles",
    "text": "MLOps Principles\nMLOps, which stands for Machine Learning Operations, encompasses a set of principles and practices aimed at streamlining the lifecycle of machine learning projects and promoting collaboration and communication between teams. Here are some key principles of MLOps:\n\nVersion control: Implementing version control systems, such as Git, enables tracking and management of changes to ML models, code, and data. It ensures reproducibility, facilitates collaboration and helps teams work together effectively.\nContinuous Integration and Continuous Deployment (CI/CD): MLOps encourages the use of CI/CD pipelines for automating the build, testing, and deployment of ML models. This iterative approach enables fast feedback, reduces errors, and accelerates the development and deployment process.\nInfrastructure as Code (IaC): MLOps embraces the concept of treating infrastructure as code using tools like Terraform or AWS CloudFormation. By defining infrastructure configurations using code, teams can easily manage and version control their cloud infrastructure. IaC enables reproducibility by providing a consistent and automated way to provision and manage resources in the cloud. It also facilitates scalability, allowing teams to easily scale up or down their infrastructure as needed, ensuring efficient and cost-effective deployments for ML models.\nModel Monitoring: Monitoring ML models in production is vital to detect performance issues, data drift, and anomalies. Dedicated monitoring tools help track model performance, identify deviations, and trigger alerts for timely updates and maintenance.\nCollaboration and Communication: Effective collaboration and communication between data scientists, engineers, and operations teams are critical in MLOps. Sharing knowledge, documenting processes, and fostering a collaborative culture enhance team productivity and ensure the successful delivery of ML projects.\nAutomated Testing: Implementing automated testing frameworks ensures the quality and reliability of ML models. This includes unit tests, integration tests, and performance tests that validate model behavior and catch potential issues early in the development process.\n\n Source: End-to-End Machine Learning Platforms By Ian Hellstr√∂m"
  },
  {
    "objectID": "posts/mlops/introduction-to-mlops.html#mlops-maturity-model",
    "href": "posts/mlops/introduction-to-mlops.html#mlops-maturity-model",
    "title": "Introduction to MLOps",
    "section": "MLOps maturity model",
    "text": "MLOps maturity model\nThe MLOps maturity model represents the level of proficiency and scalability in managing and operationalizing machine learning (ML) systems within an organization. It illustrates how effectively the company can develop, implement, monitor, and maintain ML models. The stages of MLOps maturity may vary depending on the framework or model used, but they generally progress as follows:\n\nLevel 0: No MLOps\n\nManaging the complete lifecycle of ML models is challenging.\nTeams are diverse, and releases are cumbersome.\nLack of transparency and feedback from deployed models.\n\nLevel 1: DevOps but no MLOps\n\nReleases are less cumbersome compared to Level 0 but still rely heavily on the Data Team for each new model.\nLimited feedback on model performance in production.\nDifficulties in tracing and reproducing results.\n\nLevel 2: Automated Training\n\nThe training environment is fully managed and traceable.\nModels can be easily reproduced.\nReleases are performed manually but with reduced friction.\n\nLevel 3: Automated Model Deployment\n\nReleases are automated and have low friction.\nFull traceability from deployment back to the original data.\nThe entire environment is managed, including training, testing, and production stages.\n\nLevel 4: Full MLOps Automated Operations\n\nThe entire system is automated and easily monitored.\nProduction systems provide insights for continuous improvement and can automatically incorporate new models.\nApproaching a zero-downtime system with high availability."
  },
  {
    "objectID": "posts/mlops/introduction-to-mlops.html#bonus-reading-materials",
    "href": "posts/mlops/introduction-to-mlops.html#bonus-reading-materials",
    "title": "Introduction to MLOps",
    "section": "Bonus Reading Materials",
    "text": "Bonus Reading Materials\n\nMLOps Maturity Model - Azure\nMade with ML - By Goku Mohandas\nMLOps Primer\n\nThank you for reading and I hope you found this notebook helpful. Upvote if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "posts/mlops/mlflow-on-aws.html",
    "href": "posts/mlops/mlflow-on-aws.html",
    "title": "MLflow on AWS",
    "section": "",
    "text": "In this blog post we explore how to set up MLflow on AWS, leveraging EC2 to host MLFlow Server, S3 for artifact storage and RDS-PostgreSQL for backend entity storager.\nIf you‚Äôre interested in learning about MLflow or need an introduction to its workings, I recommend checking out my previous blog post titled ‚ÄúIntroduction to MLflow‚Äù."
  },
  {
    "objectID": "posts/mlops/mlflow-on-aws.html#create-aws-ec2-instance",
    "href": "posts/mlops/mlflow-on-aws.html#create-aws-ec2-instance",
    "title": "MLflow on AWS",
    "section": "Create AWS EC2 Instance",
    "text": "Create AWS EC2 Instance\n\nGo to https://aws.amazon.com to Sign in / Create an AWS Account.\nTo launch EC2 instance, click on to services on the left-top corner of the page. Select Compute and EC2.\n\n\n\nTo launch a new instance, click on Launch Instance.\n\n\n\nName our Instance\n\n\n\nKeep the default settings for Application and OS Image, and Instance Type.\nIf you don‚Äôt already have a Key pair, you can create a new key pair. You would be asked to download and save your key pair.\n\n\n\n\n\n\n\nTip\n\n\n\nSave your key pair at ~/.ssh/ folder.\n\n\n\n\n\n\nKeep all the other settings as default and click on Launch Instance."
  },
  {
    "objectID": "posts/mlops/mlflow-on-aws.html#configure-security-group",
    "href": "posts/mlops/mlflow-on-aws.html#configure-security-group",
    "title": "MLflow on AWS",
    "section": "Configure Security Group",
    "text": "Configure Security Group\n\nAfter the instance is launched, click on Security Section on the Instance Summary Page.\n\n\n\nClick on Edit Inbound Rules and add a new rule for Custom TCP with port 5000 and source. Save the changes."
  },
  {
    "objectID": "posts/mlops/mlflow-on-aws.html#create-s3-bucket",
    "href": "posts/mlops/mlflow-on-aws.html#create-s3-bucket",
    "title": "MLflow on AWS",
    "section": "Create S3 Bucket",
    "text": "Create S3 Bucket\n\nGo to services on the left-top corner of the page. Select Storage and S3. Click on Create Bucket.\n\n\n\nName your bucket and select the region. Keep all the other settings as default and click on Create Bucket.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPlease make note of the bucket name for later use."
  },
  {
    "objectID": "posts/mlops/mlflow-on-aws.html#create-rds-database",
    "href": "posts/mlops/mlflow-on-aws.html#create-rds-database",
    "title": "MLflow on AWS",
    "section": "Create RDS Database",
    "text": "Create RDS Database\n\nGo to services on the left-top corner of the page. Select Database and RDS. Click on Create Database.\n\n\n\nChoose Standard create and select PostgreSQL as the engine.\nSelect Free tier in the Templates section.\nIn the Settings section, name your database i.e., DB Instance Identifier (eg. mlflow-database). In the Credentials section, enter a username (eg. mlflow) and Tick the Auto generate a password checkbox.\nIn the Additional configuration section, Set the Initial database name (eg. mlflow_db) under the Database options.\n\n\n\n\n\n\n\nImportant\n\n\n\nPlease make note of the username and the database name for later use.\n\n\n\nKeep all the other settings as default and click on Create Database.\nThe database would take a few minutes to be created. To check the password, click on View credential details.\n\n\n\n\n\n\n\nWarning\n\n\n\nYou would need the password save the password for later use. This is the only time you would be able to view the password. However, you can always reset the password.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAfter the database is created, please save the endpoint and port for later use. You can find the endpoint and port on the database summary page. Marked in the image below with a green dotted box.\n\n\n\nNext, you would need to add an inbound rule to the security group of the database. To do so, click on the Security section on the database summary page. Click on Edit Inbound Rules and add a new rule for PostgreSQL with port 5432 and source. Save the changes. This allows the EC2 instance to connect to the database.\n\n\n\nSelect the security group that was created automatically when we launched the EC2 instance."
  },
  {
    "objectID": "posts/mlops/mlflow-on-aws.html#install-mlflow",
    "href": "posts/mlops/mlflow-on-aws.html#install-mlflow",
    "title": "MLflow on AWS",
    "section": "Install MLflow",
    "text": "Install MLflow\n\nWe‚Äôll utilize the easiest way to connect to the EC2 instance. Click on the Connect button on the EC2 instance summary page. In the EC2 Instance Connect section, click on Connect.\n\n\n\nThis would open a terminal window in the browser.\n\nRun the following commands to install MLflow.\n\nsudo yum update\n\npip3 install mlflow boto3 psycopg2-binary\n\n\n\n\n\n\nNote\n\n\n\nIf you get an error saying No module named pip, run the following command to install pip.\npython3 -m ensurepip --upgrade\n\n\n\nNext, we need to set up the MLflow Tracking Server. To do so, run the following command.\n\nmlflow server -h 0.0.0.0 -p 5000 --backend-store-uri postgresql://DB_USER:DB_PASSWORD@DB_ENDPOINT:PORT/DB_NAME --default-artifact-root s3://S3_BUCKET_NAME\n\n# Example, Replace the following values\nmlflow server -h 0.0.0.0 -p 5000 --backend-store-uri postgresql://mlflow:WJgpP1lv4PQVnhzdq7T5@mlflow-database.c4rrlovvb5cx.us-east-1.rds.amazonaws.com:5432/mlflow_db --default-artifact-root s3://mlflow-artifact-remote-storage\n\n\n\n\n\n\nNote\n\n\n\nWe made note of the ENDPOINT and PORT which can be replaced for DB_ENDPOINT and PORT respectively. We also saved the PASSWORD when creating the RDS-Postgresql database which can be replaced for DB_PASSWORD respectively.\nSimilarly, we made note of the USERNAME and DATABASE_NAME which can be replaced for DB_USER and DB_NAME respectively. We also made note of the BUCKET_NAME which can be replaced for S3_BUCKET_NAME.\n\n\n\nTo checkout the MLflow UI, open a new tab in the browser and enter the following URL.\n\nhttp://EC2_INSTANCE_PUBLIC_IPv4_ADDRESS:5000\n\n# Example, Replace the following values\nhttp://52.91.235.206:5000/\n\nVoila! You have successfully set up MLflow on AWS EC2 instance."
  },
  {
    "objectID": "posts/mlops/mlflow-on-aws.html#setup-aws-cli-and-profile-on-local",
    "href": "posts/mlops/mlflow-on-aws.html#setup-aws-cli-and-profile-on-local",
    "title": "MLflow on AWS",
    "section": "Setup AWS CLI and Profile on local",
    "text": "Setup AWS CLI and Profile on local\n\nInstall AWS CLI on your local machine. You can follow the instructions here.\nGo to IAM on AWS Console and click on Policies on the left panel. Click on Create Policy.\n\n\n\nSearch for S3 and click on s3.\n\n\n\nSelect all the access level for List, and Read. Select all the marked options for Write.\n\n\n\n\nMark all the checkboxes in the Resources section. Click on Next.\n\n\n\nGive the name for the Policy and click on Create Policy.\n\n\n\nGo to IAM and click on UsersGroup on the left panel. Click on Create group. Give a name for the group and select the policy that was created in the previous step. Click on Create group.\n\n\n\nGo to IAM and click on Users on the left panel. Click on Add user. Give a name for the user and click on Next.\n\n\n\n\nSelect Add user to group Permission option and select the group that was created in the previous step. Click on Next. Review the details and click on Create user.\n\n\n\nGo to Users again and click on the user that was created in the previous step. Click on Security credentials tab. Click on Create access key.\n\n\n\nClick on Other and click on Next Button. Then Click on Create Access Key.\nClick on Download .csv file and save the file. This file contains the Access Key ID and Secret Access Key which would be used to configure the AWS CLI.\nOpen the terminal and run the following command.\n\naws configure\n\nEnter the Access Key ID and Secret Access Key that was saved in the previous step. For other options like Default region name and Default output format, keep the default values by pressing enter.\nYou can check if the AWS CLI is configured correctly by running the following command.\n\naws s3 ls\nYou should be able to see the list of buckets that are present in your AWS account."
  },
  {
    "objectID": "posts/mlops/mlflow-on-aws.html#mlflow-in-action",
    "href": "posts/mlops/mlflow-on-aws.html#mlflow-in-action",
    "title": "MLflow on AWS",
    "section": "MLflow in Action",
    "text": "MLflow in Action\nI‚Äôll refer to the Introduction to MLflow post for this section. I‚Äôll be using the same code and data for this section.\n\nSetup\nCreate a new virtual environment and install MLflow and other libraries using the following command:\n\n\nrequirements.txt\n\nmlflow\njupyter\nscikit-learn\npandas\nxgboost\nfastparquet\nhyperopt\noptuna\n\n# Create conda environment\nconda create -p venv python=3.9 -y\n\n# Activate conda environment\nconda activate venv/\n\n# Install required libraries\npip install -r requirements.txt --no-cache-dir\nDownload the dataset using the following command:\n# Create data directory\nmkdir data\n\n# Move to data directory\ncd data\n\n# Download dataset\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-01.parquet # January 2022\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-02.parquet # February 2022\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-03.parquet # March 2022\nWe‚Äôll also create a jupyter notebook named mlflow.ipynb to run our experiment. After following the above steps, you should have the following directory structure:\n.\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îú‚îÄ‚îÄ green_tripdata_2022-01.parquet\n‚îÇ   ‚îú‚îÄ‚îÄ green_tripdata_2022-02.parquet\n‚îÇ   ‚îî‚îÄ‚îÄ green_tripdata_2022-03.parquet\n‚îú‚îÄ‚îÄ mlflow.ipynb\n‚îú‚îÄ‚îÄ requirements.txt\n‚îî‚îÄ‚îÄ venv\nThe below hidden code block imports the required libraries, loads & transforms the dataset, and splits the dataset into train, validation, and test sets.\n\n\nCode\nimport os\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport mlflow\nimport optuna\n\nfrom mlflow.entities import ViewType\nfrom mlflow.tracking import MlflowClient\nfrom optuna.samplers import TPESampler\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\ndef read_dataframe(filename: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Reads a Parquet file into a pandas DataFrame, performs data transformations, and returns the resulting DataFrame.\n\n    Parameters:\n        filename (str): The path to the Parquet file to be read.\n\n    Returns:\n        pandas.DataFrame: The processed DataFrame containing the data from the Parquet file.\n\n    Raises:\n        [Any exceptions raised by pandas.read_parquet()]\n\n    Notes:\n        - The function performs the following transformations on the DataFrame:\n            - Converts 'lpep_dropoff_datetime' and 'lpep_pickup_datetime' columns to pandas datetime objects.\n            - Computes the 'duration' column by subtracting 'lpep_pickup_datetime' from 'lpep_dropoff_datetime'\n              and converting the result to minutes.\n            - Filters the DataFrame to include rows where the 'duration' is between 1 and 60 minutes (inclusive).\n            - Converts 'PULocationID' and 'DOLocationID' columns to string type.\n\n    Example:\n        filename = 'data.parquet'\n        df = read_dataframe(filename)\n    \"\"\"\n    # Read the Parquet file into a DataFrame\n    df = pd.read_parquet(filename)\n\n    # Convert 'lpep_dropoff_datetime' and 'lpep_pickup_datetime' columns to pandas datetime objects\n    df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n    df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)\n\n    # Compute the 'duration' column by subtracting 'lpep_pickup_datetime' from 'lpep_dropoff_datetime' and converting to minutes\n    df['duration'] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n\n    # Filter the DataFrame to include rows where the 'duration' is between 1 and 60 minutes (inclusive)\n    df = df[(df.duration &gt;= 1) & (df.duration &lt;= 60)]\n\n    # Convert 'PULocationID' and 'DOLocationID' columns to string type\n    categorical = ['PULocationID', 'DOLocationID']\n    df[categorical] = df[categorical].astype(str)\n    \n    # Return the processed DataFrame\n    return df\n\n# Read the Parquet file for training data into a DataFrame\ndf_train = read_dataframe('./data/green_tripdata_2022-01.parquet')\n\n# Read the Parquet file for validation data into a DataFrame\ndf_val = read_dataframe('./data/green_tripdata_2022-02.parquet')\n\n# Read the Parquet file for testing data into a DataFrame\ndf_test = read_dataframe('./data/green_tripdata_2022-03.parquet')\n\ndef preprocess(df: pd.DataFrame, dv: DictVectorizer, fit_dv: bool = False):\n    \"\"\"\n    Preprocesses a pandas DataFrame by creating new features, transforming categorical features into a numerical format,\n    and returning the transformed data along with the DictVectorizer.\n\n    Parameters:\n        df (pandas.DataFrame): The input DataFrame to be preprocessed.\n        dv (sklearn.feature_extraction.DictVectorizer): The DictVectorizer instance to be used for transforming categorical features.\n        fit_dv (bool, optional): Indicates whether to fit the DictVectorizer on the data. Defaults to False.\n\n    Returns:\n        tuple: A tuple containing the transformed feature matrix and the DictVectorizer instance.\n\n    Notes:\n        - The function assumes that the DataFrame contains the columns 'PULocationID' and 'DOLocationID'.\n        - The function creates a new feature 'PU_DO' by concatenating 'PULocationID' and 'DOLocationID'.\n        - The categorical feature 'PU_DO' and numerical feature 'trip_distance' are selected for transformation.\n        - The function transforms the selected features into a dictionary representation and applies the DictVectorizer.\n        - If fit_dv is True, the DictVectorizer is fitted on the data. Otherwise, the existing fitted DictVectorizer is used.\n\n    Example:\n        df = read_dataframe('data.parquet')\n        dv = DictVectorizer()\n        X, dv = preprocess(df, dv, fit_dv=True)\n    \"\"\"\n    # Create a new feature 'PU_DO' by concatenating 'PULocationID' and 'DOLocationID'\n    df['PU_DO'] = df['PULocationID'] + '_' + df['DOLocationID']\n\n    # Select categorical and numerical features for transformation\n    categorical = ['PU_DO']\n    numerical = ['trip_distance']\n\n    # Convert the selected features into a dictionary representation\n    dicts = df[categorical + numerical].to_dict(orient='records')\n\n    # Apply DictVectorizer for transforming categorical features\n    if fit_dv:\n        # Fit the DictVectorizer on the data\n        X = dv.fit_transform(dicts)\n    else:\n        # Transform using the existing fitted DictVectorizer\n        X = dv.transform(dicts)\n\n    # Return the transformed feature matrix and DictVectorizer\n    return X, dv\n\n# Extract the target variable\ntarget = 'duration'\n\n# Extract the target variable from the training, validation and testing datasets\ny_train = df_train[target].values\ny_val = df_val[target].values\ny_test = df_test[target].values\n\n# Initialize a DictVectorizer for preprocessing\ndv = DictVectorizer()\n\n# Preprocess the training data\nX_train, dv = preprocess(df_train, dv, fit_dv=True)\n\n# Preprocess the validation data using the fitted DictVectorizer from the training data\nX_val, _ = preprocess(df_val, dv, fit_dv=False)\n\n# Preprocess the testing data using the fitted DictVectorizer from the training data\nX_test, _ = preprocess(df_test, dv, fit_dv=False)\n\ndef dump_pickle(obj, filename: str):\n    \"\"\"\n    Pickles (serializes) an object and saves it to a file.\n\n    Parameters:\n        obj (Any): The object to be pickled.\n        filename (str): The path and filename to save the pickled object.\n\n    Returns:\n        None\n\n    Notes:\n        - The function uses the 'pickle' module to serialize the object and save it to a file.\n        - The file is opened in binary mode for writing using the \"wb\" mode.\n    \"\"\"\n    with open(filename, \"wb\") as f_out:\n        return pickle.dump(obj, f_out)\n\n\nSet up the MLflow Tracking Server URI.\n\nTRACKING_SERVER_HOST = \"52.91.235.206\" # fill in with the public IPv4 of the EC2 instance\nmlflow.set_tracking_uri(f\"http://{TRACKING_SERVER_HOST}:5000\")\n\n\nprint(f\"tracking URI: '{mlflow.get_tracking_uri()}'\")\n\ntracking URI: 'http://52.91.235.206:5000'\n\n\n\n# Create a new experiment\nmlflow.set_experiment(\"nyc-taxi-experiment\")\n\n2023/05/31 11:43:55 INFO mlflow.tracking.fluent: Experiment with name 'nyc-taxi-experiment' does not exist. Creating a new experiment.\n\n\n&lt;Experiment: artifact_location='s3://mlflow-artifact-remote-storage/1', creation_time=1685547835140, experiment_id='1', last_update_time=1685547835140, lifecycle_stage='active', name='nyc-taxi-experiment', tags={}&gt;\n\n\n\n\nTrain Model\nTrain a simple Linear Regression model and log the model parameters, metrics, and artifacts.\n\n# Specify the destination path for saving model files\nmodel_path = \"./outputs/models\"\n\n# Start an MLflow run\nwith mlflow.start_run():\n    # Set a tag for the developer\n    mlflow.set_tag(\"developer\", \"Sagar\")\n\n    # Initialize and train a LinearRegression model\n    lr = LinearRegression()\n    lr.fit(X_train, y_train)\n\n    # Make predictions on the validation data\n    yhat = lr.predict(X_val)\n\n    # Calculate the root mean squared error (RMSE)\n    rmse = mean_squared_error(y_val, yhat, squared=False)\n\n    # Log the RMSE metric to MLflow\n    mlflow.log_metric(\"rmse\", rmse)\n\n    # Create dest_path folder unless it already exists\n    os.makedirs(model_path, exist_ok=True)\n\n    # Save the trained model as a pickle file\n    dump_pickle(lr, os.path.join(model_path, \"lin_reg.pkl\"))\n\n    # Log the trained model as an artifact to MLflow\n    mlflow.log_artifact(local_path=f\"{model_path}/lin_reg.pkl\", artifact_path=\"model\")\n\nBelow you can see the MLflow UI with the experiment run.\n\n\nYou can also see that the artifacts are stored in S3 bucket.\n\n\n\nHyperparameter Tuning\nNext, we‚Äôll create a new experiment to perform hyperparameter tuning using Hyperopt and Optuna.\n\n# Create a new experiment\nmlflow.set_experiment(\"random-forest-hyperopt\")\n\n&lt;Experiment: artifact_location='s3://mlflow-artifact-remote-storage/3', creation_time=1685548270366, experiment_id='3', last_update_time=1685548270366, lifecycle_stage='active', name='random-forest-hyperopt', tags={}&gt;\n\n\n\ndef run_optimization(num_trials: int = 10):\n    \"\"\"\n    Runs the optimization process using Optuna library to find the optimal hyperparameters for RandomForestRegressor.\n\n    Parameters:\n        num_trials (int): The number of optimization trials to perform. Default is 10.\n\n    Returns:\n        None\n\n    Notes:\n        - The function defines an objective function for Optuna to minimize the root mean squared error (RMSE).\n        - The objective function samples hyperparameters, trains a RandomForestRegressor model with those hyperparameters,\n          evaluates the model on the validation data, and logs the RMSE metric to MLflow.\n        - Optuna performs the optimization process by searching for the set of hyperparameters that minimizes the RMSE.\n    \"\"\"\n\n    def objective(trial):\n        \"\"\"\n        Objective function for Optuna optimization.\n\n        Parameters:\n            trial (optuna.Trial): A trial object representing a single optimization trial.\n\n        Returns:\n            float: The value of the objective function (RMSE).\n\n        Notes:\n            - The objective function samples hyperparameters from the defined search space.\n            - It initializes and trains a RandomForestRegressor model with the sampled hyperparameters.\n            - The model is evaluated on the validation data, and the RMSE is calculated.\n            - The RMSE and the sampled hyperparameters are logged to MLflow.\n        \"\"\"\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 10, 50, 1),\n            'max_depth': trial.suggest_int('max_depth', 1, 20, 1),\n            'min_samples_split': trial.suggest_int('min_samples_split', 2, 10, 1),\n            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 4, 1),\n            'random_state': 42,\n            'n_jobs': -1\n        }\n        \n        # Start a new MLflow run for each trial\n        with mlflow.start_run():\n            # Set a tag for the model type\n            mlflow.set_tag(\"model\", \"RandomForestRegressor\")\n            \n            # Log the sampled hyperparameters to MLflow\n            mlflow.log_params(params)\n\n            # Initialize a RandomForestRegressor model with the sampled hyperparameters\n            rf = RandomForestRegressor(**params)\n\n            # Train the model on the training data\n            rf.fit(X_train, y_train)\n\n            # Make predictions on the validation data\n            y_pred = rf.predict(X_val)\n\n            # Calculate the root mean squared error (RMSE)\n            rmse = mean_squared_error(y_val, y_pred, squared=False)\n\n            # Log the RMSE metric to MLflow\n            mlflow.log_metric(\"rmse\", rmse)\n\n        return rmse\n\n    # Use the Tree-structured Parzen Estimator (TPE) sampler for efficient hyperparameter search\n    sampler = TPESampler(seed=42)\n\n    # Create an Optuna study with the defined objective function and search direction\n    study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n\n    # Run the optimization process with the specified number of trials\n    study.optimize(objective, n_trials=num_trials)\n\nrun_optimization()\n\n[I 2023-05-31 11:57:34,678] A new study created in memory with name: no-name-41ada3cf-dabd-4100-a384-dd6a2ee09103\n[I 2023-05-31 11:57:38,080] Trial 0 finished with value: 6.012747224033297 and parameters: {'n_estimators': 25, 'max_depth': 20, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 0 with value: 6.012747224033297.\n[I 2023-05-31 11:57:38,649] Trial 1 finished with value: 6.249433998787504 and parameters: {'n_estimators': 16, 'max_depth': 4, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 0 with value: 6.012747224033297.\n[I 2023-05-31 11:57:42,149] Trial 2 finished with value: 6.039045655830305 and parameters: {'n_estimators': 34, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 0 with value: 6.012747224033297.\n[I 2023-05-31 11:57:43,620] Trial 3 finished with value: 6.179387143797027 and parameters: {'n_estimators': 44, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 0 with value: 6.012747224033297.\n[I 2023-05-31 11:57:45,445] Trial 4 finished with value: 6.075505898039151 and parameters: {'n_estimators': 22, 'max_depth': 11, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 0 with value: 6.012747224033297.\n[I 2023-05-31 11:57:46,134] Trial 5 finished with value: 6.441117537172997 and parameters: {'n_estimators': 35, 'max_depth': 3, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 0 with value: 6.012747224033297.\n[I 2023-05-31 11:57:49,416] Trial 6 finished with value: 6.0285791267371165 and parameters: {'n_estimators': 28, 'max_depth': 16, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 0 with value: 6.012747224033297.\n[I 2023-05-31 11:57:49,958] Trial 7 finished with value: 7.881244954282265 and parameters: {'n_estimators': 34, 'max_depth': 1, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 0 with value: 6.012747224033297.\n[I 2023-05-31 11:57:51,295] Trial 8 finished with value: 6.025608014492215 and parameters: {'n_estimators': 12, 'max_depth': 19, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 0 with value: 6.012747224033297.\n[I 2023-05-31 11:57:51,848] Trial 9 finished with value: 7.071070856187059 and parameters: {'n_estimators': 22, 'max_depth': 2, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 0 with value: 6.012747224033297.\n\n\n\n\n\nModel Registry\nFinally, we‚Äôll take the top 5 models from the hyperparameter tuning experiment and run them on the test dataset. We‚Äôll log the metrics and artifacts for each model. We‚Äôll register the model with the best metrics to the model registry.\n\n# Hyperparameter optimization experiment name\nHPO_EXPERIMENT_NAME = \"random-forest-hyperopt\"\n\n# Best model experiment name\nEXPERIMENT_NAME = \"random-forest-best-models\"\n\n# Create a new experiment for the best models\nmlflow.set_experiment(EXPERIMENT_NAME)\n\n&lt;Experiment: artifact_location='s3://mlflow-artifact-remote-storage/2', creation_time=1685548138856, experiment_id='2', last_update_time=1685548138856, lifecycle_stage='active', name='random-forest-best-models', tags={}&gt;\n\n\n\n# Automatically log parameters and metrics\nmlflow.sklearn.autolog()\n\n\n# RandomForest Parameters\nRF_PARAMS = ['max_depth', 'n_estimators', 'min_samples_split',\n             'min_samples_leaf', 'random_state', 'n_jobs']\n\n\ndef train_and_log_model(params):\n    \"\"\"\n    Trains a RandomForestRegressor model with the given hyperparameters and logs evaluation metrics to MLflow.\n\n    Parameters:\n        params (dict): Dictionary of hyperparameters for RandomForestRegressor.\n\n    Returns:\n        None\n\n    Notes:\n        - The function starts an MLflow run to track the model training and evaluation process.\n        - It converts certain hyperparameters to integers.\n        - A RandomForestRegressor model is initialized with the provided hyperparameters.\n        - The model is trained on the training data.\n        - The trained model is evaluated on the validation and test sets, and the root mean squared error (RMSE) is calculated and logged to MLflow as evaluation metrics.\n    \"\"\"\n\n    with mlflow.start_run():\n        # Convert specific hyperparameters to integers\n        for param in RF_PARAMS:\n            params[param] = int(params[param])\n\n        # Initialize a RandomForestRegressor model with the given hyperparameters\n        rf = RandomForestRegressor(**params)\n\n        # Train the model on the training data\n        rf.fit(X_train, y_train)\n\n        # Evaluate the trained model on the validation set\n        val_rmse = mean_squared_error(y_val, rf.predict(X_val), squared=False)\n\n        # Log the validation RMSE metric to MLflow\n        mlflow.log_metric(\"val_rmse\", val_rmse)\n\n        # Evaluate the trained model on the test set\n        test_rmse = mean_squared_error(y_test, rf.predict(X_test), squared=False)\n\n        # Log the test RMSE metric to MLflow\n        mlflow.log_metric(\"test_rmse\", test_rmse)\n\ndef run_register_model(top_n: int):\n    \"\"\"\n    Runs the process to register the best model based on the top_n model runs with the lowest test RMSE.\n\n    Parameters:\n        top_n (int): The number of top model runs to consider.\n\n    Returns:\n        None\n\n    Notes:\n        - The function interacts with the MLflow tracking server to retrieve and register models.\n        - It retrieves the top_n model runs based on the lowest validation RMSE.\n        - For each run, it trains a model using the hyperparameters from the run and logs evaluation metrics to MLflow.\n        - After evaluating the models, it selects the one with the lowest test RMSE.\n        - The selected model is registered with a specified name in MLflow.\n    \"\"\"\n\n    # Connect to the MLflow tracking server\n    client = MlflowClient()\n\n    # Retrieve the top_n model runs and log the models\n    experiment = client.get_experiment_by_name(HPO_EXPERIMENT_NAME)\n\n    # Retrieve the top_n model runs based on the lowest validation RMSE\n    runs = client.search_runs(\n        experiment_ids=experiment.experiment_id,\n        run_view_type=ViewType.ACTIVE_ONLY,\n        max_results=top_n,\n        order_by=[\"metrics.rmse ASC\"]\n    )\n\n    # Train and log the model for each run\n    for run in runs:\n        # Train and log the model based on the hyperparameters from the run\n        train_and_log_model(params=run.data.params)\n\n    # Select the model with the lowest test RMSE\n    experiment = client.get_experiment_by_name(EXPERIMENT_NAME)\n\n    # Retrieve model runs based on the lowest test RMSE, and select the first run (with the lowest test RMSE)\n    best_run = client.search_runs(\n        experiment_ids=experiment.experiment_id,\n        run_view_type=ViewType.ACTIVE_ONLY,\n        order_by=[\"metrics.test_rmse ASC\"]\n    )[0]\n\n    # Register the best model\n    model_uri = f\"runs:/{best_run.info.run_id}/model\"\n\n    # Register the best model with a specified name\n    mlflow.register_model(\n        model_uri=model_uri,\n        name=\"random-forest-best-model\"\n    )\n\n\n# The number of top model runs to consider\ntop_n = 5\n\nrun_register_model(top_n=top_n)\n\n2023/05/31 11:58:21 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/wizard/Astronaut/Dev/MLOps/week2/venv/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\nSuccessfully registered model 'random-forest-best-model'.\n2023/05/31 11:58:49 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: random-forest-best-model, version 1\nCreated version '1' of model 'random-forest-best-model'.\n\n\n\nCongratulations! üéâ You have successfully set up MLflow on AWS EC2 instance and used it to track your machine learning experiments.\n\n\n\n\n\n\nSTOP EC2 instance\n\n\n\nPlease remember to stop the EC2 instance after completing your work to avoid incurring any additional charges.\n\n\nThank you for reading and I hope you found this post helpful. Upvote if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "posts/kaggle/credit-card-fraud-detection-eda.html",
    "href": "posts/kaggle/credit-card-fraud-detection-eda.html",
    "title": "EDA - Playground Series Season 3, Episode 4",
    "section": "",
    "text": "Code\n# Import libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.options.display.max_columns = None\n%matplotlib inline"
  },
  {
    "objectID": "posts/kaggle/credit-card-fraud-detection-eda.html#data-description",
    "href": "posts/kaggle/credit-card-fraud-detection-eda.html#data-description",
    "title": "EDA - Playground Series Season 3, Episode 4",
    "section": "1. Data Description",
    "text": "1. Data Description\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Credit Card Fraud Detection. Feature distributions are close to, but not exactly the same, as the original.\nWe are given three files:\n\ntrain.csv - the training dataset; Class is the target\ntest.csv - the test dataset; our objective is to predict Class\nsample_submission.csv - a sample submission file in the correct format\n\nDescription of each column:\n\nFeature Description\n\n\n\n\n\n\nFeature\nDescription\n\n\n\n\nid\nIndentifier for unique rows\n\n\nTime\nNumber of seconds elapsed between this transaction and the first transaction in the dataset\n\n\nV1-V28\nFeatures generated from the original dataset\n\n\nAmount\nTransaction amount\n\n\nClass\nTarget Feature: 1 for fraudulent transactions, 0 otherwise"
  },
  {
    "objectID": "posts/kaggle/credit-card-fraud-detection-eda.html#overview-and-structure",
    "href": "posts/kaggle/credit-card-fraud-detection-eda.html#overview-and-structure",
    "title": "EDA - Playground Series Season 3, Episode 4",
    "section": "2. Overview and Structure",
    "text": "2. Overview and Structure\n\n__dirname = '../input/playground-series-s3e4/'\n\ntrain = pd.read_csv(__dirname + 'train.csv')\ntest = pd.read_csv(__dirname + 'test.csv')\n\n\n# Display top 5 rows of train set\ntrain.head()\n\n\n\n\n\n\n\n\nid\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\nV10\nV11\nV12\nV13\nV14\nV15\nV16\nV17\nV18\nV19\nV20\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\n0\n0\n0.0\n2.074329\n-0.129425\n-1.137418\n0.412846\n-0.192638\n-1.210144\n0.110697\n-0.263477\n0.742144\n0.108782\n-1.070243\n-0.234910\n-1.099360\n0.502467\n0.169318\n0.065688\n-0.306957\n-0.323800\n0.103348\n-0.292969\n-0.334701\n-0.887840\n0.336701\n-0.110835\n-0.291459\n0.207733\n-0.076576\n-0.059577\n1.98\n0\n\n\n1\n1\n0.0\n1.998827\n-1.250891\n-0.520969\n-0.894539\n-1.122528\n-0.270866\n-1.029289\n0.050198\n-0.109948\n0.908773\n0.836798\n-0.056580\n-0.120990\n-0.144028\n-0.039582\n1.653057\n-0.253599\n-0.814354\n0.716784\n0.065717\n0.054848\n-0.038367\n0.133518\n-0.461928\n-0.465491\n-0.464655\n-0.009413\n-0.038238\n84.00\n0\n\n\n2\n2\n0.0\n0.091535\n1.004517\n-0.223445\n-0.435249\n0.667548\n-0.988351\n0.948146\n-0.084789\n-0.042027\n-0.818383\n-0.376512\n-0.226546\n-0.552869\n-0.886466\n-0.180890\n0.230286\n0.590579\n-0.321590\n-0.433959\n-0.021375\n-0.326725\n-0.803736\n0.154495\n0.951233\n-0.506919\n0.085046\n0.224458\n0.087356\n2.69\n0\n\n\n3\n3\n0.0\n1.979649\n-0.184949\n-1.064206\n0.120125\n-0.215238\n-0.648829\n-0.087826\n-0.035367\n0.885838\n-0.007527\n0.637441\n0.676960\n-1.504823\n0.554039\n-0.824356\n-0.527267\n-0.095838\n-0.312519\n0.642659\n-0.340089\n-0.095514\n-0.079792\n0.167701\n-0.042939\n0.000799\n-0.096148\n-0.057780\n-0.073839\n1.00\n0\n\n\n4\n4\n0.0\n1.025898\n-0.171827\n1.203717\n1.243900\n-0.636572\n1.099074\n-0.938651\n0.569239\n0.692665\n-0.097495\n1.338869\n1.391399\n-0.128167\n-0.081836\n0.100548\n-0.338937\n0.090864\n-0.423645\n-0.731939\n-0.203628\n0.099157\n0.608908\n0.027901\n-0.262813\n0.257834\n-0.252829\n0.108338\n0.021051\n1.00\n0\n\n\n\n\n\n\n\n\n2.1. Data Structure and Statistics\n\nprint(f\"Training set has {train.shape[0]} rows and {train.shape[1]} columns\")\nprint(f\"Testing set has {test.shape[0]} rows and {test.shape[1]} columns\")\n\nTraining set has 219129 rows and 32 columns\nTesting set has 146087 rows and 31 columns\n\n\n\ntrain.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 219129 entries, 0 to 219128\nData columns (total 32 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   id      219129 non-null  int64  \n 1   Time    219129 non-null  float64\n 2   V1      219129 non-null  float64\n 3   V2      219129 non-null  float64\n 4   V3      219129 non-null  float64\n 5   V4      219129 non-null  float64\n 6   V5      219129 non-null  float64\n 7   V6      219129 non-null  float64\n 8   V7      219129 non-null  float64\n 9   V8      219129 non-null  float64\n 10  V9      219129 non-null  float64\n 11  V10     219129 non-null  float64\n 12  V11     219129 non-null  float64\n 13  V12     219129 non-null  float64\n 14  V13     219129 non-null  float64\n 15  V14     219129 non-null  float64\n 16  V15     219129 non-null  float64\n 17  V16     219129 non-null  float64\n 18  V17     219129 non-null  float64\n 19  V18     219129 non-null  float64\n 20  V19     219129 non-null  float64\n 21  V20     219129 non-null  float64\n 22  V21     219129 non-null  float64\n 23  V22     219129 non-null  float64\n 24  V23     219129 non-null  float64\n 25  V24     219129 non-null  float64\n 26  V25     219129 non-null  float64\n 27  V26     219129 non-null  float64\n 28  V27     219129 non-null  float64\n 29  V28     219129 non-null  float64\n 30  Amount  219129 non-null  float64\n 31  Class   219129 non-null  int64  \ndtypes: float64(30), int64(2)\nmemory usage: 53.5 MB\n\n\n\ntest.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 146087 entries, 0 to 146086\nData columns (total 31 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   id      146087 non-null  int64  \n 1   Time    146087 non-null  float64\n 2   V1      146087 non-null  float64\n 3   V2      146087 non-null  float64\n 4   V3      146087 non-null  float64\n 5   V4      146087 non-null  float64\n 6   V5      146087 non-null  float64\n 7   V6      146087 non-null  float64\n 8   V7      146087 non-null  float64\n 9   V8      146087 non-null  float64\n 10  V9      146087 non-null  float64\n 11  V10     146087 non-null  float64\n 12  V11     146087 non-null  float64\n 13  V12     146087 non-null  float64\n 14  V13     146087 non-null  float64\n 15  V14     146087 non-null  float64\n 16  V15     146087 non-null  float64\n 17  V16     146087 non-null  float64\n 18  V17     146087 non-null  float64\n 19  V18     146087 non-null  float64\n 20  V19     146087 non-null  float64\n 21  V20     146087 non-null  float64\n 22  V21     146087 non-null  float64\n 23  V22     146087 non-null  float64\n 24  V23     146087 non-null  float64\n 25  V24     146087 non-null  float64\n 26  V25     146087 non-null  float64\n 27  V26     146087 non-null  float64\n 28  V27     146087 non-null  float64\n 29  V28     146087 non-null  float64\n 30  Amount  146087 non-null  float64\ndtypes: float64(30), int64(1)\nmemory usage: 34.6 MB\n\n\nWe find:\n\nAll the columns in the both train and test set are either int64 or float64.\nBased on the value for Non-Null we can observe we don‚Äôt have any missing values in our datasets.\nAs categorical type features are not present, it reduces some pain points while data preprocessing.\n\n\ntrain.describe()\n\n\n\n\n\n\n\n\nid\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\nV10\nV11\nV12\nV13\nV14\nV15\nV16\nV17\nV18\nV19\nV20\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\ncount\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n219129.000000\n\n\nmean\n109564.000000\n62377.415376\n0.096008\n0.048345\n0.592102\n0.069273\n-0.161555\n0.133688\n-0.128224\n0.149534\n-0.048337\n-0.039758\n0.153632\n-0.061038\n0.014330\n0.067649\n0.108643\n0.013650\n0.036815\n-0.033927\n-0.008302\n0.009708\n-0.031064\n-0.050852\n-0.050531\n-0.002992\n0.124005\n0.009881\n0.014034\n0.017313\n66.359803\n0.002140\n\n\nstd\n63257.237906\n25620.348569\n1.395425\n1.159805\n1.132884\n1.253125\n1.069530\n1.202411\n0.817207\n0.716212\n1.054143\n0.821889\n0.976946\n0.998470\n1.039145\n0.801335\n0.891613\n0.786654\n0.691709\n0.784454\n0.739928\n0.439521\n0.422777\n0.597812\n0.318175\n0.593100\n0.406741\n0.473867\n0.233355\n0.164859\n150.795017\n0.046214\n\n\nmin\n0.000000\n0.000000\n-29.807725\n-44.247914\n-19.722872\n-5.263650\n-37.591259\n-25.659750\n-31.179799\n-28.903442\n-8.756951\n-22.092656\n-4.190145\n-16.180165\n-4.373778\n-15.585021\n-4.155728\n-11.778839\n-20.756768\n-7.456060\n-4.281628\n-18.679066\n-14.689621\n-8.748979\n-11.958588\n-2.836285\n-3.958591\n-1.858672\n-9.234767\n-4.551680\n0.000000\n0.000000\n\n\n25%\n54782.000000\n47933.000000\n-0.846135\n-0.573728\n-0.027154\n-0.769256\n-0.847346\n-0.631835\n-0.646730\n-0.095948\n-0.711444\n-0.499563\n-0.576969\n-0.476890\n-0.671601\n-0.329905\n-0.461596\n-0.461077\n-0.406675\n-0.496990\n-0.463035\n-0.167927\n-0.190418\n-0.473099\n-0.174478\n-0.332540\n-0.126080\n-0.318330\n-0.050983\n-0.009512\n5.990000\n0.000000\n\n\n50%\n109564.000000\n63189.000000\n0.385913\n0.046937\n0.735895\n0.064856\n-0.229929\n-0.087778\n-0.098970\n0.111219\n-0.131323\n-0.106034\n0.090545\n0.087649\n-0.016837\n0.049266\n0.178975\n0.054550\n-0.013949\n-0.039451\n-0.002935\n-0.037702\n-0.042858\n-0.032856\n-0.063307\n0.038708\n0.145934\n-0.086388\n0.015905\n0.022163\n21.900000\n0.000000\n\n\n75%\n164346.000000\n77519.000000\n1.190661\n0.814145\n1.306110\n0.919353\n0.356856\n0.482388\n0.385567\n0.390976\n0.583715\n0.403967\n0.917392\n0.608480\n0.695547\n0.460837\n0.791255\n0.531777\n0.410978\n0.446448\n0.455718\n0.126750\n0.109187\n0.354910\n0.060221\n0.394566\n0.402926\n0.253869\n0.076814\n0.066987\n68.930000\n0.000000\n\n\nmax\n219128.000000\n120580.000000\n2.430494\n16.068473\n6.145578\n12.547997\n34.581260\n16.233967\n39.824099\n18.270586\n13.423914\n15.878405\n9.417789\n5.406614\n5.976265\n6.078453\n4.693323\n5.834992\n8.845303\n4.847887\n4.090974\n15.407839\n22.062945\n6.163541\n12.734391\n4.572739\n3.111624\n3.402344\n13.123618\n23.263746\n7475.000000\n1.000000\n\n\n\n\n\n\n\n\ntest.describe()\n\n\n\n\n\n\n\n\nid\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\nV10\nV11\nV12\nV13\nV14\nV15\nV16\nV17\nV18\nV19\nV20\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\n\n\n\n\ncount\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n146087.000000\n\n\nmean\n292172.000000\n144637.928166\n0.512929\n-0.013098\n-0.697478\n-0.273258\n0.321856\n-0.050204\n0.073419\n0.043803\n-0.071620\n0.013962\n-0.249980\n0.108453\n-0.127648\n-0.151857\n-0.199467\n0.027958\n-0.052971\n0.128470\n-0.008261\n-0.056678\n0.044729\n0.175461\n0.018471\n0.016029\n-0.118352\n-0.015199\n0.006236\n0.002035\n66.182463\n\n\nstd\n42171.828725\n14258.025396\n1.628455\n1.247749\n1.292522\n1.365752\n1.146215\n1.332880\n0.946681\n0.749513\n0.924996\n0.932453\n0.881086\n0.686018\n0.916833\n0.832182\n0.774293\n0.811819\n0.713056\n0.798006\n0.722225\n0.458364\n0.449017\n0.710704\n0.359597\n0.633929\n0.479720\n0.446154\n0.255935\n0.174613\n153.151535\n\n\nmin\n219129.000000\n120580.000000\n-34.755944\n-37.803827\n-18.934952\n-5.497560\n-25.639591\n-14.133040\n-18.715915\n-26.926164\n-4.823352\n-12.333747\n-4.333619\n-8.836286\n-4.131766\n-14.172557\n-4.072435\n-7.639810\n-11.868164\n-4.342548\n-4.823554\n-26.412867\n-13.087263\n-5.392649\n-12.814296\n-2.789084\n-3.361564\n-1.743223\n-9.412538\n-8.262339\n0.000000\n\n\n25%\n255650.500000\n132698.000000\n-0.679988\n-0.715885\n-1.619268\n-1.021205\n-0.418547\n-0.891441\n-0.570042\n-0.231824\n-0.634695\n-0.636147\n-0.957520\n-0.324639\n-0.717325\n-0.617641\n-0.737472\n-0.451743\n-0.572054\n-0.380886\n-0.383668\n-0.237529\n-0.166715\n-0.393667\n-0.135059\n-0.368957\n-0.409938\n-0.284914\n-0.066037\n-0.057447\n5.990000\n\n\n50%\n292172.000000\n144493.000000\n0.285798\n0.009058\n-0.719060\n-0.482945\n0.306851\n-0.372813\n0.118545\n0.014979\n-0.075909\n-0.065457\n-0.182940\n0.124780\n-0.098396\n-0.065481\n-0.167537\n0.076469\n-0.123968\n0.123625\n-0.002966\n-0.096729\n0.058393\n0.250169\n0.017835\n0.029727\n-0.142325\n-0.069342\n-0.003539\n-0.026955\n21.790000\n\n\n75%\n328693.500000\n156140.000000\n1.974015\n0.827420\n0.073874\n0.369725\n0.955997\n0.302724\n0.734503\n0.296969\n0.513770\n0.564146\n0.453913\n0.581384\n0.504763\n0.395024\n0.305768\n0.546893\n0.372321\n0.677771\n0.374562\n0.065753\n0.244817\n0.749555\n0.167514\n0.562138\n0.182937\n0.216632\n0.069334\n0.066954\n66.000000\n\n\nmax\n365215.000000\n172790.000000\n2.452901\n12.390128\n4.492640\n11.232928\n24.352818\n16.596635\n27.023955\n12.098322\n7.888980\n14.735004\n6.204939\n5.107089\n3.928334\n7.869385\n5.374923\n5.570906\n7.136535\n3.758750\n4.929496\n15.829261\n15.333546\n5.771245\n17.481609\n4.541724\n4.555960\n3.374748\n12.673968\n13.093229\n4630.600000\n\n\n\n\n\n\n\nWe find:\n\nThe feature names don‚Äôt help us understand what the feature is about or how to interpret it.\nFor most of the features the minimum and maximum values are to different extremes whereas their 1st quatile, median, and 3rd quatile are close to each other.\n\nFor example, V1 has minimum value as -34.755944 and maximum value as 2.452901.\n25%: -0.679988, 50%: 0.285798, and 75%: 1.974015\n\n\n\n\n2.2. Missing values\n\n# Check for missing values in train set\nprint(f\"Number of missing values in training set: {sum(train.isna().sum())}\")\nprint(f\"Number of missing values in testing set: {sum(test.isna().sum())}\")\n\nNumber of missing values in training set: 0\nNumber of missing values in testing set: 0\n\n\nWe find:\n\nAs seen in the info() results we confirm we don‚Äôt have any missing values in the both datasets."
  },
  {
    "objectID": "posts/kaggle/credit-card-fraud-detection-eda.html#visualizations",
    "href": "posts/kaggle/credit-card-fraud-detection-eda.html#visualizations",
    "title": "EDA - Playground Series Season 3, Episode 4",
    "section": "3. Visualizations",
    "text": "3. Visualizations\n\n# Extract the columns which contains 'V'\nv_columns = [col for col in train.columns if col[0] == 'V']\nuseful_columns = [col for col in train.columns if col not in ['id', 'Time']]\n\n\n3.1. Univariate Analysis\n\nsns.countplot(x='Class', data=train)\nplt.title('Distribution of Class', fontsize=14)\nplt.show()\n\n\n\n\n\ntrain.Class.value_counts() / train.shape[0] * 100\n\n0    99.785971\n1     0.214029\nName: Class, dtype: float64\n\n\nWe find:\n\nThe two classes are highly imbalanced.\n~99.78% of the data is labelled as Class 0\n~0.21% of the data is labelled as Class 1.\n\n\nax = train[v_columns].plot(kind='box', figsize=(20, 8))\nplt.title('Box plots of features start\\'s with V', fontsize=14)\nplt.show()\n\n\n\n\nWe find:\n\nSimilar, to what we saw in the result from the describe() method, most of the data is centered around 0.\nAlso, there are a lot of data points that appear to be outliers.\nFew of the columns, for example, V1 appear to be skewed.\n\n\n# Explore distribution of each feature for train and test sets.\n\nfig, axes = plt.subplots(nrows=29, ncols=3, figsize=(20, 5*29))\n\nfor row, col in enumerate(v_columns + ['Amount']):\n    sns.kdeplot(data=train, x=col, ax=axes[row, 0], fill=True)\n    sns.kdeplot(data=test, x=col, ax=axes[row, 0], fill=True)\n    axes[row, 0].set_title('Density plot: ' + col, fontsize=14)\n    axes[row, 0].legend()\n    axes[row, 0].set_xlabel('')\n\n    sns.boxplot(data=train, x=col, ax=axes[row, 1], orient='h')\n    axes[row, 1].set_title('Box plot (Train): ' + col, fontsize=14)\n    axes[row, 1].set_xlabel('')\n\n    sns.boxplot(data=test, x=col, ax=axes[row, 2], orient='h')\n    axes[row, 2].set_title('Box plot (Test): ' + col, fontsize=14)\n    axes[row, 2].set_xlabel('')\n\nfig.tight_layout()\nplt.show()\n\n\n\n\nWe find:\n\nThe distribution of train and test data are quite similar.\nThis helps to the model to better perform on the unseen data.\n\n\n\n3.2. Bivariate Analysis\n\nCreating a scatter plot for all combinations of features would be explode and be overwhelming.\nI would encourage you to play with different combinations and look at the data distribution.\nUsing pairplot is also an good option but the graphs are too tiny to observe. You can definititely try using it and then explore any plot you find interesting.\n\nBelow are some plot I found interesting.\n\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(18, 12))\n\nsns.scatterplot(data=train, x='V24', y='V2', hue=train.Class, ax=axes[0, 0])\naxes[0, 0].set_title('V24 vs V2', fontsize=14)\n\nsns.scatterplot(data=train, x='V10', y='V9', hue=train.Class, ax=axes[0, 1])\naxes[0, 1].set_title('V10 vs V9', fontsize=14)\n\nsns.scatterplot(data=train, x='V17', y='V2', hue=train.Class, ax=axes[0, 2])\naxes[0, 2].set_title('V17 vs V2', fontsize=14)\n\nsns.scatterplot(data=train, x='V20', y='V21', hue=train.Class, ax=axes[1, 0])\naxes[1, 0].set_title('V20 vs V21', fontsize=14)\n\nsns.scatterplot(data=train, x='V21', y='V22', hue=train.Class, ax=axes[1, 1])\naxes[1, 1].set_title('V21 vs V22', fontsize=14)\n\nsns.scatterplot(data=train, x='V21', y='V2', hue=train.Class, ax=axes[1, 2])\naxes[1, 2].set_title('V21 vs V2', fontsize=14)\n\nsns.scatterplot(data=train, x='V24', y='V1', hue=train.Class, ax=axes[2, 0])\naxes[2, 0].set_title('V24 vs V1', fontsize=14)\n\nsns.scatterplot(data=train, x='V15', y='V19', hue=train.Class, ax=axes[2, 1])\naxes[2, 1].set_title('V15 vs V19', fontsize=14)\n\nsns.scatterplot(data=train, x='V20', y='Amount',\n                hue=train.Class, ax=axes[2, 2])\naxes[2, 2].set_title('V20 vs Amount', fontsize=14)\n\nfig.tight_layout()\nplt.show()\n\n\n\n\nWe find:\n\nFeatures such as V9 and V10, V21 and V22 show some degree of correlation.\nWhereas others are distributed in ramdom fashion.\nV15 and V19 are totall random and doesn‚Äôt show any patterns that can be useful for us.\nThere are small clusters, example in figure V24 vs V1 where we don‚Äôt find any samples of data with Class 1.\n\n\ntrain.V20/(train.Amount + 1e-6)\n\n0        -0.147964\n1         0.000782\n2        -0.007946\n3        -0.340089\n4        -0.203627\n            ...   \n219124   -0.000985\n219125   -0.004856\n219126   -0.214989\n219127   -0.002170\n219128    0.015379\nLength: 219129, dtype: float64\n\n\n\n# Look at heatmap for correlation between different numeric features\n\ncorr = train[v_columns + ['Amount']].corr()\nfig = plt.figure(figsize=(25, 12))\nsns.heatmap(corr, annot=True, fmt=\".2f\")\nplt.show()\n\n\n\n\nWe find:\n\nV20 and Amount has the highest value of positive correlation, followed by V5 and V6, followed by V21 and V22\nV2 and Amount has the highest value of negative correlation, followed by V5 and Amount, followed by V12 and V14"
  },
  {
    "objectID": "posts/kaggle/credit-card-fraud-detection-eda.html#conclusion",
    "href": "posts/kaggle/credit-card-fraud-detection-eda.html#conclusion",
    "title": "EDA - Playground Series Season 3, Episode 4",
    "section": "4. Conclusion",
    "text": "4. Conclusion\nThank you for reading and I hope you found this notebook helpful. You can find my modeling notebook here\nAny feedback is welcomed, I aim to learn and improve my skillset in my kaggle journey.\nUpvote if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "posts/kaggle/song-popularity-eda.html",
    "href": "posts/kaggle/song-popularity-eda.html",
    "title": "Song Popularity EDA",
    "section": "",
    "text": "This Python notebook is the Python version of Song Popularity EDA - Live Coding Fun by Martin Henze\nPurpose of this notebook is to recreate the plots in python for learning purpose.\nThe recording of the live-coding session can be found on Abhishek Thakur‚Äôs YouTube channel:"
  },
  {
    "objectID": "posts/kaggle/song-popularity-eda.html#introduction",
    "href": "posts/kaggle/song-popularity-eda.html#introduction",
    "title": "Song Popularity EDA",
    "section": "1. Introduction",
    "text": "1. Introduction\nThe competition is about Song Prediction based on a set of different features. The dataset contains the basic file such as train.csv, test.csv and submission_sample.csv. The dataset used in this competition is in tabular format. The evaluation metric used for this competition is AUC score."
  },
  {
    "objectID": "posts/kaggle/song-popularity-eda.html#preparation",
    "href": "posts/kaggle/song-popularity-eda.html#preparation",
    "title": "Song Popularity EDA",
    "section": "2. Preparation",
    "text": "2. Preparation\nInitially we‚Äôll load different libraries used in our analysis. Also, load the train and test data.\n\n\nCode\n# Import libraries and load the data\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport random\nimport warnings\n\nfrom plotnine import *\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")\n\n\n\n\nCode\n# Load the data\ntrain = pd.read_csv(\"/kaggle/input/song-popularity-prediction/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/song-popularity-prediction/test.csv\")"
  },
  {
    "objectID": "posts/kaggle/song-popularity-eda.html#overview-structure-and-data-content",
    "href": "posts/kaggle/song-popularity-eda.html#overview-structure-and-data-content",
    "title": "Song Popularity EDA",
    "section": "3. Overview: structure and data content",
    "text": "3. Overview: structure and data content\nThe first step we‚Äôll do is look at the raw data. This tell us about the different features in the dataset, missing values, and types of features (numeric, string, categorical, etc.).\n\n3.1. Look at the data\nLet‚Äôs look at the basic structure of the data\n\n\nCode\nprint('\\nInformation about Data')\ndisplay(train.info())\n\n\n\nInformation about Data\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 40000 entries, 0 to 39999\nData columns (total 15 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   id                40000 non-null  int64  \n 1   song_duration_ms  35899 non-null  float64\n 2   acousticness      36008 non-null  float64\n 3   danceability      35974 non-null  float64\n 4   energy            36025 non-null  float64\n 5   instrumentalness  36015 non-null  float64\n 6   key               35935 non-null  float64\n 7   liveness          35914 non-null  float64\n 8   loudness          36043 non-null  float64\n 9   audio_mode        40000 non-null  int64  \n 10  speechiness       40000 non-null  float64\n 11  tempo             40000 non-null  float64\n 12  time_signature    40000 non-null  int64  \n 13  audio_valence     40000 non-null  float64\n 14  song_popularity   40000 non-null  int64  \ndtypes: float64(11), int64(4)\nmemory usage: 4.6 MB\n\n\nNone\n\n\nWe find:\n\nThere are 40000 entries and 15 features in total.\nAll the column data type is either int or float i.e.¬†all the columns are numeric. This make is comparatively easier to work with compared to columns contains string type data.\nWe can also observe there are columns that contain less than 40K Non-Null values which indicates missing values in the dataset.\n\nLet‚Äôs now look at the top 20 rows of the data.\n\n\nCode\n\"\"\"Display top 20 rows of the train data\"\"\"\ndisplay(train.head(20).style.set_caption(\"First Twenty rows of Training Data\"))\n\n\n\n\n\nFirst Twenty rows of Training Data\n\n\n¬†\nid\nsong_duration_ms\nacousticness\ndanceability\nenergy\ninstrumentalness\nkey\nliveness\nloudness\naudio_mode\nspeechiness\ntempo\ntime_signature\naudio_valence\nsong_popularity\n\n\n\n\n0\n0\n212990.000000\n0.642286\n0.856520\n0.707073\n0.002001\n10.000000\nnan\n-5.619088\n0\n0.082570\n158.386236\n4\n0.734642\n0\n\n\n1\n1\nnan\n0.054866\n0.733289\n0.835545\n0.000996\n8.000000\n0.436428\n-5.236965\n1\n0.127358\n102.752988\n3\n0.711531\n1\n\n\n2\n2\n193213.000000\nnan\n0.188387\n0.783524\n-0.002694\n5.000000\n0.170499\n-4.951759\n0\n0.052282\n178.685791\n3\n0.425536\n0\n\n\n3\n3\n249893.000000\n0.488660\n0.585234\n0.552685\n0.000608\n0.000000\n0.094805\n-7.893694\n0\n0.035618\n128.715630\n3\n0.453597\n0\n\n\n4\n4\n165969.000000\n0.493017\nnan\n0.740982\n0.002033\n10.000000\n0.094891\n-2.684095\n0\n0.050746\n121.928157\n4\n0.741311\n0\n\n\n5\n5\n188891.000000\n0.035655\n0.825919\n0.804528\n-0.000005\n4.000000\n0.120758\n-6.122926\n0\n0.039012\n115.679128\n4\n0.709408\n0\n\n\n6\n6\n161061.000000\n0.081743\n0.673588\n0.880181\n0.000327\n0.000000\n0.535411\n-2.909607\n1\n0.030902\n98.046205\n4\n0.982729\n0\n\n\n7\n7\n196202.000000\n0.259747\n0.813214\n0.554385\n0.000390\n8.000000\n0.276580\n-7.794237\n0\n0.207067\n158.626764\n3\n0.662987\n1\n\n\n8\n8\n169660.000000\nnan\n0.653263\n0.917034\n0.001748\n0.000000\nnan\n-4.422089\n0\n0.031608\n122.382398\n3\n0.297683\n1\n\n\n9\n9\n167245.000000\n0.019617\n0.595235\n0.820039\n0.761884\n5.000000\n0.181098\n-5.154293\n0\n0.054493\n110.524824\n4\n0.535453\n0\n\n\n10\n10\n128274.000000\n0.614007\n0.397899\n0.346820\n0.002853\n3.000000\n0.132549\nnan\n1\n0.059512\n87.363516\n3\n0.671581\n1\n\n\n11\n11\n213121.000000\n0.044053\n0.817874\n0.729679\n0.003660\n5.000000\n0.137938\n-4.880149\n0\n0.038814\n124.199541\n4\n0.816472\n1\n\n\n12\n12\n219730.000000\n0.339275\n0.660707\nnan\nnan\n0.000000\n0.223173\n-12.005655\n0\n0.089726\n164.877811\n3\n0.322253\n1\n\n\n13\n13\nnan\n0.455778\n0.448538\n0.754924\nnan\nnan\n0.076379\n-3.158905\n0\n0.034837\n118.664526\n3\n0.862989\n0\n\n\n14\n14\nnan\n0.462876\n0.384318\n0.653525\n0.781326\n6.000000\nnan\n-10.362441\n0\n0.065149\n141.581118\n3\n0.432883\n0\n\n\n15\n15\nnan\n0.059284\n0.164167\n0.877743\n0.002113\n8.000000\n0.227997\n-3.627678\n0\n0.246330\n174.445180\n4\n0.530006\n1\n\n\n16\n16\n248851.000000\n0.097600\n0.718901\n0.618376\n0.002925\n4.000000\n0.075377\n-7.715512\n1\n0.083494\n96.831665\n4\n0.935569\n0\n\n\n17\n17\n153340.000000\n0.012866\n0.715635\n0.796742\n0.002236\n6.000000\n0.101808\n-4.879090\n0\n0.172036\n120.830046\n3\n0.497743\n0\n\n\n18\n18\n170983.000000\n0.123631\n0.524386\n0.566983\nnan\n5.000000\nnan\n-7.312097\n0\n0.210055\n114.609197\n4\n0.951705\n1\n\n\n19\n19\n266726.000000\n0.021030\n0.323277\nnan\n-0.000861\nnan\n0.239698\n-12.692935\n1\n0.031522\n124.811208\n4\n0.350090\n1\n\n\n\n\n\nWe find:\n\nThere are missing values that can be seen as nan in the table above\nThe id column seems to have values in increasing order\nThe values in the features are in different scales\n\nNow, let‚Äôs look at some basic statistics about our features in the data\n\n\nCode\ndisplay(train.describe().style.set_caption(\"Basic statistics about Train Data\"))\n\n\n\n\n\nBasic statistics about Train Data\n\n\n¬†\nid\nsong_duration_ms\nacousticness\ndanceability\nenergy\ninstrumentalness\nkey\nliveness\nloudness\naudio_mode\nspeechiness\ntempo\ntime_signature\naudio_valence\nsong_popularity\n\n\n\n\ncount\n40000.000000\n35899.000000\n36008.000000\n35974.000000\n36025.000000\n36015.000000\n35935.000000\n35914.000000\n36043.000000\n40000.000000\n40000.000000\n40000.000000\n40000.000000\n40000.000000\n40000.000000\n\n\nmean\n19999.500000\n193165.847572\n0.276404\n0.570951\n0.683932\n0.036527\n5.042605\n0.198514\n-7.407596\n0.321150\n0.094107\n116.562815\n3.394375\n0.580645\n0.364400\n\n\nstd\n11547.149720\n45822.127679\n0.297928\n0.190010\n0.212662\n0.150024\n3.372728\n0.151670\n3.877198\n0.466924\n0.083591\n26.167911\n0.524405\n0.237351\n0.481268\n\n\nmin\n0.000000\n25658.000000\n-0.013551\n0.043961\n-0.001682\n-0.004398\n0.000000\n0.027843\n-32.117911\n0.000000\n0.015065\n62.055779\n2.000000\n0.013398\n0.000000\n\n\n25%\n9999.750000\n166254.500000\n0.039618\n0.424760\n0.539276\n0.000941\n2.000000\n0.111796\n-9.578139\n0.000000\n0.038500\n96.995309\n3.000000\n0.398669\n0.000000\n\n\n50%\n19999.500000\n186660.000000\n0.140532\n0.608234\n0.704453\n0.001974\n5.000000\n0.135945\n-6.345413\n0.000000\n0.055881\n113.795959\n3.000000\n0.598827\n0.000000\n\n\n75%\n29999.250000\n215116.000000\n0.482499\n0.718464\n0.870503\n0.003225\n8.000000\n0.212842\n-4.620711\n1.000000\n0.118842\n128.517383\n4.000000\n0.759635\n1.000000\n\n\nmax\n39999.000000\n491671.000000\n1.065284\n0.957131\n1.039741\n1.075415\n11.000000\n1.065298\n-0.877346\n1.000000\n0.560748\n219.163578\n5.000000\n1.022558\n1.000000\n\n\n\n\n\nWe find:\n\nMost of the features are in the range of 0 and 1\nThere are features with only negative values (loudness), binary features (audio_mode) , and seems to be categorical (key and time_signature)\n\n\n\n3.2. Missing data\nNow let‚Äôs take a closer look at the missing values in the dataset\n\n\nCode\n\"\"\"Missing Values\"\"\"\nprint(f\"Train set has {train.isnull().sum().sum()} missing values, and test set has {test.isnull().sum().sum()} missing values\")\n\n\nTrain set has 32187 missing values, and test set has 7962 missing values\n\n\n\n\nCode\n# Refrence (edited): https://datavizpyr.com/visualizing-missing-data-with-seaborn-heatmap-and-displot/\nfig = plt.figure(figsize=(18,6))\n\nsns.displot(\n    data=train.isna().melt(value_name=\"missing\"),\n    y=\"variable\",\n    hue=\"missing\",\n    multiple=\"fill\",\n    aspect=3\n)\nplt.title(\"Missing values shown using Bar plot\", fontsize=17)\nplt.ylabel(\"\")\nplt.xlabel(\"\")\n\nplt.figure(figsize=(18,10))\nsns.heatmap(train.isna().transpose())\nplt.title('Heatmap showing Missing Values in Train data', fontsize=17)\nplt.ylabel(\"\")\nplt.xlabel(\"\")\nplt.show()\n\n\n&lt;Figure size 1296x432 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nCode\ntrain_null = train.isna().sum().sort_values(ascending = False)\ntest_null = test.isna().sum().sort_values(ascending = False)\n\nnon_zero_train_values = train_null[train_null.values &gt; 0]\nnon_zero_test_values = test_null[test_null.values &gt; 0]\n\nfig, axes = plt.subplots(1,2, figsize=(15,8))\nsns.barplot(y=non_zero_test_values.index , x=non_zero_test_values.values, ax=axes[1], palette = \"viridis\")\nsns.barplot(y=non_zero_train_values.index , x=non_zero_train_values.values, ax=axes[0], palette = \"viridis\")\naxes[0].set_title(\"Train data\", fontsize=14)\naxes[1].set_title(\"Test data\", fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/kaggle/song-popularity-eda.html#visualization---individual-features",
    "href": "posts/kaggle/song-popularity-eda.html#visualization---individual-features",
    "title": "Song Popularity EDA",
    "section": "4. Visualization - Individual Features",
    "text": "4. Visualization - Individual Features\nAfter getting an initial idea about our features and their values, we can now dive into the visual part of the exploration. I recommend to always plot your data. Sometimes this might be challenging, e.g.¬†because you have tons of features. In that case, you want to start at least with a subset before you run any dimensionality reduction or other tools. This step is as much about spotting issues and irregularities as it is about learning more about the shapes and distributions of your features.\n\n4.1. Predictor variables\n\nIn the live session, we were building this plot step by step. (Well, we got most of the way there.) It really pays off to take the time and investigate each feature separately. This is one of the most instructive steps in the EDA process, where you aim to learn how messed up your features are. No dataset is perfect. We want to figure out how severe those imperfections are, and whether we can live with them or have to address them.\nDifferent kind of data types go best with different kind of visuals. My recommendation is to start out with density plots or histograms for numerical features, and with barcharts for those that are better expressed as types of categories.\n\n\n\nCode\nuseful_cols = [col for col in train.columns if col not in [\"id\", \"song_popularity\"]]\nnumeric_cols = [col for col in useful_cols if col not in [\"key\", \"audio_mode\", \"time_signature\"]]\n\nn_rows = 5\nn_cols = 3\nindex = 1\n\ncolors = [\"red\", \"darkblue\", \"green\"]\n\nfig = plt.figure(figsize=(16,20))\n\nfor index, col in enumerate(train[useful_cols].columns):\n    plt.subplot(n_rows,n_cols,index+1)\n    \n    if col in numeric_cols:\n        sns.kdeplot(train[col], color=random.sample(colors, 1), fill=True)\n        plt.title(col, fontsize=14)\n        plt.xlabel(\"\")\n        plt.ylabel(\"\")\n        plt.tight_layout()\n    else:\n        sns.countplot(train[col])\n        plt.title(col, fontsize=14)\n        plt.xlabel(\"\")\n        plt.ylabel(\"\")\n        plt.tight_layout()\n\nplt.subplot(n_rows,n_cols,14)\nsns.kdeplot(np.log(train['instrumentalness']), color=random.sample(colors, 1), fill=True)\nplt.title('instrumentalness (log transformed)', fontsize=14)\nplt.ylabel(\" \")\nplt.xlabel(\" \")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nWe find:\n\nOur initial impressions of the data types have largely been confirmed: audio_mode is a boolean feature, and time_signature and key are ordinal or categorical ones (or integer; although a better understanding of those musical concepts would certainly benefit from some domain knowledge.)\nA number of features are bounded between 0 and 1: accosticness, danceability, energy, liveliness, speechiness, and audio_valence.\nThe feature loudness looks like it refer to the decibel scale.\nThe distribution of instrumentalness is heavily right-skewed, and even after a log transform this feature doesn‚Äôt look very well-behaved. This might need a bit more work.\n\n\n\n4.2. Target: Song Popularity\nOn to the target itself. We figured out that song_popularity is a binary feature, and thus we can express it as boolean. Here we plot a barchart.\n\n\nCode\nsns.countplot(train.song_popularity.astype(\"bool\"))\nplt.title(\"Target: Song Popularity\", fontsize=14)\nplt.xlabel(\"\")\nplt.ylabel(\"\")\nplt.show()\n\n\n\n\n\nWe find:\n\nThere is a slight imbalance in the target distribution: a bit more than 60/40. Not super imbalanced, but something to keep in mind."
  },
  {
    "objectID": "posts/kaggle/song-popularity-eda.html#feature-interactions",
    "href": "posts/kaggle/song-popularity-eda.html#feature-interactions",
    "title": "Song Popularity EDA",
    "section": "5. Feature interactions",
    "text": "5. Feature interactions\nAfter learning more about each individual feature, we now want to see them interacting with one another. It‚Äôs best to perfom those steps in that order, so that you can understand and interpret the interactions in the context of the overall distributions.\n\n5.1. Target impact\nWe have seen all the feature distributions, now we want to investigate whether they look different based on the target value. Here‚Äôs an example for song_duration:\n\n\nCode\nfig = plt.figure(figsize=(16,18))\nn_rows = 4\nn_cols = 3\n\nfor index, col in enumerate(numeric_cols):\n    plt.subplot(n_rows, n_cols, index+1)\n    \n    sns.kdeplot(train[col], hue=train.song_popularity.astype(\"bool\"), fill=True)\n    plt.title(col, fontsize=14)\n    plt.xlabel(\"\")\n    plt.ylabel(\"\")\n    plt.tight_layout()\nplt.show()\n\n\n\n\n\nObservations:\n\nBy looking at the probability distribution of different variables we find that popular songs are almost exactly the same length as unpopular ones. There is a slight difference, but it‚Äôs pretty small.\n\nNow we can check the categorical features.\n\n\nCode\nfig = plt.figure(figsize=(18,5))\n\nfor index, col in enumerate([\"key\", \"audio_mode\", \"time_signature\"]):\n    plt.subplot(1,3,index+1)\n    \n    sns.countplot(train[col], hue=train.song_popularity.astype(\"bool\"))\n    plt.title(col, fontsize=14)\n    plt.xlabel(\"\")\n    plt.ylabel(\"\")\n    plt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n5.2. Feature Interaction\nHow do the predictor features interact with each other? Are there any redundancies or strong relationships? We will start out with a correlation matrix, and then look at features of interest in a bit more detail.\n\n5.2.1. Correlations overview\n\n\nCode\n# Refrence (edited): https://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec\ndef heatmap(data):\n    corr = pd.melt(data.reset_index(), id_vars='index') # Unpivot the dataframe, so we can get pair of arrays for x and y\n    corr.columns = ['x', 'y', 'value']\n    x=corr['x']\n    y=corr['y']\n    size=corr['value'].abs()\n    color=corr['value']\n    \n    fig, ax = plt.subplots(figsize=(10,10))\n    plot_grid = plt.GridSpec(1, 15, hspace=0.2, wspace=0.1) # Setup a 1x15 grid\n    ax = plt.subplot(plot_grid[:,:-1]) # Use the leftmost 14 columns of the grid for the main plot\n    \n    n_colors = 256 # Use 256 colors for the diverging color palette\n    palette = sns.diverging_palette(20, 220, n=n_colors) # Create the palette\n    color_min, color_max = [-1, 1] # Range of values that will be mapped to the palette, i.e. min and max possible correlation\n    size_min, size_max = 0, 1\n    \n    def value_to_color(val):\n        val_position = float((val - color_min)) / (color_max - color_min) # position of value in the input range, relative to the length of the input range\n        val_position = min(max(val_position, 0), 1) # bound the position betwen 0 and 1\n        ind = int(val_position * (n_colors - 1)) # target index in the color palette\n        return palette[ind]\n    \n    def value_to_size(val):\n        val_position = (val - size_min) * 0.99 / (size_max - size_min) + 0.01 # position of value in the input range, relative to the length of the input range\n        val_position = min(max(val_position, 0), 1) # bound the position betwen 0 and 1\n        return val_position * size_scale\n        \n    # Mapping from column names to integer coordinates\n    x_labels = [v for v in sorted(x.unique())]\n    y_labels = [v for v in sorted(y.unique())]\n    x_to_num = {p[1]:p[0] for p in enumerate(x_labels)} \n    y_to_num = {p[1]:p[0] for p in enumerate(y_labels)} \n    \n    size_scale = 500\n    ax.scatter(\n        x=x.map(x_to_num), # Use mapping for x\n        y=y.map(y_to_num), # Use mapping for y\n        s=size.apply(value_to_size), # Vector of square sizes, proportional to size parameter\n        c=color.apply(value_to_color), # Vector of square color values, mapped to color palette\n        marker='s' # Use square as scatterplot marker\n    )\n    \n    # Show column labels on the axes\n    ax.set_xticks([x_to_num[v] for v in x_labels])\n    ax.set_xticklabels(x_labels, rotation=45, horizontalalignment='right')\n    ax.set_yticks([y_to_num[v] for v in y_labels])\n    ax.set_yticklabels(y_labels)\n    \n    ax.grid(False, 'major')\n    ax.grid(True, 'minor')\n    ax.set_xticks([t + 0.5 for t in ax.get_xticks()], minor=True)\n    ax.set_yticks([t + 0.5 for t in ax.get_yticks()], minor=True)\n        \n    ax.set_xlim([-0.5, max([v for v in x_to_num.values()]) + 0.5])\n    ax.set_ylim([-0.5, max([v for v in y_to_num.values()]) + 0.5])\n    ax.set_facecolor('#F1F1F1')\n    \n    # Add color legend on the right side of the plot\n    ax = plt.subplot(plot_grid[:,-1]) # Use the rightmost column of the plot\n\n    col_x = [0]*len(palette) # Fixed x coordinate for the bars\n    bar_y=np.linspace(color_min, color_max, n_colors) # y coordinates for each of the n_colors bars\n\n    bar_height = bar_y[1] - bar_y[0]\n    ax.barh(\n        y=bar_y,\n        width=[5]*len(palette), # Make bars 5 units wide\n        left=col_x, # Make bars start at 0\n        height=bar_height,\n        color=palette,\n        linewidth=0\n    )\n    ax.set_xlim(1, 2) # Bars are going from 0 to 5, so lets crop the plot somewhere in the middle\n    ax.grid(False) # Hide grid\n    ax.set_facecolor('white') # Make background white\n    ax.set_xticks([]) # Remove horizontal ticks\n    ax.set_yticks(np.linspace(min(bar_y), max(bar_y), 3)) # Show vertical ticks for min, middle and max\n    ax.yaxis.tick_right() # Show vertical ticks on the right\n    \n\n\n\n\nCode\nheatmap(train[numeric_cols].corr())\n\n\n\n\n\nBelow is a similar correlation heatmap but only using the lower triangle to show the correlation.\n\n\nCode\n# Refrence (edited): https://medium.com/@szabo.bibor/how-to-create-a-seaborn-correlation-heatmap-in-python-834c0686b88e\n\nfig = plt.figure(figsize=(10,10))\nmatrix = np.triu(np.ones_like(train[numeric_cols].corr(), dtype=np.bool))\nsns.heatmap(train[numeric_cols].corr(), mask=matrix, vmin=-1, vmax=1, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))\nplt.show()\n\n\n\n\n\nWe find:\n\nThere‚Äôs a strong anti-correlation between acousticness vs energy and loudness, respectively. Consequently, energy and loudness share a strong correlation.\nNone of the features individually show a notable correlation with the target song_popularity.\n\n\n\n5.2.2. Categorical feature interactions\nWhenever we‚Äôre looking at categorical features, we can assign a visualisation dimension like colour, size, or facets to those. We will start modifying our trusted density plots to look at the distributions of energy (potentially one of the more interesting numerical features) for the different values of time_signature (here encoded as colour):\n\n\nCode\nfig = plt.figure(figsize=(10,8))\nsns.kdeplot(x=\"energy\", hue=\"time_signature\", data=train, fill=True, bw=0.03)\nplt.show()\n\n\n\n\n\n\n\nCode\n(ggplot(train, aes(\"key\", \"time_signature\", fill = \"energy\")) \n + geom_tile()\n + theme(figure_size=(16,5))\n + scale_x_continuous(breaks=range(0,12)))\n\n\n\n\n\nWe find:\n\nFor time_signatures 2 and 5 we have no instances of key == 11. This is no big surprise, since those three values are already rare individually, which makes their combinations even more rare.\nThere are no clear clusters of high vs low energy features here.\nWe can see certain combinations that are particularly low energy, such as key == 2 and time_signature == 1 or 8. key == 3 and time_signature == 1 seems to be a particularly energetic combination.\n\n\n\n\n5.3. Feature Target Interaction\nOnce we have found interesting correlations we can look for clustering in the target variable.\n\n\nCode\n(ggplot(train, aes('key', 'time_signature')) \n + geom_tile(aes(fill='energy')) \n + facet_wrap(\"song_popularity\", nrow = 2) \n + theme_minimal() \n + theme(figure_size=(16, 8))\n + scale_x_continuous(breaks=range(0,12)))\n\n\n\n\n\n\n\nCode\nsns.displot(data=train, x=\"energy\", y=\"audio_valence\", col=\"song_popularity\", kind=\"kde\", fill=True, legend=True, height=8, aspect=0.75)\nplt.show()\n\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize=(12,6))\nsns.scatterplot(x=\"energy\", y=\"acousticness\", hue=\"song_popularity\", data=train)\nplt.show()\n\n\n\n\n\n\n\nMore Resources:\n\nChoosing different color palette in Seaborn\nSee also\n\nPairGrid: Subplot grid for plotting pairwise relationships\nrelplot: Combine a relational plot and a FacetGrid\ndisplot: Combine a distribution plot and a FacetGrid\ncatplot: Combine a categorical plot and a FacetGrid\nlmplot: Combine a regression plot and a FacetGrid\n\n\nSpecial Thanks to Martin Henze for sharing his knowledge during the live coding session. Also, thank you Abhishek Thakur for hosting these wonderful sessions for people to learn. I look forward to learn more.\nShare if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "posts/kaggle/credit-card-fraud-detection-ensemble-model.html",
    "href": "posts/kaggle/credit-card-fraud-detection-ensemble-model.html",
    "title": "Modeling - Playground Series Season 3, Episode 4",
    "section": "",
    "text": "To read more about Exploratory Data Analysis (EDA) for this problem statement, you can read my notebook here.\nIt was my first try at a kaggle comptetion and I got a lot to learn. This notebook helped me get a score of:\nPosition: 138/641\nCode\n# Import libraries\n\nimport numpy as np\nimport pandas as pd\nimport catboost\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom collections import defaultdict\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom boruta import BorutaPy\n\npd.options.display.max_columns = None\n%matplotlib inline"
  },
  {
    "objectID": "posts/kaggle/credit-card-fraud-detection-ensemble-model.html#data-description",
    "href": "posts/kaggle/credit-card-fraud-detection-ensemble-model.html#data-description",
    "title": "Modeling - Playground Series Season 3, Episode 4",
    "section": "1. Data Description",
    "text": "1. Data Description\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Credit Card Fraud Detection. Feature distributions are close to, but not exactly the same, as the original.\nWe are given three files:\n\ntrain.csv - the training dataset; Class is the target\ntest.csv - the test dataset; our objective is to predict Class\nsample_submission.csv - a sample submission file in the correct format\n\nDescription of each column:\n\nFeature Description\n\n\n\n\n\n\nFeature\nDescription\n\n\n\n\nid\nIndentifier for unique rows\n\n\nTime\nNumber of seconds elapsed between this transaction and the first transaction in the dataset\n\n\nV1-V28\nFeatures generated from the original dataset\n\n\nAmount\nTransaction amount\n\n\nClass\nTarget Feature: 1 for fraudulent transactions, 0 otherwise"
  },
  {
    "objectID": "posts/kaggle/credit-card-fraud-detection-ensemble-model.html#data-preparation",
    "href": "posts/kaggle/credit-card-fraud-detection-ensemble-model.html#data-preparation",
    "title": "Modeling - Playground Series Season 3, Episode 4",
    "section": "2. Data Preparation",
    "text": "2. Data Preparation\n\n__dirname = '../input/playground-series-s3e4/'\nog_data = '/kaggle/input/creditcardfraud/creditcard.csv'\n\ntrain = pd.read_csv(__dirname + 'train.csv')\ntest = pd.read_csv(__dirname + 'test.csv')\noriginal = pd.read_csv(og_data)\nsubmissions = pd.read_csv(__dirname + 'sample_submission.csv')\n\n\nprint(f\"Training set has {train.shape[0]} rows and {train.shape[1]} columns\")\nprint(f\"Testing set has {test.shape[0]} rows and {test.shape[1]} columns\")\nprint(\n    f\"Original set has {original.shape[0]} rows and {original.shape[1]} columns\")\n\nTraining set has 219129 rows and 32 columns\nTesting set has 146087 rows and 31 columns\nOriginal set has 284807 rows and 31 columns\n\n\n\n# Check for missing values\nprint(f\"Number of missing values in training set: {sum(train.isna().sum())}\")\nprint(f\"Number of missing values in testing set: {sum(test.isna().sum())}\")\nprint(\n    f\"Number of missing values in original set: {sum(original.isna().sum())}\")\n\nNumber of missing values in training set: 0\nNumber of missing values in testing set: 0\nNumber of missing values in original set: 0\n\n\n\n# Merge train and original\ntrain = pd.concat([train, original], axis=0, ignore_index=True)\n\n\ncols = [col for col in train.columns if col not in ['id', 'Time']]\nprint(' Number of duplicates with new criteria:',\n      train[cols].duplicated().sum())\n\n Number of duplicates with new criteria: 9254\n\n\n\n# Drop Duplicates\ntrain.drop_duplicates(subset=cols, inplace=True, keep='first')\n\n\n# Drop columns\ntrain.drop(['id', 'Time'], axis=1, inplace=True)\ntest.drop(['id', 'Time'], axis=1, inplace=True)"
  },
  {
    "objectID": "posts/kaggle/credit-card-fraud-detection-ensemble-model.html#modeling",
    "href": "posts/kaggle/credit-card-fraud-detection-ensemble-model.html#modeling",
    "title": "Modeling - Playground Series Season 3, Episode 4",
    "section": "3. Modeling",
    "text": "3. Modeling\n\n# Perform feature scaling\nscaler = RobustScaler()\n\ncols = test.columns\ntrain[cols] = scaler.fit_transform(train[cols])\ntest[cols] = scaler.transform(test[cols])\n\n\nX_train, y_train = train.drop('Class', axis=1), train.Class\n\n\n# Dictionary to save model results\nresults = defaultdict(lambda: defaultdict(list))\n\n\n# Function to plot feature importance\ndef plot_feature_imp(df, col):\n    df = pd.concat(df, axis=1).head(15)\n    df.sort_values(col).plot(kind='barh', figsize=(\n        15, 10), title=\"Feature Imp Across Folds\")\n    plt.show()\n\n\n3.1. Baseline Model\n\nn_folds = 5\nseed = 42\nmodel = 'Logistic Regression'\n\nskf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\ntest_pred = 0\n\nresults[model] = defaultdict(list)\n\nfor idx, (train_idx, test_idx) in enumerate(skf.split(X_train, y_train)):\n    X, y = X_train.iloc[train_idx], y_train.iloc[train_idx]\n    X_valid, y_valid = X_train.iloc[test_idx], y_train.iloc[test_idx]\n\n    lr = LogisticRegression()\n\n    # Fit the model\n    lr.fit(X, y)\n    # Predict on validation set\n    pred_proba = lr.predict_proba(X_valid)[:, 1]\n\n    # Predict on test set\n    test_pred_proba = lr.predict_proba(test)[:, 1]\n    test_pred += test_pred_proba/n_folds\n\n    # Calcualte AUC score\n    auc = roc_auc_score(y_valid, pred_proba)\n\n    # Feature Importance\n    feature_imp = pd.DataFrame(index=X_train.columns,\n                               data=lr.coef_[0],\n                               columns=[f\"{idx + 1}_importance\"])\n\n    print(f\"Fold={idx+1}, AUC score: {auc:.2f}\")\n\n    # Save Results\n    results[model]['auc'].append(auc)\n    results[model]['model'].append(lr)\n    results[model]['feature_imp'].append(feature_imp)\n\nresults[model]['test_pred'] = test_pred\nprint(f\"Mean AUC: {np.mean(results[model]['auc'])}\")\n\nFold=1, AUC score: 0.85\nFold=2, AUC score: 0.88\nFold=3, AUC score: 0.88\nFold=4, AUC score: 0.87\nFold=5, AUC score: 0.86\nMean AUC: 0.8679439362536849\n\n\n\n# Plot feature importance for logistic regression model\nplot_feature_imp(results[model]['feature_imp'], \"2_importance\")\n\n\n\n\n\n# Made a baseline submission to see how the model performs\nlr_submission = submissions.copy()\nlr_submission['Class'] = results[model]['test_pred']\nlr_submission.to_csv('lr_submission.csv', index=False)\n\n\nlr_submission.head()\n\n\n\n\n\n\n\n\nid\nClass\n\n\n\n\n0\n219129\n0.001192\n\n\n1\n219130\n0.000621\n\n\n2\n219131\n0.000201\n\n\n3\n219132\n0.000699\n\n\n4\n219133\n0.000361\n\n\n\n\n\n\n\n\n\n3.2. XGBoost Classifier\n\nxgb_params = {\n    'n_estimators': 2000,\n    'min_child_weight': 96,\n    'max_depth': 7,\n    'learning_rate': 0.18,\n    'subsample': 0.95,\n    'colsample_bytree': 0.95,\n    'reg_lambda': 1.50,\n    'reg_alpha': 1.50,\n    'gamma': 1.50,\n    'max_bin': 512,\n    'random_state': seed,\n    'objective': 'binary:logistic',\n    'tree_method': 'hist',\n    'eval_metric': 'auc'\n}\n\n\nn_folds = 5\nseed = 42\nmodel = 'XGBoostClassifier'\n\nskf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\ntest_pred = 0\n\nresults[model] = defaultdict(list)\n\nfor idx, (train_idx, test_idx) in enumerate(skf.split(X_train, y_train)):\n    X, y = X_train.iloc[train_idx], y_train.iloc[train_idx]\n    X_valid, y_valid = X_train.iloc[test_idx], y_train.iloc[test_idx]\n\n    xgb = XGBClassifier(**xgb_params)\n\n    # Fit the model\n    xgb.fit(X, y)\n    # Predict on validation set\n    pred_proba = xgb.predict_proba(X_valid)[:, 1]\n\n    # Predict on test set\n    test_pred_proba = xgb.predict_proba(test)[:, 1]\n    test_pred += test_pred_proba/n_folds\n\n    # Calcualte AUC score\n    auc = roc_auc_score(y_valid, pred_proba)\n\n    # Feature Importance\n    feature_imp = pd.DataFrame(index=X_train.columns,\n                               data=xgb.feature_importances_,\n                               columns=[f\"{idx + 1}_importance\"])\n\n    print(f\"Fold={idx+1}, AUC score: {auc:.2f}\")\n\n    # Save Results\n    results[model]['auc'].append(auc)\n    results[model]['model'].append(lr)\n    results[model]['feature_imp'].append(feature_imp)\n\nresults[model]['test_pred'] = test_pred\nprint(f\"Mean AUC: {np.mean(results[model]['auc'])}\")\n\nFold=1, AUC score: 0.88\nFold=2, AUC score: 0.90\nFold=3, AUC score: 0.91\nFold=4, AUC score: 0.89\nFold=5, AUC score: 0.90\nMean AUC: 0.8951748159271997\n\n\n\n# Plot feature importance for XGBoost model\nplot_feature_imp(results[model]['feature_imp'], \"2_importance\")\n\n\n\n\n\n# Made a submission to see how the model performs\nxgb_submission = submissions.copy()\nxgb_submission['Class'] = results[model]['test_pred']\nxgb_submission.to_csv('xgb_submission.csv', index=False)\n\n\nxgb_submission.head()\n\n\n\n\n\n\n\n\nid\nClass\n\n\n\n\n0\n219129\n0.001166\n\n\n1\n219130\n0.000573\n\n\n2\n219131\n0.000367\n\n\n3\n219132\n0.001084\n\n\n4\n219133\n0.000131\n\n\n\n\n\n\n\n\n\n3.3. LightGBM Classifier\n\nlgbm_params = {\n    'n_estimators': 500,\n    'learning_rate': 0.1,\n    'num_leaves': 195,\n    'max_depth': 9,\n    'min_data_in_leaf': 46,\n    'lambda_l1': 0.01,\n    'lambda_l2': 0.6,\n    'min_gain_to_split': 1.42,\n    'bagging_fraction': 0.45,\n    'feature_fraction': 0.3,\n    'verbosity': -1,\n    'boosting_type': 'dart',\n    'random_state': seed,\n    'objective': 'binary'\n}\n\n\nn_folds = 5\nseed = 42\nmodel = 'LGBMClassifier'\n\nskf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\ntest_pred = 0\n\nresults[model] = defaultdict(list)\n\nfor idx, (train_idx, test_idx) in enumerate(skf.split(X_train, y_train)):\n    X, y = X_train.iloc[train_idx], y_train.iloc[train_idx]\n    X_valid, y_valid = X_train.iloc[test_idx], y_train.iloc[test_idx]\n\n    lgbm = LGBMClassifier(**lgbm_params)\n\n    # Fit the model\n    lgbm.fit(X, y)\n    # Predict on validation set\n    pred_proba = lgbm.predict_proba(X_valid)[:, 1]\n\n    # Predict on test set\n    test_pred_proba = lgbm.predict_proba(test)[:, 1]\n    test_pred += test_pred_proba/n_folds\n\n    # Calcualte AUC score\n    auc = roc_auc_score(y_valid, pred_proba)\n\n    # Feature Importance\n    feature_imp = pd.DataFrame(index=X_train.columns,\n                               data=lgbm.feature_importances_,\n                               columns=[f\"{idx + 1}_importance\"])\n\n    print(f\"Fold={idx+1}, AUC score: {auc:.2f}\")\n\n    # Save Results\n    results[model]['auc'].append(auc)\n    results[model]['model'].append(lr)\n    results[model]['feature_imp'].append(feature_imp)\n\nresults[model]['test_pred'] = test_pred\nprint(f\"Mean AUC: {np.mean(results[model]['auc'])}\")\n\n[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n[LightGBM] [Warning] bagging_fraction is set=0.45, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45\n[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n[LightGBM] [Warning] min_gain_to_split is set=1.42, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=1.42\n[LightGBM] [Warning] lambda_l2 is set=0.6, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6\nFold=1, AUC score: 0.89\n[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n[LightGBM] [Warning] bagging_fraction is set=0.45, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45\n[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n[LightGBM] [Warning] min_gain_to_split is set=1.42, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=1.42\n[LightGBM] [Warning] lambda_l2 is set=0.6, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6\nFold=2, AUC score: 0.90\n[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n[LightGBM] [Warning] bagging_fraction is set=0.45, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45\n[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n[LightGBM] [Warning] min_gain_to_split is set=1.42, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=1.42\n[LightGBM] [Warning] lambda_l2 is set=0.6, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6\nFold=3, AUC score: 0.91\n[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n[LightGBM] [Warning] bagging_fraction is set=0.45, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45\n[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n[LightGBM] [Warning] min_gain_to_split is set=1.42, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=1.42\n[LightGBM] [Warning] lambda_l2 is set=0.6, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6\nFold=4, AUC score: 0.89\n[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n[LightGBM] [Warning] bagging_fraction is set=0.45, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45\n[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n[LightGBM] [Warning] min_gain_to_split is set=1.42, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=1.42\n[LightGBM] [Warning] lambda_l2 is set=0.6, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6\nFold=5, AUC score: 0.90\nMean AUC: 0.8997556737837178\n\n\n\n# Plot feature importance for LightGBM model\nplot_feature_imp(results[model]['feature_imp'], \"4_importance\")\n\n\n\n\n\n# Made a submission to see how the model performs\nlgbm_submission = submissions.copy()\nlgbm_submission['Class'] = results[model]['test_pred']\nlgbm_submission.to_csv('lgbm_submission.csv', index=False)\n\n\nlgbm_submission.head()\n\n\n\n\n\n\n\n\nid\nClass\n\n\n\n\n0\n219129\n0.001045\n\n\n1\n219130\n0.000622\n\n\n2\n219131\n0.000337\n\n\n3\n219132\n0.000817\n\n\n4\n219133\n0.000307\n\n\n\n\n\n\n\n\n\n3.4. CatBoot Classifier\n\ncatboost_params = {\n    'n_estimators': 500,\n    'learning_rate': 0.1,\n    'one_hot_max_size': 12,\n    'depth': 9,\n    'l2_leaf_reg': 0.6,\n    'colsample_bylevel': 0.06,\n    'min_data_in_leaf': 12,\n    'bootstrap_type': 'Bernoulli',\n    'verbose': False,\n    'random_state': seed,\n    'objective': 'Logloss',\n    'eval_metric': 'AUC'\n}\n\n\nn_folds = 5\nseed = 42\nmodel = 'CatBoostClassifier'\n\nskf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\ntest_pred = 0\n\nresults[model] = defaultdict(list)\n\nfor idx, (train_idx, test_idx) in enumerate(skf.split(X_train, y_train)):\n    X, y = X_train.iloc[train_idx], y_train.iloc[train_idx]\n    X_valid, y_valid = X_train.iloc[test_idx], y_train.iloc[test_idx]\n\n    catb = CatBoostClassifier(**catboost_params)\n\n    # Fit the model\n    catb.fit(X, y)\n    # Predict on validation set\n    pred_proba = catb.predict_proba(X_valid)[:, 1]\n\n    # Predict on test set\n    test_pred_proba = catb.predict_proba(test)[:, 1]\n    test_pred += test_pred_proba/n_folds\n\n    # Calcualte AUC score\n    auc = roc_auc_score(y_valid, pred_proba)\n\n    # Feature Importance\n    feature_imp = pd.DataFrame(index=X_train.columns,\n                               data=catb.feature_importances_,\n                               columns=[f\"{idx + 1}_importance\"])\n\n    print(f\"Fold={idx+1}, AUC score: {auc:.2f}\")\n\n    # Save Results\n    results[model]['auc'].append(auc)\n    results[model]['model'].append(lr)\n    results[model]['feature_imp'].append(feature_imp)\n\nresults[model]['test_pred'] = test_pred\nprint(f\"Mean AUC: {np.mean(results[model]['auc'])}\")\n\nFold=1, AUC score: 0.87\nFold=2, AUC score: 0.88\nFold=3, AUC score: 0.88\nFold=4, AUC score: 0.89\nFold=5, AUC score: 0.89\nMean AUC: 0.877921430469104\n\n\n\n# Plot feature importance for CatBoost model\nplot_feature_imp(results[model]['feature_imp'], \"2_importance\")\n\n\n\n\n\n# Made a submission to see how the model performs\ncatb_submission = submissions.copy()\ncatb_submission['Class'] = results[model]['test_pred']\ncatb_submission.to_csv('catb_submission.csv', index=False)\n\n\ncatb_submission.head()\n\n\n\n\n\n\n\n\nid\nClass\n\n\n\n\n0\n219129\n0.000671\n\n\n1\n219130\n0.000433\n\n\n2\n219131\n0.000124\n\n\n3\n219132\n0.000599\n\n\n4\n219133\n0.000103\n\n\n\n\n\n\n\n\n\n3.5. Ensemble Results\n\na = 0.1\nb = 0.4\nc = 0.3\nd = 0.2\n\npred = a * lr_submission['Class'] + b * xgb_submission['Class'] + \\\n    c * lgbm_submission['Class'] + d * catb_submission['Class']\n\n# Made a submission to see how the model performs\nsubmission = submissions.copy()\nsubmission['Class'] = pred\nsubmission.to_csv('submission_CatB.csv', index=False)\n\n\na = 0\nb = 0.4\nc = 0.4\nd = 0.2\n\npred = a * lr_submission['Class'] + b * xgb_submission['Class'] + \\\n    c * lgbm_submission['Class'] + d * catb_submission['Class']\n\n# Made a submission to see how the model performs\nsubmission = submissions.copy()\nsubmission['Class'] = pred\nsubmission.to_csv('submission_noLR.csv', index=False)"
  },
  {
    "objectID": "posts/kaggle/credit-card-fraud-detection-ensemble-model.html#conclusion",
    "href": "posts/kaggle/credit-card-fraud-detection-ensemble-model.html#conclusion",
    "title": "Modeling - Playground Series Season 3, Episode 4",
    "section": "4. Conclusion",
    "text": "4. Conclusion\nThank you for reading so far and I hope you found this notebook helpful.\nUpvote if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "posts/mlops/introduction-to-mlflow.html",
    "href": "posts/mlops/introduction-to-mlflow.html",
    "title": "Introduction to MLflow",
    "section": "",
    "text": "Welcome to our latest blog post, where we delve into the fascinating world of experiment tracking and its powerful implementation using MLflow. üìö We‚Äôll explore the concept of experiment tracking and learn how MLflow can revolutionize the way we track and manage our machine learning experiments.\nIn this comprehensive guide, we will walk you through the process of leveraging MLflow to effectively track your experiments, enabling you to gain valuable insights into model performance, parameter tuning, and results. Discover how MLflow‚Äôs model registry feature empowers you to version your models, ensuring reproducibility and seamless collaboration."
  },
  {
    "objectID": "posts/mlops/introduction-to-mlflow.html#what-is-experiment-tracking",
    "href": "posts/mlops/introduction-to-mlflow.html#what-is-experiment-tracking",
    "title": "Introduction to MLflow",
    "section": "What is Experiment Tracking?",
    "text": "What is Experiment Tracking?\nIn a nutshell, it is the process of building an ML model. More formally, Experiment tracking is the process of keeping track of all the relevant information from an ML experiment, which includes:\n\nSource code\nEnvironment\nData\nModel\nHyperparameters\nMetrics\n\nTerminologies associated with ML Experiments:\n\nExperiment Run: Each trail while building an ML model is called an experiment run. Whether it‚Äôs training a vanilla Linear Regression model or conducting hyperparameter tuning with various combinations, each distinct attempt is considered an experiment run. It allows for systematic organization and analysis of different approaches and variations.\nRun Artifact: Any file that is associated with an ML run is an Artifact. Example: Model weights, Model predictions, Model metrics, etc.\nExperiment Metadata: Additional information such as Model Signature - description of a model‚Äôs inputs and outputs, Model Input Example - example of a valid model input."
  },
  {
    "objectID": "posts/mlops/introduction-to-mlflow.html#why-is-experiment-tracking-important",
    "href": "posts/mlops/introduction-to-mlflow.html#why-is-experiment-tracking-important",
    "title": "Introduction to MLflow",
    "section": "Why is Experiment Tracking Important?",
    "text": "Why is Experiment Tracking Important?\n\nReproducibility: It is important to be able to reproduce the results of an experiment. This is especially true in the case of ML experiments where the results are not deterministic. Experiment tracking helps us to reproduce the results of an experiment by keeping track of all the relevant information.\nOrganization: Any file associated with an ML run is considered an artifact. These artifacts can include crucial components such as model weights, model predictions, model metrics, or any other files generated during the experiment. They serve as valuable resources for evaluating and understanding the outcomes of each run.\nOptimization: In addition to the core elements of an ML experiment, there is additional metadata that provides valuable context and insights. This metadata includes the model signature, which describes the inputs and outputs of the model, and the model input example, which provides an example of a valid input for the model. These details contribute to a comprehensive understanding of the experiment and aid in reproducibility.\n\nIn the past, before the advent of dedicated experiment tracking tools, researchers and data scientists relied on spreadsheets to keep track of their experiments. However, this manual approach presented several challenges and limitations. Let‚Äôs explore the major points of concern:\n\nError Prone: The process of manually copying and pasting the results of an experiment from a Jupyter notebook or other sources into a spreadsheet was tedious and error-prone. It often led to mistakes or inaccuracies in recording the data, undermining the reliability and integrity of the experiment records.\nNo standard format: Without a standardized format for organizing and documenting experiment results, researchers faced difficulties in comparing and analyzing the outcomes of different experiments. The lack of consistency hindered the ability to draw meaningful insights and make informed decisions based on the collected data.\nVisibility & Collaboration: Sharing experiment results with other team members was a cumbersome task. Spreadsheet-based tracking offered limited visibility and collaboration capabilities, impeding effective teamwork and knowledge sharing. It was challenging to provide access, gather feedback, or collaborate on an experiment in a seamless manner."
  },
  {
    "objectID": "posts/mlops/introduction-to-mlflow.html#getting-started-with-mlflow",
    "href": "posts/mlops/introduction-to-mlflow.html#getting-started-with-mlflow",
    "title": "Introduction to MLflow",
    "section": "Getting Started with MLflow",
    "text": "Getting Started with MLflow\nMLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It provides a comprehensive set of tools for tracking experiments, packaging ML code, and deploying models. MLflow is designed to work with any ML library and language, and it is built on an open API, enabling seamless integration with other platforms and tools.\nTo see MLflow in action, let‚Äôs walk through a simple example of training a Linear Regression model on NY Green Taxi Trips dataset. We will use MLflow to track the experiment and record the results.\n\nInstall MLflow and Setup Environment\nCreate a new virtual environment and install MLflow and other libraries using the following command:\n\n\nrequirements.txt\n\nmlflow\njupyter\nscikit-learn\npandas\nxgboost\nfastparquet\nhyperopt\noptuna\n\n# Create conda environment\nconda create -p venv python=3.9 -y\n\n# Activate conda environment\nconda activate venv/\n\n# Install required libraries\npip install -r requirements.txt --no-cache-dir\nDownload the dataset using the following command:\n# Create data directory\nmkdir data\n\n# Move to data directory\ncd data\n\n# Download dataset\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-01.parquet # January 2022\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-02.parquet # February 2022\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-03.parquet # March 2022\nWe‚Äôll also create a jupyter notebook named mlflow.ipynb to run our experiment. After following the above steps, you should have the following directory structure:\n.\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îú‚îÄ‚îÄ green_tripdata_2022-01.parquet\n‚îÇ   ‚îú‚îÄ‚îÄ green_tripdata_2022-02.parquet\n‚îÇ   ‚îî‚îÄ‚îÄ green_tripdata_2022-03.parquet\n‚îú‚îÄ‚îÄ mlflow.ipynb\n‚îú‚îÄ‚îÄ requirements.txt\n‚îî‚îÄ‚îÄ venv\nTo see MLflow UI in action, we‚Äôll run the following command while you‚Äôre in the root directory in the terminal:\nmlflow ui\nThis will start the MLflow server on port 5000. You can access the MLflow UI at http://127.0.0.1:5000. Shown below:\n\n\n\n\n\n\n\nNote\n\n\n\nIf you face an Access to 127.0.0.1 was denied. You don‚Äôt have authorization to view this page. HTTP ERROR 403 error while accessing the MLflow UI, you can resolve it by clearing the browser cache and cookies.\n\n\nOne interesting thing to note here is that MLflow will create a new directory named mlruns in the current working directory. This directory will contain all the experiment runs and artifacts. This configuration of backend and artifact storage is called MLflow on localhost. There are other configurations available as well, which we will explore in another post.\n\n\n\nSource: MLflow Documentation\n\n\n\n\nLoad Dataset\nLet‚Äôs start by importing the required libraries and loading the dataset.\n\n\nCode\nimport os\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport mlflow\nimport optuna\n\nfrom mlflow.entities import ViewType\nfrom mlflow.tracking import MlflowClient\nfrom optuna.samplers import TPESampler\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\n\n\ndef read_dataframe(filename: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Reads a Parquet file into a pandas DataFrame, performs data transformations, and returns the resulting DataFrame.\n\n    Parameters:\n        filename (str): The path to the Parquet file to be read.\n\n    Returns:\n        pandas.DataFrame: The processed DataFrame containing the data from the Parquet file.\n\n    Raises:\n        [Any exceptions raised by pandas.read_parquet()]\n\n    Notes:\n        - The function performs the following transformations on the DataFrame:\n            - Converts 'lpep_dropoff_datetime' and 'lpep_pickup_datetime' columns to pandas datetime objects.\n            - Computes the 'duration' column by subtracting 'lpep_pickup_datetime' from 'lpep_dropoff_datetime'\n              and converting the result to minutes.\n            - Filters the DataFrame to include rows where the 'duration' is between 1 and 60 minutes (inclusive).\n            - Converts 'PULocationID' and 'DOLocationID' columns to string type.\n\n    Example:\n        filename = 'data.parquet'\n        df = read_dataframe(filename)\n    \"\"\"\n    # Read the Parquet file into a DataFrame\n    df = pd.read_parquet(filename)\n\n    # Convert 'lpep_dropoff_datetime' and 'lpep_pickup_datetime' columns to pandas datetime objects\n    df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n    df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)\n\n    # Compute the 'duration' column by subtracting 'lpep_pickup_datetime' from 'lpep_dropoff_datetime' and converting to minutes\n    df['duration'] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n\n    # Filter the DataFrame to include rows where the 'duration' is between 1 and 60 minutes (inclusive)\n    df = df[(df.duration &gt;= 1) & (df.duration &lt;= 60)]\n\n    # Convert 'PULocationID' and 'DOLocationID' columns to string type\n    categorical = ['PULocationID', 'DOLocationID']\n    df[categorical] = df[categorical].astype(str)\n    \n    # Return the processed DataFrame\n    return df\n\n\n# Read the Parquet file for training data into a DataFrame\ndf_train = read_dataframe('./data/green_tripdata_2022-01.parquet')\n\n# Read the Parquet file for validation data into a DataFrame\ndf_val = read_dataframe('./data/green_tripdata_2022-02.parquet')\n\n# Read the Parquet file for testing data into a DataFrame\ndf_test = read_dataframe('./data/green_tripdata_2022-03.parquet')\n\n\n\nPrepare and Transform Dataset\n\ndef preprocess(df: pd.DataFrame, dv: DictVectorizer, fit_dv: bool = False):\n    \"\"\"\n    Preprocesses a pandas DataFrame by creating new features, transforming categorical features into a numerical format,\n    and returning the transformed data along with the DictVectorizer.\n\n    Parameters:\n        df (pandas.DataFrame): The input DataFrame to be preprocessed.\n        dv (sklearn.feature_extraction.DictVectorizer): The DictVectorizer instance to be used for transforming categorical features.\n        fit_dv (bool, optional): Indicates whether to fit the DictVectorizer on the data. Defaults to False.\n\n    Returns:\n        tuple: A tuple containing the transformed feature matrix and the DictVectorizer instance.\n\n    Notes:\n        - The function assumes that the DataFrame contains the columns 'PULocationID' and 'DOLocationID'.\n        - The function creates a new feature 'PU_DO' by concatenating 'PULocationID' and 'DOLocationID'.\n        - The categorical feature 'PU_DO' and numerical feature 'trip_distance' are selected for transformation.\n        - The function transforms the selected features into a dictionary representation and applies the DictVectorizer.\n        - If fit_dv is True, the DictVectorizer is fitted on the data. Otherwise, the existing fitted DictVectorizer is used.\n\n    Example:\n        df = read_dataframe('data.parquet')\n        dv = DictVectorizer()\n        X, dv = preprocess(df, dv, fit_dv=True)\n    \"\"\"\n    # Create a new feature 'PU_DO' by concatenating 'PULocationID' and 'DOLocationID'\n    df['PU_DO'] = df['PULocationID'] + '_' + df['DOLocationID']\n\n    # Select categorical and numerical features for transformation\n    categorical = ['PU_DO']\n    numerical = ['trip_distance']\n\n    # Convert the selected features into a dictionary representation\n    dicts = df[categorical + numerical].to_dict(orient='records')\n\n    # Apply DictVectorizer for transforming categorical features\n    if fit_dv:\n        # Fit the DictVectorizer on the data\n        X = dv.fit_transform(dicts)\n    else:\n        # Transform using the existing fitted DictVectorizer\n        X = dv.transform(dicts)\n\n    # Return the transformed feature matrix and DictVectorizer\n    return X, dv\n\n\n# Extract the target variable\ntarget = 'duration'\n\n# Extract the target variable from the training, validation and testing datasets\ny_train = df_train[target].values\ny_val = df_val[target].values\ny_test = df_test[target].values\n\n# Initialize a DictVectorizer for preprocessing\ndv = DictVectorizer()\n\n# Preprocess the training data\nX_train, dv = preprocess(df_train, dv, fit_dv=True)\n\n# Preprocess the validation data using the fitted DictVectorizer from the training data\nX_val, _ = preprocess(df_val, dv, fit_dv=False)\n\n# Preprocess the testing data using the fitted DictVectorizer from the training data\nX_test, _ = preprocess(df_test, dv, fit_dv=False)\n\n\ndef dump_pickle(obj, filename: str):\n    \"\"\"\n    Pickles (serializes) an object and saves it to a file.\n\n    Parameters:\n        obj (Any): The object to be pickled.\n        filename (str): The path and filename to save the pickled object.\n\n    Returns:\n        None\n\n    Notes:\n        - The function uses the 'pickle' module to serialize the object and save it to a file.\n        - The file is opened in binary mode for writing using the \"wb\" mode.\n    \"\"\"\n    with open(filename, \"wb\") as f_out:\n        return pickle.dump(obj, f_out)\n\n\n# Specify the destination path for saving files\ndest_path = './outputs'\n\n# Create dest_path folder unless it already exists\nos.makedirs(dest_path, exist_ok=True)\n\n# Save DictVectorizer object as a pickle file\ndump_pickle(dv, os.path.join(dest_path, \"dv.pkl\"))\n\n\n\nTrain Linear Regression Model with MLflow\nIn mlflow you create one experiment and all the runs are stored under that experiment. You can create a new experiment using the following command:\n\n# Create a new experiment\nmlflow.set_experiment(\"nyc-taxi-experiment\")\n\nIf you check your MLflow UI, you‚Äôll notice a new experiment named ny-taxi-experiment has been created under Experiments section. All the runs and artifacts will be stored under this experiment.\n\n\n# Specify the destination path for saving model files\nmodel_path = \"./outputs/models\"\n\n# Start an MLflow run\nwith mlflow.start_run():\n    # Set a tag for the developer\n    mlflow.set_tag(\"developer\", \"Sagar\")\n\n    # Initialize and train a LinearRegression model\n    lr = LinearRegression()\n    lr.fit(X_train, y_train)\n\n    # Make predictions on the validation data\n    yhat = lr.predict(X_val)\n\n    # Calculate the root mean squared error (RMSE)\n    rmse = mean_squared_error(y_val, yhat, squared=False)\n\n    # Log the RMSE metric to MLflow\n    mlflow.log_metric(\"rmse\", rmse)\n\n    # Create dest_path folder unless it already exists\n    os.makedirs(model_path, exist_ok=True)\n\n    # Save the trained model as a pickle file\n    dump_pickle(lr, os.path.join(model_path, \"lin_reg.pkl\"))\n\n    # Log the trained model as an artifact to MLflow\n    mlflow.log_artifact(local_path=f\"{model_path}/lin_reg.pkl\", artifact_path=\"models_pickle\")\n\nAfter running an experiment run, go to your MLflow UI and there you can see an entry of the run. If you click on the run you have observer other details associated with the run such as Metrics, Tags, Artifacts. In the Artifacts section you can see the model_pickle folder which contains the model file. You can download the model file from there and use it for prediction.\n\n\n\n\n\n\nTip\n\n\n\nIf you don‚Äôt see your run on the UI, you can refresh using Refresh button on the right side of the page above the table.\n\n\n\n\nVoila üéâ! You have successfully trained a Linear Regression model and tracked the experiment using MLflow. You can find the complete code.\n\n\nTraing a Lasso Regression Model\n\n# Specify the destination path for saving model files\nmodel_path = \"./outputs/models\"\n\n# Start an MLflow run\nwith mlflow.start_run():\n    # Set a tag for the developer\n    mlflow.set_tag(\"developer\", \"Sagar\")\n\n    # Set the value for the regularization parameter (alpha)\n    alpha = 0.1\n\n    # Log the regularization parameter (alpha) as a parameter in MLflow\n    mlflow.log_param(\"alpha\", alpha)\n\n    # Initialize and train a Lasso Regression model\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train, y_train)\n\n    # Make predictions on the validation data\n    yhat = lasso.predict(X_val)\n\n    # Calculate the root mean squared error (RMSE)\n    rmse = mean_squared_error(y_val, yhat, squared=False)\n\n    # Log the RMSE metric to MLflow\n    mlflow.log_metric(\"rmse\", rmse)\n\n    # Create the destination folder for saving model files if it doesn't exist\n    os.makedirs(model_path, exist_ok=True)\n\n    # Save the trained Lasso Regression model as a pickle file\n    dump_pickle(lr, os.path.join(model_path, \"lasso_reg.pkl\"))\n\n    # Log the trained model as an artifact to MLflow\n    mlflow.log_artifact(local_path=f\"{model_path}/lasso_reg.pkl\", artifact_path=\"models_pickle\")\n\nSimilar to Linear Regression, we can train a Lasso Regression model and track the experiment using MLflow."
  },
  {
    "objectID": "posts/mlops/introduction-to-mlflow.html#hyperparameter-tuning",
    "href": "posts/mlops/introduction-to-mlflow.html#hyperparameter-tuning",
    "title": "Introduction to MLflow",
    "section": "Hyperparameter Tuning",
    "text": "Hyperparameter Tuning\nTools such as MLflow shine when it comes to hyperparameter tuning and model selection. Let‚Äôs see how we can leverage MLflow to tune the hyperparameters of our model.\nWe‚Äôll setup a new experiment for hyperparameter tuning and use MLflow to track the results. We‚Äôll use Hyperopt and Optuna libraries for hyperparameter tuning.\n\n# Create a new experiment\nmlflow.set_experiment(\"random-forest-hyperopt\")\n\n\ndef run_optimization(num_trials: int = 10):\n    \"\"\"\n    Runs the optimization process using Optuna library to find the optimal hyperparameters for RandomForestRegressor.\n\n    Parameters:\n        num_trials (int): The number of optimization trials to perform. Default is 10.\n\n    Returns:\n        None\n\n    Notes:\n        - The function defines an objective function for Optuna to minimize the root mean squared error (RMSE).\n        - The objective function samples hyperparameters, trains a RandomForestRegressor model with those hyperparameters,\n          evaluates the model on the validation data, and logs the RMSE metric to MLflow.\n        - Optuna performs the optimization process by searching for the set of hyperparameters that minimizes the RMSE.\n    \"\"\"\n\n    def objective(trial):\n        \"\"\"\n        Objective function for Optuna optimization.\n\n        Parameters:\n            trial (optuna.Trial): A trial object representing a single optimization trial.\n\n        Returns:\n            float: The value of the objective function (RMSE).\n\n        Notes:\n            - The objective function samples hyperparameters from the defined search space.\n            - It initializes and trains a RandomForestRegressor model with the sampled hyperparameters.\n            - The model is evaluated on the validation data, and the RMSE is calculated.\n            - The RMSE and the sampled hyperparameters are logged to MLflow.\n        \"\"\"\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 10, 50, 1),\n            'max_depth': trial.suggest_int('max_depth', 1, 20, 1),\n            'min_samples_split': trial.suggest_int('min_samples_split', 2, 10, 1),\n            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 4, 1),\n            'random_state': 42,\n            'n_jobs': -1\n        }\n        \n        # Start a new MLflow run for each trial\n        with mlflow.start_run():\n            # Set a tag for the model type\n            mlflow.set_tag(\"model\", \"RandomForestRegressor\")\n            \n            # Log the sampled hyperparameters to MLflow\n            mlflow.log_params(params)\n\n            # Initialize a RandomForestRegressor model with the sampled hyperparameters\n            rf = RandomForestRegressor(**params)\n\n            # Train the model on the training data\n            rf.fit(X_train, y_train)\n\n            # Make predictions on the validation data\n            y_pred = rf.predict(X_val)\n\n            # Calculate the root mean squared error (RMSE)\n            rmse = mean_squared_error(y_val, y_pred, squared=False)\n\n            # Log the RMSE metric to MLflow\n            mlflow.log_metric(\"rmse\", rmse)\n\n        return rmse\n\n    # Use the Tree-structured Parzen Estimator (TPE) sampler for efficient hyperparameter search\n    sampler = TPESampler(seed=42)\n\n    # Create an Optuna study with the defined objective function and search direction\n    study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n\n    # Run the optimization process with the specified number of trials\n    study.optimize(objective, n_trials=num_trials)\n\nrun_optimization()\n\n[I 2023-05-26 12:23:03,233] A new study created in memory with name: no-name-19f4f970-eafc-402c-89b3-2aa461ab17fc\n[I 2023-05-26 12:23:04,744] Trial 0 finished with value: 6.012747224033297 and parameters: {'n_estimators': 25, 'max_depth': 20, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 0 with value: 6.012747224033297.\n[I 2023-05-26 12:23:04,872] Trial 1 finished with value: 6.249433998787504 and parameters: {'n_estimators': 16, 'max_depth': 4, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 0 with value: 6.012747224033297.\n[I 2023-05-26 12:23:06,223] Trial 2 finished with value: 6.039045655830305 and parameters: {'n_estimators': 34, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 0 with value: 6.012747224033297.\n[I 2023-05-26 12:23:06,636] Trial 3 finished with value: 6.179387143797025 and parameters: {'n_estimators': 44, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 0 with value: 6.012747224033297.\n[I 2023-05-26 12:23:07,140] Trial 4 finished with value: 6.075505898039151 and parameters: {'n_estimators': 22, 'max_depth': 11, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 0 with value: 6.012747224033297.\n[I 2023-05-26 12:23:07,316] Trial 5 finished with value: 6.441117537172997 and parameters: {'n_estimators': 35, 'max_depth': 3, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 0 with value: 6.012747224033297.\n[I 2023-05-26 12:23:08,432] Trial 6 finished with value: 6.0285791267371165 and parameters: {'n_estimators': 28, 'max_depth': 16, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 0 with value: 6.012747224033297.\n[I 2023-05-26 12:23:08,532] Trial 7 finished with value: 7.881244954282265 and parameters: {'n_estimators': 34, 'max_depth': 1, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 0 with value: 6.012747224033297.\n[I 2023-05-26 12:23:09,146] Trial 8 finished with value: 6.025608014492215 and parameters: {'n_estimators': 12, 'max_depth': 19, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 0 with value: 6.012747224033297.\n[I 2023-05-26 12:23:09,239] Trial 9 finished with value: 7.071070856187059 and parameters: {'n_estimators': 22, 'max_depth': 2, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 0 with value: 6.012747224033297.\n\n\nAfter running the experiment, you can see the results in the MLflow UI. You can also select multiple runs and compare them using the Compare button on the top left corner of the table.\n\n\n\nOne of the most interesting plot in the MLflow UI is the Parallel Coordinates Plot. It allows you to visualize the relationship between the hyperparameters and the metrics.\nScatter plot allows you to visualize the relationship between a single hyperparameter and the metric.\nBox Plot are similar to scatter plot in terms of visualization.\nContour Plot allows you to visualize the relationship between two hyperparameters and the metric.\n\nApart from the visualizations, they also provide more information of the Run Duration, Parameters, Metrics and Tags associated with the run in a tabular format to compare the runs."
  },
  {
    "objectID": "posts/mlops/introduction-to-mlflow.html#model-registry",
    "href": "posts/mlops/introduction-to-mlflow.html#model-registry",
    "title": "Introduction to MLflow",
    "section": "Model Registry",
    "text": "Model Registry\nMLflow Model Registry is a centralized model store, set of APIs, and UI, to collaboratively manage the full lifecycle of an MLflow Model. It provides model lineage (which MLflow Experiment and Run produced the model), model versioning, stage transitions (for example from staging to production), annotations, and other functionality to track the model lifecycle.\nFrom our example above for hyperparameter tuning, we can register the best model which performs well on the test dataset.\nTo interact with an MLflow Tracking Server and an MLflow Registry Server, we utilize the Client class. It allows us to create and manage experiments and runs in the Tracking Server, as well as create and manage registered models and model versions in the Registry Server. The mlflow.client module offers a Python interface for performing CRUD operations on MLflow Experiments, Runs, Model Versions, and Registered Models. It serves as a lower-level API that directly corresponds to MLflow‚Äôs REST API calls.\nMore information about the MLflow Client can be found here.\n\n# Hyperparameter optimization experiment name\nHPO_EXPERIMENT_NAME = \"random-forest-hyperopt\"\n\n# Best model experiment name\nEXPERIMENT_NAME = \"random-forest-best-models\"\n\n# Create a new experiment for the best models\nmlflow.set_experiment(EXPERIMENT_NAME)\n\n\n# Automatically log parameters and metrics\nmlflow.sklearn.autolog()\n\n\n# RandomForest Parameters\nRF_PARAMS = ['max_depth', 'n_estimators', 'min_samples_split',\n             'min_samples_leaf', 'random_state', 'n_jobs']\n\n\ndef train_and_log_model(params):\n    \"\"\"\n    Trains a RandomForestRegressor model with the given hyperparameters and logs evaluation metrics to MLflow.\n\n    Parameters:\n        params (dict): Dictionary of hyperparameters for RandomForestRegressor.\n\n    Returns:\n        None\n\n    Notes:\n        - The function starts an MLflow run to track the model training and evaluation process.\n        - It converts certain hyperparameters to integers.\n        - A RandomForestRegressor model is initialized with the provided hyperparameters.\n        - The model is trained on the training data.\n        - The trained model is evaluated on the validation and test sets, and the root mean squared error (RMSE) is calculated and logged to MLflow as evaluation metrics.\n    \"\"\"\n\n    with mlflow.start_run():\n        # Convert specific hyperparameters to integers\n        for param in RF_PARAMS:\n            params[param] = int(params[param])\n\n        # Initialize a RandomForestRegressor model with the given hyperparameters\n        rf = RandomForestRegressor(**params)\n\n        # Train the model on the training data\n        rf.fit(X_train, y_train)\n\n        # Evaluate the trained model on the validation set\n        val_rmse = mean_squared_error(y_val, rf.predict(X_val), squared=False)\n\n        # Log the validation RMSE metric to MLflow\n        mlflow.log_metric(\"val_rmse\", val_rmse)\n\n        # Evaluate the trained model on the test set\n        test_rmse = mean_squared_error(y_test, rf.predict(X_test), squared=False)\n\n        # Log the test RMSE metric to MLflow\n        mlflow.log_metric(\"test_rmse\", test_rmse)\n\n\ndef run_register_model(top_n: int):\n    \"\"\"\n    Runs the process to register the best model based on the top_n model runs with the lowest test RMSE.\n\n    Parameters:\n        top_n (int): The number of top model runs to consider.\n\n    Returns:\n        None\n\n    Notes:\n        - The function interacts with the MLflow tracking server to retrieve and register models.\n        - It retrieves the top_n model runs based on the lowest validation RMSE.\n        - For each run, it trains a model using the hyperparameters from the run and logs evaluation metrics to MLflow.\n        - After evaluating the models, it selects the one with the lowest test RMSE.\n        - The selected model is registered with a specified name in MLflow.\n    \"\"\"\n\n    # Connect to the MLflow tracking server\n    client = MlflowClient()\n\n    # Retrieve the top_n model runs and log the models\n    experiment = client.get_experiment_by_name(HPO_EXPERIMENT_NAME)\n\n    # Retrieve the top_n model runs based on the lowest validation RMSE\n    runs = client.search_runs(\n        experiment_ids=experiment.experiment_id,\n        run_view_type=ViewType.ACTIVE_ONLY,\n        max_results=top_n,\n        order_by=[\"metrics.rmse ASC\"]\n    )\n\n    # Train and log the model for each run\n    for run in runs:\n        # Train and log the model based on the hyperparameters from the run\n        train_and_log_model(params=run.data.params)\n\n    # Select the model with the lowest test RMSE\n    experiment = client.get_experiment_by_name(EXPERIMENT_NAME)\n\n    # Retrieve model runs based on the lowest test RMSE, and select the first run (with the lowest test RMSE)\n    best_run = client.search_runs(\n        experiment_ids=experiment.experiment_id,\n        run_view_type=ViewType.ACTIVE_ONLY,\n        order_by=[\"metrics.test_rmse ASC\"]\n    )[0]\n\n    # Register the best model\n    model_uri = f\"runs:/{best_run.info.run_id}/model\"\n\n    # Register the best model with a specified name\n    mlflow.register_model(\n        model_uri=model_uri,\n        name=\"random-forest-best-model\"\n    )\n\n\n# The number of top model runs to consider\ntop_n = 5\n\nrun_register_model(top_n=top_n)\n\n2023/05/26 13:27:30 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/wizard/Astronaut/Dev/MLOps/week2/venv/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\nSuccessfully registered model 'random-forest-best-model'.\n2023/05/26 13:27:40 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: random-forest-best-model, version 1\nCreated version '1' of model 'random-forest-best-model'.\n\n\nBy following the mentioned steps, a new experiment called random-forest-best-models will be created, displaying five runs. You may observe that each run provides additional information. This is due to the utilization of MLflow‚Äôs autologging feature.\nRead more about autologging here\nAutologging captures more parameters, metrics, and tags, and also saves the model as an artifact along with environment details. This feature proves particularly beneficial when you aim to reproduce the results. You have the option to download the model and environment details, enabling you to replicate the outcomes accurately.\n\nClick on the run with the best model stored and access additional details about the run.\n\nBy clicking on the link to model registry in the above image, you will be redirected to the model registry page.\n\nAnother way to access the model registry is by clicking on the model registry tab on the left side of the MLflow UI. You will be able to see the model registry page with the best model registered.\n\nYou also have an option to change the stage of the model to Staging, Production, or Archived. You can do this by clicking on the stage transition button on the right side of the model name.\nIn the MLflow ecosystem, the responsibility of determining which models are ready for production lies with the Data Scientist. Once a model is registered in the model registry, the Deployment Engineer can review important details such as the model‚Äôs parameters, size, and performance. Based on this information, they can make informed decisions on moving the model across different stages.\nIt‚Äôs important to note that the model registry itself does not handle the deployment of models. It serves as a centralized repository to list the models that are considered production-ready, with stages acting as labels. To complete the deployment process, it is recommended to complement the model registry with a CI/CD pipeline specifically designed for deploying models. This pipeline would handle the actual deployment process, incorporating the production-ready models identified in the model registry."
  },
  {
    "objectID": "posts/mlops/introduction-to-mlflow.html#mlflow-limitations",
    "href": "posts/mlops/introduction-to-mlflow.html#mlflow-limitations",
    "title": "Introduction to MLflow",
    "section": "MLflow Limitations",
    "text": "MLflow Limitations\nMLflow, while a powerful tool for experiment tracking and model management, has certain limitations to consider.\n\nAuthentication & Users: It lacks built-in authentication and user management capabilities, making it less suitable for environments requiring strict access control.\nData versioning: MLflow does not provide a native solution for data versioning, requiring alternative approaches.\nModel/Data Monitoring & Alerting: Moreover, for model and data monitoring, as well as alerting, other specialized tools may be more appropriate."
  },
  {
    "objectID": "posts/mlops/introduction-to-mlflow.html#alternatives-to-mlflow",
    "href": "posts/mlops/introduction-to-mlflow.html#alternatives-to-mlflow",
    "title": "Introduction to MLflow",
    "section": "Alternatives to MLflow",
    "text": "Alternatives to MLflow\n\nNeptune.ai\nComet\nWeights & Biases\n\nCongratulation üéâ! You have successfully trained ML model and tracked the experiment using MLflow.\nThank you for reading and I hope you found this notebook helpful. Upvote if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "posts/mlops/prefect-deployment.html",
    "href": "posts/mlops/prefect-deployment.html",
    "title": "Prefect Deployment",
    "section": "",
    "text": "In this post, we‚Äôll be looking at how to deploy a Prefect flow locally."
  },
  {
    "objectID": "posts/mlops/prefect-deployment.html#what-does-deployment-mean",
    "href": "posts/mlops/prefect-deployment.html#what-does-deployment-mean",
    "title": "Prefect Deployment",
    "section": "What does deployment mean?",
    "text": "What does deployment mean?\nSo far, we‚Äôve learned how to create and run a workflow on our local machine. The Prefect API has been helpful in tracking the workflow‚Äôs status through the user interface.\nHowever, what if we need to run the workflow on a different machine or schedule it to run automatically? This is where deployment comes into play."
  },
  {
    "objectID": "posts/mlops/prefect-deployment.html#deployment-in-prefect",
    "href": "posts/mlops/prefect-deployment.html#deployment-in-prefect",
    "title": "Prefect Deployment",
    "section": "Deployment in Prefect",
    "text": "Deployment in Prefect\nTo deploy a Prefect workflow, we need to package the workflow code, settings, and infrastructure configuration. This packaging process enables us to manage the workflow using the Prefect API and execute it remotely on a Prefect agent.\nI understand this might sound technical, so let‚Äôs simplify it further.\n\nDeployment can be thought of as shipping a package that includes the workflow code, settings, and infrastructure configuration required by Prefect to run the workflow.\nAll the necessary files are packed into a single box and labeled with a yaml file e.g.¬†deployment.yaml. This file contains information about the workflow, its location, and additional metadata.\nUsing the deployment.yaml file, we can ship the package to the Prefect API, which creates a deployment object and stores it in the Prefect database. The deployment is now ready to be executed.\nHowever, simply deploying the workflow doesn‚Äôt start its execution.\nIn Prefect, we have the concept of work-pools and agents/workers. When a deployed workflow runs, it creates a flow run which is similar to what we did previously when running a Prefect flow.\nHowever, during deployment, the flow run is submitted to a specific work-pool for scheduling. Think of the work-pool as a staging area where the flow run waits to be picked up by an agent/worker. (Imagine package lying in the warehouse waiting for the courier to be picked).\nAn agent or worker (courier), operating within the execution environment, polls the work-pool for new runs to execute.\nOnce an agent/worker picks up a flow run, it proceeds to execute it and reports the status back to the Prefect API.\n\nIt might seem like a lot of steps, but it‚Äôs quite simple once you get the hang of it. We‚Äôll be going through each step in detail in the next section."
  },
  {
    "objectID": "posts/mlops/prefect-deployment.html#deployment-in-action",
    "href": "posts/mlops/prefect-deployment.html#deployment-in-action",
    "title": "Prefect Deployment",
    "section": "Deployment in Action",
    "text": "Deployment in Action\nTo demonstrate deployment, we‚Äôll be using a simple prefect flow that has a flow with one task. The task simply prints a message to the console. You can find the code for the flow below.\n\n\n\"prefect_demo.py\n\nfrom prefect import flow, task\n\n@task(name=\"say_hello\", log_prints=True)\ndef say_hello():\n    print(\"Hello, I'm a Task!\")\n\n@flow(name=\"hello-flow\", log_prints=True)\ndef flow_hello():\n    print(\"Hello, I'm a Flow!\")\n    print(\"I'm about to call a Task...\")\n    say_hello()\n\nif __name__ == \"__main__\":\n    flow_hello()\n\nThe flow is simple and will help us focus more on the deployment process rather than understanding what the flow does. Let‚Äôs get started.\n\nStep 1: Start a Prefect server\nBefore we can deploy our flow, we need to start a Prefect server. We‚Äôll be using the Prefect CLI command to start the server. Run the following command in your terminal.\nprefect server start\n\n\nStep 2: Create a yaml file for the deployment\nWe‚Äôll use the prefect deployment build command to create a deployment definition yaml file. Run the Prefect CLI command from the folder containing your flow script and any dependencies of the script.\nprefect deployment build [OPTIONS] PATH\nPath to the flow is specified in the format path-to-script:flow-function-name ‚Äî The path and filename of the flow script file, a colon, then the name of the entrypoint flow function.\nFor our example, the command will be:\nprefect deployment build -n flow_test -p default-agent-pool -q test prefect_demo.py:flow_hello\nLet‚Äôs break down the command:\n\n-n specifies the name of the deployment. We‚Äôve named our deployment flow_test.\n-p specifies the name of the work-pool. Prefect provides a default work-pool named default-agent-pool.\n-q specifies the name of the work queue. We‚Äôve named our queue test.\nprefect_demo.py:flow_hello specifies the path to the flow script, a colon, then the name of the flow function.\n\nWhen you run this command, Prefect:\n\nCreates a flow_hello-deployment.yaml file for your deployment based on your flow code and options. Usually the format is &lt;flow_name&gt;-deployment.yaml. You can specify a different name using the --output option.\nUploads your flow files to the configured storage location (local by default). (We‚Äôll cover storage in a later post).\nSubmit your deployment to the work queue test. The work queue test will be created if it doesn‚Äôt exist.\n\nYou can find the yaml file in the same folder as your flow script.\n\n\nWork Queues\n\nImagine work-pool has a big highway and work-queues are lanes on the highway. Each work-queue is a separate lane on the highway. When a flow run is submitted to a work-pool, it is placed in a work-queue.\nAs different lanes in a highway have different priorities, and purpose so do work-queues. For example, you might have a work-queue for high priority flows and another for low priority flows. You can seperate your test and production flows by using different work-queues.\nBelow you‚Äôll find a brief overview of work-queues and I highly recommend you read the official documentation\n\nWork pools have a default queue where all work is sent by default, but additional queues can be added for finer control over work delivery.\nWork queues have priorities indicated by unique positive integers, with lower numbers having higher priority. New queues can be added without affecting the priority of existing queues.\nWork queues can also have their own concurrency limits, allowing for precise control over the execution of work. However, all queues are subject to the global work pool concurrency limit.\n\n\n\nStep 3: Create a deployment on Prefect API\nNow that we have the deployment.yaml file, we can create a deployment on the Prefect API. To do this, we‚Äôll use the prefect deployment apply Prefect CLI command.\nprefect deployment apply PATH_TO_YAML_FILE\nFor our example, the command will be:\nprefect deployment apply flow_hello-deployment.yaml\nOnce the deployment is created, you‚Äôll see it in the CLI and the Prefect UI. Run the prefect deployment ls command to see a list of all deployments.\nprefect deployment ls\n\n\nTo view the work-pool and work-queue in the Prefect UI, navigate to the Work Pools section and click on the default-agent-pool work-pool. Click on the Work Queues tab to view the test work-queue. You‚Äôll also notice a default work queue. This is the default work queue that Prefect creates when you create any work-pool.\n\n\n\nStep 4: Start an agent\nAgent processes are lightweight polling services that regularly check a work-pool for scheduled work and execute the corresponding flow runs.\nBy default, agents poll for work every 15 seconds. You can adjust this interval by configuring the PREFECT_AGENT_QUERY_INTERVAL setting in the profile settings.\nYou can have multiple agent processes running for a single work pool. Each agent process sends a unique ID to the server, which helps distinguish them from one another and informs users about the number of active agents.\nWe‚Äôll use the prefect agent start command to start an agent.\nprefect agent start -p [WORK_POOL_NAME]\nFor our example, the command will be:\nprefect agent start -p default-agent-pool\n\n\nStep 5: Execute the deployment\nNow that we have an agent running, we can execute the deployment. To do this, we‚Äôll use the prefect deployment run command.\nprefect deployment run [OPTIONS] DEPLOYMENT_NAME\nIf you don‚Äôt know the name of the deployment, you can use the prefect deployment ls command to get a list of all deployments.\nFor our example, the command will be:\nprefect deployment run hello-flow/flow_test\nAfter running the command, you‚Äôll see the flow run in the Prefect UI. Navigate to the Flow Runs section to view the flow run. Click on the flow run to view the logs.\n\nYou can also run the flow from the Prefect UI from the Flow section.\n\nCongratulations! You‚Äôve successfully deployed and executed your first Prefect flow. Yet there‚Äôs so much more to cover like schedules, storage, agents, workers, and more.\nCongratulations! üéâ You‚Äôve successfully deployed and executed your first Prefect flow. Yet there‚Äôs so much more to cover like üìÖ schedules, üì¶ storage, üí™ agents, workers, and more."
  },
  {
    "objectID": "posts/mlops/prefect-deployment.html#deployment.yaml-file",
    "href": "posts/mlops/prefect-deployment.html#deployment.yaml-file",
    "title": "Prefect Deployment",
    "section": "deployment.yaml file",
    "text": "deployment.yaml file\nThe YAML file for a deployment contains extra settings required to create the deployment on the server.\nA single flow can have multiple deployments created for it, each with different schedules, tags, and other configurations.\nTo achieve this, you can have multiple deployment YAML files referencing the same flow definition, each specifying distinct settings. The only rule is that each deployment must have a unique name.\n\nEach deployment is linked to a specific flow, but a flow can be referenced by multiple deployments.\nDeployments are uniquely identified based on the combination of the flow name and deployment name.\nThis allows you to execute a single flow with various parameters, on multiple schedules, and in different environments. It also enables you to run different versions of the same flow for testing or production purposes."
  },
  {
    "objectID": "posts/mlops/prefect-deployment.html#schedules",
    "href": "posts/mlops/prefect-deployment.html#schedules",
    "title": "Prefect Deployment",
    "section": "Schedules",
    "text": "Schedules\nSchedules allow you to instruct the Prefect API to automatically generate new flow runs for you at regular intervals.\nYou can attach a schedule to any flow deployment. The Prefect Scheduler service regularly checks each deployment and generates new flow runs based on the schedule defined for that deployment.\nThere are four recommended ways to create a schedule for a deployment:\n\nUse the Prefect UI\nUse the cron, interval, or rrule flags with the CLI deployment build command\nUse the schedule parameter with a Python deployment file\nManually edit the deployment YAML file‚Äôs schedule section\n\nPrefect offers different types of schedules that cater to various needs and provide a high level of customization:\n\nCron: This type is well-suited for users who have prior experience with cron and want to leverage its functionality.\nInterval: Ideal for deployments that require a consistent cadence of execution, irrespective of specific timestamps.\nRRule: Designed for deployments that rely on calendar-based logic, allowing for simple recurring schedules, irregular intervals, exclusions, or adjustments based on specific days of the month.\n\nThese schedule types offer flexibility and can accommodate a wide range of scheduling requirements.\nWe‚Äôll go through an example of scheduling a flow run in the section below."
  },
  {
    "objectID": "posts/mlops/prefect-deployment.html#agents-and-workers",
    "href": "posts/mlops/prefect-deployment.html#agents-and-workers",
    "title": "Prefect Deployment",
    "section": "Agents and Workers",
    "text": "Agents and Workers\nWork-pools serve as a way to organize work that agents or workers pick up for execution. The coordination between deployments and agents happens through a shared work-pool name.\nIn case you want to create a new work pool, you would have to choose the type of the work pool. Type defines the type of infrastructure that can execute runs from this work pool.\nThat‚Äôs where which polling service (Agent / Worker) is decided. View the table below to understand the difference between the two.\n\n\n\n\n\n\n\n\nWork-Pool Type\nAgent/Worker\nDescription\n\n\n\n\nAmazon Elastic Container Service\nWorker\nExecutes flow runs as ECS tasks\n\n\nAzure Container Service\nWorker\nExecutes flow runs in ACI containers\n\n\nGoogle Cloud Run\nWorker\nExecutes flow runs as Google Cloud Run jobs\n\n\nDocker\nWorker\nExecutes flow runs within Docker containers\n\n\nKubernetes\nWorker\nExecutes flow runs as Kubernetes jobs\n\n\nProcess\nWorker\nExecutes flow runs in subprocesses\n\n\nPrefect Agent\nAgent\nExecutes flow runs in subprocesses\n\n\n\nWe have seen an example of an agent in the previous section. Let‚Äôs look at an example of a worker."
  },
  {
    "objectID": "posts/mlops/prefect-deployment.html#worker",
    "href": "posts/mlops/prefect-deployment.html#worker",
    "title": "Prefect Deployment",
    "section": "Worker",
    "text": "Worker\n\n\n\n\n\n\nWarning\n\n\n\nWorkers are a beta feature and are subject to change in future releases.\n\n\nWorkers are similar to Agents in that they are long-running processes that poll for work i.e., fetch scheduled runs from a work pool and carry out their execution.\nHowever, workers provide the advantage of more control over infrastructure configuration and the capability to route work to specific types of execution environments.\nEach worker is assigned a type that corresponds to the specific execution environment where it will submit flow runs. Workers can only join work pools that match their designated type. Consequently, when deployments are associated with a work pool, you can determine the execution environment in which scheduled flow runs for that deployment will be executed.\nAbove table shows the different types of workers available. Prefect also provides a way to create your own worker type."
  },
  {
    "objectID": "posts/mlops/prefect-deployment.html#scheduled-flow-run-example-with-worker",
    "href": "posts/mlops/prefect-deployment.html#scheduled-flow-run-example-with-worker",
    "title": "Prefect Deployment",
    "section": "Scheduled Flow Run Example with Worker",
    "text": "Scheduled Flow Run Example with Worker\n\nStep 1: Create a new work-pool.\nprefect work-pool create [OPTIONS] NAME\nFor our example, the command will be:\nprefect work-pool create test-pool -t process\n\n-t or --type flag is used to specify the type of work-pool. In our case, we are using the process type.\n\nOn CLI you can use the command prefect work-pool ls to list all the work-pools.\n\n\n\n\nStep 2: Create a new yaml file.\nprefect deployment build -n demo_schedule -p test-pool -q demo --cron \"0 0 * * *\" --output demo_schedule.yaml prefect_demo.py:flow_hello\n\n--cron flag is used to specify the cron schedule.\n\nThis create flow runs for this deployment every day at midnight.\n\n\nStep 3: Apply the deployment.\nprefect deployment apply demo_schedule.yaml\n\n\n\n\nStep 4: Start the worker.\nprefect worker start -p test-pool\n\n\nStep 5: Execute the deployment.\nEither run the deployment from the Prefect UI or use the CLI command.\n\n\n\n\n\n\nNote\n\n\n\nOne observation you might have made is that we are using the same flow for both the deployments but with different deployments. This is because we want to run the first flow manually through CLI or UI and second on a schedule.\n\n\nFirst can be used to test the flow and second can be used to run the flow on a schedule. It completely depends on your use case.\nI hope this provides a comprehensive overview of Prefect deployment and steps to deploy a flow. Hush‚Ä¶ there‚Äôs an abundance yet to explore."
  },
  {
    "objectID": "posts/mlops/prefect-deployment.html#storage",
    "href": "posts/mlops/prefect-deployment.html#storage",
    "title": "Prefect Deployment",
    "section": "Storage",
    "text": "Storage\nPrefect follows a hybrid model where your flow code stays within your storage and execution infrastructure and never lives on the Prefect server or database. What does that mean?\nIt means that there‚Äôs always a boundary between your code, your private infrastructure, and the Prefect backend. You can use your existing storage infrastructure to store your flow code. Prefect supports a wide range of storage options like GitHub, local storage, Bitbucket, Amazon S3, Google Cloud Storage, Azure Storage, and more.\nTill now we have been using local storage to store our flow code and run the Prefect Server. However, you might choose to store the flow code in a GitHub repository and run the Prefect Server on your local machine. Prefect supports this as well.\n\nWhile creating a deployment, you can specify the storage location of your flow code. This let‚Äôs Prefect Agent/Worker know where to look for the flow code.\nOne cool thing about it is if you make changes to your flow code, you don‚Äôt have to create a new deployment. Prefect Agent/Worker will automatically pick up the changes and execute the flow with the new code.\n\n\n\n\n\n\nWarning\n\n\n\nHowever, changes to the infrastructure code will require a new deployment. Such as entrypoint. This part of changing the infrastructure code is experimental for me, I have to try it out more to better understand it.\n\n\nThis post has been a long one. Hence, I have not covered storage in detail. You can refer to the official Prefect documentation on this topic for more details. Hopefully, in the next post I will cover the remaining topics.\nWait, there is ONE more thing!"
  },
  {
    "objectID": "posts/mlops/prefect-deployment.html#prefect-projects",
    "href": "posts/mlops/prefect-deployment.html#prefect-projects",
    "title": "Prefect Deployment",
    "section": "Prefect Projects",
    "text": "Prefect Projects\n\n\n\n\n\n\nWarning\n\n\n\nProjects are a beta feature and are subject to change in future releases.\n\n\nPrefect Projects is a new feature that describe how to prepare one or more flow deployments. You can create a project using the Prefect CLI command prefect project init.\nI would encourage you to follow along in a new directory so as to seperate what we did so far from the project.\n# Create a new directory\nmkdir project\n\n# Change directory\ncd project\n\n# Create a new Prefect project\nprefect project init\nOnce you run the command, three files and a directory will be created - deployment.yaml, prefect.yaml, .prefectignore, and .prefect/ directory.\n\ndeployment.yaml: a YAML file describing base settings for deployments produced from this project\nprefect.yaml: a YAML file describing procedural steps for preparing a deployment from this project, as well as instructions for preparing the execution environment for a deployment run\n.prefect/: a hidden directory where Prefect will store workflow metadata\n.prefectignore: a file that specifies files and directories to ignore when preparing a deployment\n\nBelow we‚Äôll go through an example of how to use Prefect Projects to create a deployment.\n\n\nStep 1: Create a new project.\nprefect project init\n\n\nStep 2: Create a flow.\nWe‚Äôll be utilizing the same flow we created earlier but with some changes. The flow will take a name as an argument, which are called parameters to a flow in Prefect.\n\n\nprefect_demo.py\n\nimport argparse\nfrom prefect import flow, task\n\n@task(name=\"say_hello\", log_prints=True)\ndef say_hello():\n    print(\"Hello, I'm a Task!\")\n\n@flow(name=\"hello-flow\", log_prints=True)\ndef flow_hello(name: str):\n    print(f\"Hello {name}, I'm a Flow!\")\n    print(\"I'm about to call a Task...\")\n    say_hello()\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--name\", help=\"Your name\", default=\"World\")\n    args = parser.parse_args()    \n    \n    flow_hello(args.name)\n\n\n\nStep 3: Start the Prefect Server.\nprefect server start\n\n\nStep 4: Start the Prefect Worker.\nprefect worker start -p test-pool\n\n\nStep 5: Configure the deployment.yaml file.\nPreviously, we created seperated files for creating a deployment and executing it.\nHowever, with Prefect Projects, we can do both in a single file. The deployment.yaml file is used to configure the deployment. Let‚Äôs look at the contents of the file.\n\n\ndeployment.yaml\n\ndeployments:\n- # base metadata\n  name: manual-deployment\n  tags: [\"test\"]\n  description: \"Trigger deployment using `run` CLI command or Prefect UI\"\n  \n  # flow-specific fields\n  entrypoint: prefect_demo.py:flow_hello\n  parameters:\n    name: \"Sagar\"\n  \n  # infra-specific fields\n  work_pool:\n    name: test-pool\n    work_queue_name: demo\n\n- # base metadata\n  name: scheduled-deployment\n  tags: [\"dev\"]\n  description: \"Trigger deployment using a Schedule\"\n  schedule:\n    cron: 0 0 * * *\n    timezone: America/Chicago\n  \n  # flow-specific fields\n  entrypoint: prefect_demo.py:flow_hello\n  parameters:\n    name: \"World\"\n  \n  # infra-specific fields\n  work_pool:\n    name: test-pool\n    work_queue_name: demo\n\n\n\nStep 6: Deploy your flow.\nThis file has two deployment declarations, each referencing a same flow in the project but with different parameters and schedule. Each deployment declaration has a unique name field and can be deployed individually by using the --name flag when deploying.\n\n\n\n\n\n\nTip\n\n\n\nYou also have the freedom to deploy different flows in the same yaml file. Example, we already have one flow file called prefect_demo.py. We can create another flow file called prefect_demo_2.py and reference it in the deployment.yaml file. Give the appropriate entrypoint and parameters and you are good to go.\n\n\nThis method of declaring multiple deployments allows the configuration for all deployments within a project to be version controlled and deployed with a single command.\nYou can deploy a single deployment by using the --name flag.\nprefect deploy --name manual-deployment\nYou can also deploy multiple deployments by providing multiple --name flags.\nprefect deploy --name manual-deployment --name scheduled-deployment\nOr, you can deploy all the deployments by using the --all flag.\nprefect deploy --all\nFor our example, we‚Äôll deploy both the deployments.\nprefect deploy --all\nYou might have noticed that we are using the prefect deploy command instead of prefect deployment command. This is because while using Prefect Projects, prefect deploy command under the hood performs multiple steps like build, push, pull, and applies the deployment. You can read more about it here.\n\n\n\n\nStep 7: Execute the deployments.\nYou can execute a deployment either through the Prefect UI or using the CLI command.\n# Using the default parameters\nprefect deployment run hello-flow/manual-deployment\n\n# Using the custom parameters\nprefect deployment run hello-flow/manual-deployment --param name=\"Prefect\"\nOutput of the CLI command with default parameters.\n\nOutput of the CLI command with custom parameters.\n\nCongratulations! üéâ You have successfully created your first Prefect Project.\nPrefect Project have a lot more to offer.\nI encourage you to read about Templating Options for deployment.yaml file that lets you refer dynamic value to the file. Also, read more about prefect.yaml file here."
  },
  {
    "objectID": "posts/mlops/prefect-deployment.html#references",
    "href": "posts/mlops/prefect-deployment.html#references",
    "title": "Prefect Deployment",
    "section": "References",
    "text": "References\n\nDeployments\nWork Pools, Workers & Agents\nStorage\nSchedules\nProjects\nDeployment Tutorial\nEnd-to-End Example Image"
  },
  {
    "objectID": "posts/mlops/prefect-deployment.html#conclusion",
    "href": "posts/mlops/prefect-deployment.html#conclusion",
    "title": "Prefect Deployment",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we covered the basics of Prefect deployment and how to deploy a flow. We also covered Prefect Projects and how to use it to create a deployment. In the next post, we‚Äôll cover storgae and deployment on Prefect Cloud.\nThank you for reading and I hope you found this notebook helpful. üëè Upvote if you liked it, üí¨ comment if you loved it. Hope to see you guys in the next one. ‚úåÔ∏è Peace!"
  },
  {
    "objectID": "posts/mlops/aws-deployment-lambda-kinesis.html",
    "href": "posts/mlops/aws-deployment-lambda-kinesis.html",
    "title": "Deployment of Dockerized ML Models with AWS Kinesis and Lambda",
    "section": "",
    "text": "This post explains the deployment of a Dockerized ML model using AWS Kinesis as the event stream and AWS Lambda as the consumer. The Dockerized ML model will be registered on AWS ECR.\nThe workflow involves triggering an AWS Lambda function upon receiving a request on AWS Kinesis, which will then invoke the ML model and push the results to another AWS Kinesis stream.\nThe interaction with AWS services and resources will be performed from a local machine using the AWS CLI."
  },
  {
    "objectID": "posts/mlops/aws-deployment-lambda-kinesis.html#prerequisites",
    "href": "posts/mlops/aws-deployment-lambda-kinesis.html#prerequisites",
    "title": "Deployment of Dockerized ML Models with AWS Kinesis and Lambda",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo follow along with this post, you‚Äôll need the AWS Command Line Interface (AWS CLI) version 2. To check if you have AWS CLI installed, run the following command:\naws --version\n\n\n\n\n\n\nImportant\n\n\n\nUse the SAME region across all the services and resources."
  },
  {
    "objectID": "posts/mlops/aws-deployment-lambda-kinesis.html#create-an-iam-role",
    "href": "posts/mlops/aws-deployment-lambda-kinesis.html#create-an-iam-role",
    "title": "Deployment of Dockerized ML Models with AWS Kinesis and Lambda",
    "section": "Create an IAM role",
    "text": "Create an IAM role\nCreate an IAM role that you can use to access AWS services and resources. Follow these steps:\n\nSign in to the AWS Management Console and open the roles page in the IAM console.\nClick on Create role.\nGive the role the following properties:\n\nTrusted entity: AWS service\nUse case: Lambda\nPermissions: AWSLambdaKinesisExecutionRole\nRole name: lamdba-kinesis-role\n\n\nThe AWSLambdaKinesisExecutionRole policy has the permissions that the function needs to read items from Kinesis and write logs to CloudWatch Logs."
  },
  {
    "objectID": "posts/mlops/aws-deployment-lambda-kinesis.html#create-a-lambda-function",
    "href": "posts/mlops/aws-deployment-lambda-kinesis.html#create-a-lambda-function",
    "title": "Deployment of Dockerized ML Models with AWS Kinesis and Lambda",
    "section": "Create a Lambda function",
    "text": "Create a Lambda function\nWe‚Äôll create a Lambda function that reads records from a Kinesis stream and writes them to CloudWatch Logs. Follow these steps:\n\nOpen the Lambda console in the AWS Management Console.\nChoose Create function.\nGive the function the following properties:\n\nSelect Author from scratch.\nFunction name: ride-duration-prediction-test\nRuntime: Python 3.9\nPermissions: Toggle the Change default execution role and select Use an existing role. Choose the lamdba-kinesis-role role that you created earlier.\n\nChoose Create function.\n\nOnce the function is created, you‚Äôll see code editor with a file name lambda_function.py created for you.\nInside the file you might have noticed the lambda_handler function. The handler function is the entry point for Lambda functions. It‚Äôs the python function that get‚Äôs executed when the Lambda function is invoked.\nThe handler function always takes two arguments: event and context.\n\nEvent\nThe event argument is the data that‚Äôs passed to the function when it‚Äôs invoked. For example, if the lambda function is invoked by an HTTP request, the event argument will contain information about the HTTP request.\nThink of the event argument as the input to the function, the function then processes the input based on some logic and may or may not return an output. The functions can be triggered by different events such as an HTTP request, a message in a queue, a file upload to S3, etc.\n\n\nContext\nThe context argument provides information about the function and the execution environment. For example, the context argument contains the name of the function, the function version, the execution time, etc.\ncontext is a Python object that implements methods and has properties that you can use to get information about the function and the execution environment. For example, you can use the context.get_remaining_time_in_millis() method to get the remaining execution time for the function in milliseconds.\nLet‚Äôs add some code to the lambda_handler function to log the event and context arguments:\nimport json\n\ndef lambda_handler(event, context):\n    print(\"Received event: \" + json.dumps(event))\n    print(\"Remaining time (ms): \" + str(context.get_remaining_time_in_millis()))\n    return {\n        'statusCode': 200,\n        'body': json.dumps('Hello from Lambda!')\n    }\n\n\n\n\n\n\nNote\n\n\n\nEverytime you make changes to the Lambda function, you need to deploy the changes. You can deploy the changes by clicking on the Deploy button in the top right corner of the Lambda console.\n\n\nTo test the function, click on the Test button and create a new test event. Give the test event a name, for example test-event.\nYou‚Äôll notice there is a drop down for the event template. For now we‚Äôll use the default hello-world template, however I encourage you to explore the other templates.\nClick on the Save button and then click on the Test button again."
  },
  {
    "objectID": "posts/mlops/aws-deployment-lambda-kinesis.html#create-a-kinesis-stream",
    "href": "posts/mlops/aws-deployment-lambda-kinesis.html#create-a-kinesis-stream",
    "title": "Deployment of Dockerized ML Models with AWS Kinesis and Lambda",
    "section": "Create a Kinesis stream",
    "text": "Create a Kinesis stream\nCreate a Kinesis stream that you can use to send data to the Lambda function. Follow these steps:\n\nOpen the Kinesis console in the AWS Management Console.\nChoose Kinesis Data Streams and then choose Create data stream.\nGive the stream the following properties:\n\nStream name: ride-event\nCapacity mode: Provisioned\nProvisioned shards: 1\n\nChoose Create data stream."
  },
  {
    "objectID": "posts/mlops/aws-deployment-lambda-kinesis.html#add-a-trigger-to-the-lambda-function",
    "href": "posts/mlops/aws-deployment-lambda-kinesis.html#add-a-trigger-to-the-lambda-function",
    "title": "Deployment of Dockerized ML Models with AWS Kinesis and Lambda",
    "section": "Add a trigger to the Lambda function",
    "text": "Add a trigger to the Lambda function\nA trigger is a configurable resource that enables an AWS service to invoke your function in response to specific events or conditions. Your function can be associated with multiple triggers, allowing it to respond to various event-driven scenarios.\nAdd a trigger to the Lambda function that we created earlier. Follow these steps:\n\nNavigate back to the ride-duration-prediction-test Lambda function in the Lambda console.\nIn the Function overview section, choose Add trigger button.\nChoose Kinesis from the list of triggers.\nGive the trigger the following properties:\n\nKinesis stream: ride-event\n\nChoose Add."
  },
  {
    "objectID": "posts/mlops/aws-deployment-lambda-kinesis.html#create-a-iam-user-and-policy",
    "href": "posts/mlops/aws-deployment-lambda-kinesis.html#create-a-iam-user-and-policy",
    "title": "Deployment of Dockerized ML Models with AWS Kinesis and Lambda",
    "section": "Create a IAM user and policy",
    "text": "Create a IAM user and policy\nTo interact with AWS services and resources from your local machine we need to create an IAM user and policy.\nThe IAM user represents the human user or workload who uses the IAM user to interact with AWS. A user in AWS consists of a name and credentials.\nThe credentials are used to authenticate the user when they interact with AWS.\nThe IAM policy defines the permissions that the user has to access AWS services and resources. We attach the policy to the user to grant the user permissions to access AWS services and resources.\nFollow these steps to create an IAM user and policy:\n\nPolicy\n\nSign in to the AWS Management Console and open the policies page in the IAM console.\nChoose Create policy.\nSearch for kinesis and click on it.\nUnder the Access level section, for Write select the PutRecord and PutRecords actions.\nUnder the Resources section, select Specific and then Add ARN.\nChoose the Text tab and Copy-Paste the ARN of the Kinesis stream that you created earlier. You can find the ARN of the Kinesis stream in the Kinesis console under the Data stream summary section.\nChoose Add ARNs and then Next.\nGive the policy the following properties:\n\nName: kinesis-write-policy\nDescription: Allows write access to the Kinesis stream\n\nChoose Create policy.\n\n\n\nUser\n\nSign in to the AWS Management Console and open the users page in the IAM console.\nChoose Add users.\nName the user kinesis-user.\nSelect Add user to group and then Create group.\nName the group kinesis-write-group.\nSearch for the policy that you created earlier and select it. In this case it‚Äôs kinesis-write-policy.\nChoose Create user group.\nSelect the user group that you created earlier and choose Next.\nChoose Create user.\n\n\n\nCreate access keys for the IAM user\nSelect the IAM user that you created earlier.\n\nChoose Security credentials.\nUnder the Access keys section, choose Create access key.\nChoose Other and then Next.\nChoose Create access key.\n\n\n\n\n\n\n\nNote\n\n\n\nMake sure you download the access key file. You will not be able to access the secret access key again after you close the dialog box.\n\n\nNext we‚Äôll configure the AWS CLI to use the access keys that we just created.\n\n\n\n\n\n\nNote\n\n\n\nMake sure you have the AWS CLI installed.\n\n\nRun the following command to configure the AWS CLI:\naws configure\nEnter the access key ID and secret access key that you downloaded earlier. For the default region name, enter the region that you created the Kinesis stream in."
  },
  {
    "objectID": "posts/mlops/aws-deployment-lambda-kinesis.html#send-data-to-the-kinesis-stream",
    "href": "posts/mlops/aws-deployment-lambda-kinesis.html#send-data-to-the-kinesis-stream",
    "title": "Deployment of Dockerized ML Models with AWS Kinesis and Lambda",
    "section": "Send data to the Kinesis stream",
    "text": "Send data to the Kinesis stream\nNow that we have a Lambda function and a Kinesis stream, let‚Äôs send some data to the Kinesis stream. Follow these steps:\nOpen terminal or command prompt and run the following command to send data to the Kinesis stream:\nKINESIS_STREAM_NAME=ride-event\naws kinesis put-record \\\n    --stream-name ${KINESIS_STREAM_NAME} \\\n    --partition-key 1 \\\n    --data \"Hello, this is a test.\" \\\n    --cli-binary-format raw-in-base64-out\nYou should see the following output:\n{\n    \"ShardId\": \"shardId-000000000000\",\n    \"SequenceNumber\": \"49605507193692568746561138951989197221575295936257281026\"\n}\nPress q to exit the output.\n\nView the logs\nNow that we have sent some data to the Kinesis stream, let‚Äôs view the logs in CloudWatch Logs. Follow these steps:\n\nNavigate back to the ride-duration-prediction-test Lambda function in the Lambda console.\nChoose the Monitor tab and then choose Logs.\nChoose View CloudWatch Logs to view the logs in CloudWatch Logs.\nUnder the Log streams tab, choose the latest log stream.\n\nYou‚Äôll see the log messages. Find the log message that says Received event: and you‚Äôll see the request body that we sent to the Kinesis stream. It should look something like this:\nReceived event: {\n    \"Records\": [\n        {\n            \"kinesis\": {\n                \"kinesisSchemaVersion\": \"1.0\",\n                \"partitionKey\": \"1\",\n                \"sequenceNumber\": \"49641836346418405924036776538604986498724303454435540994\",\n                \"data\": \"SGVsbG8sIFRoaXMgaXMgYSB0ZXN0IG1lc3NhZ2U=\",\n                \"approximateArrivalTimestamp\": 1687115491.219\n            },\n            \"eventSource\": \"aws:kinesis\",\n            \"eventVersion\": \"1.0\",\n            \"eventID\": \"shardId-000000000000:49641836346418405924036776538604986498724303454435540994\",\n            \"eventName\": \"aws:kinesis:record\",\n            \"invokeIdentityArn\": \"arn:aws:iam::247894370182:role/lamdba-kinesis-role\",\n            \"awsRegion\": \"us-east-1\",\n            \"eventSourceARN\": \"arn:aws:kinesis:us-east-1:247894370182:stream/ride-event\"\n        }\n    ]\n}\nYou‚Äôll notice that the data is base64 encoded. Let‚Äôs decode the data and print it to the logs. Add the following code to the lambda_handler function:\n# Add the following code to the lambda_handler function\nimport base64\n\nrecord = event['Records'][0]\nprint(\"Decoded message: \" + base64.b64decode(record['kinesis']['data']).decode('utf-8'))\nDeploy the changes and send some data to the Kinesis stream again. You should see the decoded message in the logs.\n\n\n\n\n\n\nNote\n\n\n\nEverytime you make changes and deploy the Lambda function, there will be a new entry in the Log streams tab. Make sure you choose the latest log stream.\n\n\nUsually after the model prediction is made, we would want to send the prediction to another service. In the next section, we‚Äôll send the prediction to another Kinesis stream."
  },
  {
    "objectID": "posts/mlops/aws-deployment-lambda-kinesis.html#send-the-prediction-to-another-kinesis-stream",
    "href": "posts/mlops/aws-deployment-lambda-kinesis.html#send-the-prediction-to-another-kinesis-stream",
    "title": "Deployment of Dockerized ML Models with AWS Kinesis and Lambda",
    "section": "Send the prediction to another Kinesis stream",
    "text": "Send the prediction to another Kinesis stream\nIn this section, we‚Äôll send the prediction to another Kinesis stream. Follow these steps:\n\nOpen the Kinesis console in the AWS Management Console.\nChoose Kinesis Data Streams and then choose Create data stream.\nGive the stream the following properties:\n\nStream name: ride-prediction\nCapacity mode: Provisioned\nProvisioned shards: 1\n\nChoose Create data stream.\n\nNow, that we have created the Kinesis stream, we want the lambda function to send the prediction to the Kinesis stream.\nHowever, lambda function doesn‚Äôt have the permission to write to the Kinesis stream. Let‚Äôs add the permission to the lambda-kinesis-role role.\nFollow these steps to create a policy and attach it to the role:\n\nCreate a policy\n\nSign in to the AWS Management Console and open the policies page in the IAM console.\nChoose Create policy.\nSearch for kinesis and click on it.\nUnder the Access level section, for Write select the PutRecord and PutRecords actions.\nUnder the Resources section, select Specific and then Add ARN.\nChoose the Text tab and Copy-Paste the ARN of the Kinesis stream for ride-prediction. You can find the ARN of the Kinesis stream in the Kinesis console under the Data stream summary section.\nChoose Add ARNs and then Next.\nGive the policy the following properties:\n\nName: kinesis-write-policy-prediction\nDescription: Allows write access to the Prediction Kinesis stream\n\nChoose Create policy.\n\n\n\nAttach the policy to the role\n\nSign in to the AWS Management Console and open the roles page in the IAM console.\nSearch for lambda-kinesis-role and click on it.\nUnder the Add Permissions dropdown, click on Attach policies.\nSearch for kinesis-write-policy-prediction and click on it. Chooes Add Permissions.\n\nBefore moving forward to the next section, let‚Äôs review what we have done so far."
  },
  {
    "objectID": "posts/mlops/aws-deployment-lambda-kinesis.html#review-what-we-have-done-so-far-optional",
    "href": "posts/mlops/aws-deployment-lambda-kinesis.html#review-what-we-have-done-so-far-optional",
    "title": "Deployment of Dockerized ML Models with AWS Kinesis and Lambda",
    "section": "Review what we have done so far [Optional]",
    "text": "Review what we have done so far [Optional]\nI‚Äôll divide the review into three parts:\n\nPart 1: Create a Lambda function and a Kinesis stream\n\nWe created a Lambda function and explained how it works.\nWe also tried to modify the Lambda function and tested it.\nWe created a Kinesis stream that will receive the data.\nWe added a Kinesis trigger to the Lambda function, so that the Lambda function is invoked when data is sent to the Kinesis stream.\nHowever, the Lambda function doesn‚Äôt have the permission to read from the Kinesis stream. So, we added the AWSLambdaKinesisExecutionRole permission to the lambda-kinesis-role role.\n\n\n\nPart 2: Send data to the Kinesis stream\n\nTo test what we have done in part 1, we sent some data to the Kinesis stream.\nSince, we are performing this action from our local machine, we need to have the AWS CLI installed and configured.\nFor a individual (user) to interact with the AWS resources, we created an IAM user. However, what are the permissions that we gave to the IAM user? We gave the permissions to send data to the Kinesis stream. We did this by attaching the kinesis-write-policy policy to the IAM user, allowing the user to write on ride-event Kinesis stream.\nSimilar to how we use API key to authenticate ourselves to a service, we used the access key and secret key of the IAM user to authenticate ourselves to AWS.\nLastly, we used the AWS CLI to send data to the Kinesis stream and view the logs in CloudWatch Logs.\n\n\n\nPart 3: Send the prediction to another Kinesis stream\n\nWe created another Kinesis stream called ride-prediction which will receive the prediction from the Lambda function.\nHowever, does the Lambda function have the permission to write to the Kinesis stream? No, it doesn‚Äôt. So, we created a policy called kinesis-write-policy-prediction that allows the Lambda function to write to the Kinesis stream.\nWe attached this policy to the lambda-kinesis-role role.\nWe‚Äôll now update the Lambda function to send the prediction to the Kinesis stream."
  },
  {
    "objectID": "posts/mlops/aws-deployment-lambda-kinesis.html#update-the-lambda-function-to-send-the-prediction-to-the-kinesis-stream",
    "href": "posts/mlops/aws-deployment-lambda-kinesis.html#update-the-lambda-function-to-send-the-prediction-to-the-kinesis-stream",
    "title": "Deployment of Dockerized ML Models with AWS Kinesis and Lambda",
    "section": "Update the Lambda function to send the prediction to the Kinesis stream",
    "text": "Update the Lambda function to send the prediction to the Kinesis stream\nNow that we have created the policy and attached it to the role, let‚Äôs update the lambda function to send the prediction to the Kinesis stream.\nimport os\nimport json\nimport boto3\nimport base64\n\n# Create a Kinesis client\nkinesis_client = boto3.client('kinesis')\n\n# Get the name of the Kinesis stream from the environment variable\nPREDICTIONS_STREAM_NAME = os.getenv('PREDICTIONS_STREAM_NAME')\n\ndef lambda_handler(event, context):\n    print(\"Received event: \" + json.dumps(event))\n    record = event['Records'][0]\n    decoded_data = base64.b64decode(record['kinesis']['data']).decode('utf-8')\n    print(\"Decoded message: \" + decoded_data)\n    \n    ride_event = json.loads(decoded_data)\n    ride_id = ride_event['ride_id']\n    \n    # Create a dummy prediction\n    prediction = {\n        'model': 'ride_duration_prediction_model',\n        'version': '12345',\n        'prediction': {\n            'ride_id': ride_id,\n            'ride_duration': 10.0\n        }\n    }\n\n    # Write the prediction on the prediction-kinesis-stream\n    kinesis_client.put_record(\n        StreamName=PREDICTIONS_STREAM_NAME,\n        Data=json.dumps(prediction),\n        PartitionKey=str(ride_id)\n    )\n\n    return {\n        'statusCode': 200,\n        'body': json.dumps('Hello from Lambda!')\n    }\nWe used boto3 to create a Kinesis client. We then used the put_record method to write the prediction to the Kinesis stream. We used the PREDICTIONS_STREAM_NAME environment variable to get the name of the Kinesis stream.\nDeploy the Lambda function.\nSetup the environment variable by following these steps:\n\nGo to the Lambda console and click on the Configuration tab.\nUnder the Environment variables section, click on Edit.\nChoose Add environment variable.\nGive the environment variable the following properties:\n\nKey: PREDICTIONS_STREAM_NAME\nValue: ride-prediction\n\nClick on Save.\n\nNow, that we have updated the Lambda function, let‚Äôs test it.\n\nTest the Lambda function\nUse the following code to test the Lambda function:\nKINESIS_STREAM_NAME=ride-event\naws kinesis put-record \\\n    --stream-name ${KINESIS_STREAM_NAME} \\\n    --partition-key 1 \\\n    --data '{\"ride_id\": 1}' \\\n    --cli-binary-format raw-in-base64-out\nLike before you can view the logs in CloudWatch Logs.\n\n\nView the prediction in the Kinesis stream\nFor the user to view the prediction, we need to give the user the permission to read from the Kinesis stream. We‚Äôll do this by attaching the kinesis-read-policy-prediction policy to the kinesis-user.\nCreate the kinesis-read-policy-prediction policy by following these steps:\n\nSign in to the AWS Management Console and open the policies page in the IAM console.\nChoose Create policy.\nSearch for kinesis and click on it.\nUnder the Access level section, for Read select the GetRecords and GetShardIterator actions.\nUnder the Resources section, select Specific and then Add ARN.\nChoose the Text tab and Copy-Paste the ARN of the Kinesis stream for ride-prediction. You can find the ARN of the Kinesis stream in the Kinesis console under the Data stream summary section.\nChoose Add ARNs and then Next.\nGive the policy the following properties:\n\nName: kinesis-read-policy-prediction\nDescription: Allows read access to the Prediction Kinesis stream\n\nChoose Create policy.\n\nAttach the kinesis-read-policy-prediction policy to the kinesis-user by following these steps:\n\nSign in to the AWS Management Console and open the users page in the IAM console.\nChoose kinesis-user.\nChoose the Add Permissions tab and then Add permissions.\nChoose Attach existing policies directly.\nSearch for kinesis-read-policy-prediction and select it. Choose Next.\nChoose Add permissions.\n\nNow that we push the prediction to the prediction stream, we can view the prediction in the Kinesis stream. Use the following code in the terminal to view the prediction:\nKINESIS_STREAM_OUTPUT='ride-prediction'\n# Get the shard id, since we only have one shard we can hardcode it\nSHARD='shardId-000000000000'\n\n# Get the shard iterator\nSHARD_ITERATOR=$(aws kinesis \\\n    get-shard-iterator \\\n        --shard-id ${SHARD} \\\n        --shard-iterator-type TRIM_HORIZON \\\n        --stream-name ${KINESIS_STREAM_OUTPUT} \\\n        --query 'ShardIterator' \\\n)\n\n# Get the records\nRESULT=$(aws kinesis get-records --shard-iterator $SHARD_ITERATOR)\n\n# Print the data\necho ${RESULT} \n\n# Print the data in a more readable format for a specific record\necho ${RESULT} | jq -r '.Records[0].Data' | base64 --decode\nA shard iterator specifies the shard position from which to start reading data records sequentially.\nThere are many types of shard iterators. We are using the TRIM_HORIZON shard iterator type. This type causes the shard iterator to point to the last untrimmed record in the shard in the system, which is the oldest data record in the shard.\nFor more information on shard iterators, see here.\nWe get the data from the iterator and print it.\n\n\n\n\n\n\nTip\n\n\n\njq is a lightweight and flexible command-line JSON processor. If you don‚Äôt have jq installed on your machine, you can install it by following these instructions"
  },
  {
    "objectID": "posts/mlops/aws-deployment-lambda-kinesis.html#dockerize-the-prediction-service",
    "href": "posts/mlops/aws-deployment-lambda-kinesis.html#dockerize-the-prediction-service",
    "title": "Deployment of Dockerized ML Models with AWS Kinesis and Lambda",
    "section": "Dockerize the prediction service",
    "text": "Dockerize the prediction service\nUntil now, we have been using a dummy script to generate the prediction. In this section, we will upload model to S3 and then use it to generate the prediction. We will use Docker to containerize the prediction service.\n\nUpload the model to S3\nWe will use the lin_reg.bin file that I had created for you. You can find it here. Download the file and upload it to the s3://ride-prediction-model bucket. Follow these steps to upload the file:\n\nGo to the S3 console and click on the Create bucket button.\nGive the bucket the following properties:\n\nBucket name: ride-prediction-model\n\nClick on Create Bucket.\nClick on the bucket name.\nClick on the Upload button.\nClick on Add files and select the lin_reg.bin file.\nClick on Upload.\n\n\n\nUpdate the lambda function\nLocally create a lambda_function.py file with the following code:\nimport os\nimport json\nimport s3fs\nimport boto3\nimport base64\nimport pickle\n\ns3 = s3fs.S3FileSystem(anon=False)\nkinesis_client = boto3.client('kinesis')\nPREDICTIONS_STREAM_NAME = os.getenv('PREDICTIONS_STREAM_NAME')\n\nfile = 's3://ride-prediction-model/lin_reg.bin'\nwith s3.open(file, 'rb') as handle:\n    dv, model = pickle.load(handle)\n\nTEST_RUN = os.getenv('TEST_RUN', 'False') == 'True'\n\ndef prepare_features(ride):\n    features = {}\n    features['PU_DO'] = '%s_%s' % (ride['PULocationID'], ride['DOLocationID'])\n    features['trip_distance'] = ride['trip_distance']\n    return features\n\ndef predict(features):\n    X = dv.transform(features)\n    pred = model.predict(X)\n    return float(pred[0])\n\ndef lambda_handler(event, context):  \n    predictions_events = []\n    \n    for record in event['Records']:\n        encoded_data = record['kinesis']['data']\n        decoded_data = base64.b64decode(encoded_data).decode('utf-8')\n        ride_event = json.loads(decoded_data)\n\n        ride = ride_event['ride']\n        ride_id = ride_event['ride_id']\n    \n        features = prepare_features(ride)\n        prediction = predict(features)\n    \n        prediction_event = {\n            'model': 'ride_duration_prediction_model',\n            'version': '123',\n            'prediction': {\n                'ride_duration': prediction,\n                'ride_id': ride_id   \n            }\n        }\n\n        # Comment the below if statement if you want to test the lambda function locally\n        if not TEST_RUN:\n            kinesis_client.put_record(\n                StreamName=PREDICTIONS_STREAM_NAME,\n                Data=json.dumps(prediction_event),\n                PartitionKey=str(ride_id)\n            )\n        \n        predictions_events.append(prediction_event)\n\n    return {\n        'predictions': predictions_events\n    }\n\n\nCreate a virtual environment\nCreate a virtual environment and install the required packages:\npipenv install boto3 scikit-learn s3fs --python 3.9\n\n\nCreate a Dockerfile\nCreate a Dockerfile with the following content:\nFROM public.ecr.aws/lambda/python:3.9\n\nRUN pip install -U pip\nRUN pip install pipenv\n\nCOPY [\"Pipfile\", \"Pipfile.lock\", \"./\"]\n\nRUN pipenv install --system --deploy\n\nCOPY [\"lambda_function.py\", \"./\"]\n\nCMD [\"lambda_function.lambda_handler\"]\n\n\nBuild the Docker image\nBuild the Docker image:\n# Windows/Linux users\ndocker build -t stream-model-duration:v1 .\n\n# Mac users\ndocker build -t stream-model-duration:v1 . --platform=linux/amd64\n\n\nGive lambda access to s3\nWe need to give the lambda function access to the ride-prediction-model bucket. Follow these steps to give the lambda function access to the bucket:\nCreate the s3-list-read policy by following these steps:\n\nSign in to the AWS Management Console and open the policies page in the IAM console.\nChoose Create policy.\nSearch for s3 and click on it.\nUnder the Access level section, for Read select the GetObject action and List select the ListBucket action.\nUnder the Resources section, select Specific and then Add ARN. You‚Äôll need to add the ARNs for s3 bucket and lin_reg.bin object in s3. See the image below for an example.\n\n\n\nChoose the Text tab and Copy-Paste the ARN of the S3 bucket and object.\nChoose Add ARNs and then Next.\nGive the policy the following properties:\n\nName: s3-list-read\nDescription: Allows lambda to list and read objects from s3\n\nChoose Create policy.\n\nAttach the policy to the role:\n\nSign in to the AWS Management Console and open the roles page in the IAM console.\nSearch for lambda-kinesis-role and click on it.\nUnder the Add Permissions dropdown, click on Attach policies.\nSearch for s3-list-read and click on it. Chooes Add Permissions.\n\n\n\nCreate ECR Repository\nWe will use ECR to store the Docker image. Follow these steps to create a repository in ECR:\n\nGo to the ECR console.\nClick on Create repository.\nGive the repository the following properties:\n\nRepository name: duration-model\n\nClick on Create repository.\n\n\n\nAttach permissions to the user that will push the Docker image to ECR\nCreate the ecr-read-write policy by following these steps:\n\nSign in to the AWS Management Console and open the policies page in the IAM console.\nChoose Create policy.\nSearch for ecr and click on it.\nUnder the Access level section,\n\nRead select BatchCheckLayerAvailability and GetAuthorizationToken\nWrite select CompleteLayerUpload, InitiateLayerUpload, PutImage, and UploadLayerPart.\n\nUnder the Resources section, select Specific and then Add ARN. You‚Äôll need to add the ARNs for ecr repository.\nChoose the Text tab and Copy-Paste the ARN of the ecr repo.\nChoose Add ARNs and then Next.\nGive the policy the following properties:\n\nName: ecr-read-write\nDescription: Allows user to read and write to ecr repo\n\nChoose Create policy.\n\nAttach the policy to the role:\n\nSign in to the AWS Management Console and open the roles page in the IAM console.\nSearch for lambda-kinesis-role and click on it.\nUnder the Add Permissions dropdown, click on Attach policies.\nSearch for ecr-read-write and click on it. Chooes Add Permissions.\n\n\n\nUpload the Docker image to ECR\nAuthenticate Docker to an Amazon ECR registry with get-login-password:\n# Authenticate Docker to an Amazon ECR registry with get-login-password\naws ecr get-login-password --region &lt;REGION&gt; | docker login --username AWS --password-stdin &lt;AWS_ACCOUNT_ID&gt;.dkr.ecr.us-east-1.amazonaws.com\nReplace the &lt;REGION&gt; and &lt;AWS_ACCOUNT_ID&gt; with the region and account id of your AWS account.\nTag the Docker image and push it to ECR:\n# Store the URI of the ECR repository\nREMOTE_URI=\"&lt;AWS_ACCOUNT_ID&gt;.dkr.ecr.us-east-2.amazonaws.com/duration-model\"\nREMOTE_TAG=\"v1\"\nREMOTE_IMAGE=${REMOTE_URI}:${REMOTE_TAG}\n\n# Tag the local image with the remote image URI\nLOCAL_IMAGE=\"stream-model-duration:v1\"\ndocker tag ${LOCAL_IMAGE} ${REMOTE_IMAGE}\n\n# Push the image to ECR\ndocker push ${REMOTE_IMAGE}\n\n\nCreate a lambda function\nWe‚Äôll create a lambda function using the docker image that we created. Follow these steps to create the lambda function:\n\nGo to the lambda console.\nClick on Create function.\nChoose Container image option.\nGive the function the following properties:\n\nFunction name: ride-duration-prediction\nContainer image URI: Browse and select the image that we pushed to ECR.\n\nToggle the Change default execution role.\nChoose the Use an existing role option.\nSelect the lambda-kinesis-role role.\nClick on Create function.\n\nSetup the environment variable by following these steps:\n\nGo to the Lambda console and click on the Configuration tab.\nUnder the Environment variables section, click on Edit.\nChoose Add environment variable.\nGive the environment variable the following properties:\n\nKey: PREDICTIONS_STREAM_NAME\nValue: ride-prediction\n\nClick on Save.\n\nUpdate the timeout and memory settings by following these steps:\n\nGo to the Lambda console and click on the Configuration tab.\nUnder the General configuration section, click on Edit.\nIncrease the Timeout to 30 seconds and Memory to 256 MB.\n\n\n\nAdd a trigger to the lambda function\nWe‚Äôll add a trigger to the lambda function so that it gets invoked when a new object is created in the ride-prediction-model bucket. Follow these steps to add a trigger to the lambda function:\n\nNavigate back to the ride-duration-prediction Lambda function in the Lambda console.\nIn the Function overview section, choose Add trigger button.\nChoose Kinesis from the list of triggers.\nGive the trigger the following properties:\n\nKinesis stream: ride-event\n\nChoose Add.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can delete the ride-duration-prediction-test lambda function that we created earlier. Since the trigger for both the lambda functions is the same, we don‚Äôt want to trigger both the lambda functions.\n\n\n\n\nTest the lambda function\nWe‚Äôll test the lambda function by invoking it with a sample event. Run the following command to invoke the lambda function:\nKINESIS_STREAM_INPUT=ride-event\naws kinesis put-record \\\n    --stream-name ${KINESIS_STREAM_INPUT} \\\n    --partition-key 1 \\\n    --data '{\"ride\": {\"PULocationID\": 130,\"DOLocationID\": 205,\"trip_distance\": 3.66}, \"ride_id\": 156}' \\\n    --cli-binary-format raw-in-base64-out\nNavigate to the CloudWatch console and click on Logs. You should see a log stream for the ride-duration-prediction lambda function. Click on the log stream and you should see the log messages from the lambda function.\nSimilar to how we read the output from the ride-prediction stream, you can follow the same steps to read the output from the ride-prediction stream."
  },
  {
    "objectID": "posts/mlops/aws-deployment-lambda-kinesis.html#clean-up-your-resources",
    "href": "posts/mlops/aws-deployment-lambda-kinesis.html#clean-up-your-resources",
    "title": "Deployment of Dockerized ML Models with AWS Kinesis and Lambda",
    "section": "Clean up your resources",
    "text": "Clean up your resources\nTo avoid incurring future charges, delete the resources you created unless you want to retain them for future use.\n\nDelete the ride-prediction stream.\nDelete the ride-event stream.\nDelete the ride-duration-prediction lambda function.\nDelete the ride-prediction-model bucket.\nDelete the duration-model repository from ECR.\nDelete all the policies, roles, and users that we created."
  },
  {
    "objectID": "posts/mlops/aws-deployment-lambda-kinesis.html#references",
    "href": "posts/mlops/aws-deployment-lambda-kinesis.html#references",
    "title": "Deployment of Dockerized ML Models with AWS Kinesis and Lambda",
    "section": "References",
    "text": "References\n\nAWS event & context\nTutorial: Using AWS Lambda with Amazon Kinesis\nAWS lambda/python docker images\nAWS ECR Private Registry Authentication\nPushing a Docker Image to AWS ECR"
  },
  {
    "objectID": "posts/mlops/aws-deployment-lambda-kinesis.html#conclusion",
    "href": "posts/mlops/aws-deployment-lambda-kinesis.html#conclusion",
    "title": "Deployment of Dockerized ML Models with AWS Kinesis and Lambda",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, you learned how to build a real-time streaming application using Kinesis Data Streams and Lambda. Also, dockerized the machine learning model, registered it with ECR, and used it in the lambda function.\nI hope you enjoyed this article. If you have any questions, feel free to reach out to me on Twitter or Email.\nüëè Upvote if you liked it, üí¨ comment if you loved it. Hope to see you guys in the next one. ‚úåÔ∏è Peace!"
  },
  {
    "objectID": "posts/mlops/prefect-blocks.html",
    "href": "posts/mlops/prefect-blocks.html",
    "title": "Prefect Blocks",
    "section": "",
    "text": "In this post, we‚Äôll be building up on the concepts we discussed in the previous post. If you haven‚Äôt read that, I highly recommend checking it out before you continue reading this. You can find the post here.\nIn the previous post, we build a simple prefect flow that downloads a file from a URL and saves it to a local directory. We used MLflow to track our experiment - train a XGBoost model on the downloaded data. In this post, we‚Äôll be implementing the same workflow but this time we‚Äôll fetch the data from S3 instead of downloading it from a URL.\nWe‚Äôll setup the S3 bucket and also create a new IAM user and attach the AmazonS3FullAccess policy to it. We‚Äôll use the Access key and Secret key to access the S3 bucket from our code."
  },
  {
    "objectID": "posts/mlops/prefect-blocks.html#setup-s3-bucket",
    "href": "posts/mlops/prefect-blocks.html#setup-s3-bucket",
    "title": "Prefect Blocks",
    "section": "Setup S3 Bucket",
    "text": "Setup S3 Bucket\n\nLog into your AWS account and click on Services on the top left corner. Search for S3 and click on it.\nClick on Create bucket and give it a name. Note: Bucket names must be unique across all AWS accounts.\nKeep the default settings and click on Create bucket."
  },
  {
    "objectID": "posts/mlops/prefect-blocks.html#upload-the-data-to-the-s3-bucket",
    "href": "posts/mlops/prefect-blocks.html#upload-the-data-to-the-s3-bucket",
    "title": "Prefect Blocks",
    "section": "Upload the data to the S3 bucket",
    "text": "Upload the data to the S3 bucket\nDownload the data on your local machine and upload it to the S3 bucket. To keep it simple we‚Äôll use the GUI to upload the data to the S3 bucket. Use the following command in your terminal / command prompt to download the data.\n# Download dataset\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-01.parquet # January 2022\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-02.parquet # February 2022\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-03.parquet # March 2022\nNow, let‚Äôs upload the data to the S3 bucket. Follow the steps below to upload the data to the S3 bucket.\n\nClick on the bucket you just created.\nCreate a new folder by clicking on Create folder and give it a name data.\nClick on Upload and select the file you want to upload.\nClick on Upload again and wait for the upload to complete.\n\nAwesome! We now have the data in the S3 bucket."
  },
  {
    "objectID": "posts/mlops/prefect-blocks.html#create-iam-user",
    "href": "posts/mlops/prefect-blocks.html#create-iam-user",
    "title": "Prefect Blocks",
    "section": "Create IAM user",
    "text": "Create IAM user\nWe‚Äôll now create a new IAM user and attach the AmazonS3FullAccess policy to it. We‚Äôll use the Access key and Secret key to access the S3 bucket from our code. Follow the steps below to create a new IAM user.\n\nClick on Services on the top left corner. Search for IAM and click on it.\nClick on Users on the left sidebar and then click on Add user.\nGive the user a name and click on Next.\nIt is recommended to use groups to manage user‚Äôs permissions. Click on Create group and give it a name. Search for AmazonS3FullAccess in the Permissions policies table and select it. Click on Create user group.\nNow, select the group you just created and click on Next.\nClick on Create User.\n\n\nGenerate Access key and Secret key\n\nClick on the user you just created.\nClick on Security credentials tab and under the Access keys section click on Create access key.\nSelect the Other option and click on Next and then click on Create access key.\nClick on Show to view the Access key and Secret key. Note: You won‚Äôt be able to view the Secret key again so make sure you save it somewhere safe. Alternatively, you can also download the CSV file using the Download .csv file button.\n\nWe‚Äôre all setup now. Let‚Äôs get started with Prefect Blocks."
  },
  {
    "objectID": "posts/mlops/prefect-blocks.html#prefect-blocks",
    "href": "posts/mlops/prefect-blocks.html#prefect-blocks",
    "title": "Prefect Blocks",
    "section": "Prefect Blocks",
    "text": "Prefect Blocks\nBlocks serve as a fundamental element in Prefect, allowing the storage of configuration information and providing an interface for seamless interaction with external systems.\nBlocks offer a secure way to store authentication credentials for various services such as AWS, GitHub, Slack, and more, enabling seamless integration with Prefect.\nBy utilizing blocks, you gain access to convenient methods that facilitate interactions with external systems. For instance, you can effortlessly download or upload data from/to an S3 bucket, query or write data in a database, or send messages to Slack channels.\nConfiguring blocks can be done either programmatically or through the user-friendly interfaces of Prefect Cloud and the Prefect server UI. This flexibility allows you to manage blocks based on your preferred approach.\nYou can find blocks on the Prefect UI under the Blocks section. Click on Add Block to see the different blocks available.\n\n\n\n\n\n\n\nNote\n\n\n\nDon‚Äôt worry if you don‚Äôt see the different blocks in your UI. We‚Äôll be adding them in the next section. You can skip this section if you find S3 Bucket and AWS Credentials Blocks in the UI.\n\n\n\nAdding blocks to Prefect UI\n\n\n\n\n\n\nNote\n\n\n\nWe‚Äôll be using the same environment we created in the previous post. If you haven‚Äôt created the environment, you can find the instructions here.\n\n\nTo see the different blocks Prefect has to offer you can find them here. Prefect also allows you to create your own custom blocks. You can find the documentation here.\nFor this post, we‚Äôll be using the AWS integration prefect-aws to use the following blocks:\n\nS3Bucket\nAwsCredentials\n\nTo use these blocks you can pip install the package, then register the blocks you want to use with Prefect Cloud or a Prefect server. Follow the steps below:\n\nInstall the prefect_aws package in your environment.\n\npip install prefect_aws\n\nRegister the blocks using the prefect register command. -m flag is used to specify the module name.\n\nprefect block register -m prefect_aws\nNow you should be able to see the blocks in the Prefect UI.\n\n\nCreate S3 Bucket block\nWe‚Äôll now create a new file create_s3_bucket.py to store the credentials and the S3 bucket configurations. We‚Äôll be using the AwsCredentials and S3Bucket blocks to create the S3 bucket.\n\n\ncreate_s3_bucket.py\n\nfrom time import sleep\nfrom prefect_aws import S3Bucket, AwsCredentials\n\n# Create and save AWS credentials block\ndef create_aws_creds_block():\n    # Create AWS credentials object\n    my_aws_creds_obj = AwsCredentials(\n        aws_access_key_id = \"&lt;ACCESS_KEY&gt;\", # Replace with your access key\n        aws_secret_access_key = \"&lt;SECRET_KEY&gt;\" # Replace with your secret key\n    )\n    \n    # Save the AWS credentials block\n    my_aws_creds_obj.save(name=\"my-aws-creds\", overwrite=True)\n\n# Create and save S3 bucket block\ndef create_s3_bucket_block():\n    # Load AWS credentials block\n    aws_creds = AwsCredentials.load(\"my-aws-creds\")\n    \n    # Create S3 bucket object\n    my_s3_bucket_obj = S3Bucket(\n        bucket_name = \"&lt;BUCKET_NAME&gt;\", # Replace with your bucket name\n        credentials = aws_creds\n    )\n    \n    # Save the S3 bucket block\n    my_s3_bucket_obj.save(name=\"s3-bucket-example\", overwrite=True)\n\n\nif __name__ == \"__main__\":\n    # Create and save AWS credentials block\n    create_aws_creds_block()\n    \n    # Sleep for 5 seconds to allow time for the AWS credentials to be saved\n    sleep(5)\n    \n    # Create and save S3 bucket block\n    create_s3_bucket_block()\n\nLet‚Äôs understand what‚Äôs happening in the code above.\n\nWe first create an AwsCredentials object and pass the Access key and Secret key to it. We then save the object as a block using the save method. We also pass the overwrite argument as True to overwrite the block if it already exists. (Note: Use the Access key and Secret key you created in the previous section)\nWe then create an S3Bucket object and pass the bucket name and the AwsCredentials object to it. We then save the object as a block using the save method. We also pass the overwrite argument as True to overwrite the block if it already exists.\n\nThis allows to store the credentials and configurations in a secure way, and use them in our workflows.\nprefect block ls command can be used to list all the blocks. However, since we haven‚Äôt registered the above below in Prefect, we won‚Äôt be able to see them.\nWe‚Äôll run the script to create and save the blocks in Prefect. Before running the script, make sure you start the prefect server.\n# Make sure to start the prefect server before running the script\nprefect server start\n\n# Run the script (In new terminal window)\npython create_s3_bucket.py\nIf you run the command prefect block ls you should be able to see the blocks. Similarly if you go to the Prefect UI you should be able to see the blocks under the Blocks section.\n\n\nNow that we have the blocks setup, we can use them in our workflow."
  },
  {
    "objectID": "posts/mlops/prefect-blocks.html#workflow",
    "href": "posts/mlops/prefect-blocks.html#workflow",
    "title": "Prefect Blocks",
    "section": "Workflow",
    "text": "Workflow\nWe‚Äôll be using the same workflow we created in the previous post with some modifications. We‚Äôll be using the S3Bucket block to download the data from the S3 bucket.\nWe import the S3Bucket Block using the command from prefect_aws import S3Bucket.\n\n\norchestration_s3.py\n\nimport os\nimport pickle\nimport pathlib\nimport scipy\nimport mlflow\nimport sklearn\nimport argparse\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.metrics import mean_squared_error\nfrom prefect import flow, task\nfrom prefect_aws import S3Bucket\n\n@task(name=\"Read a Parquet file\")\ndef read_data(filename: str) -&gt; pd.DataFrame:\n    \"\"\"Read data into DataFrame\"\"\"\n    df = pd.read_parquet(filename)\n\n    df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n    df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)\n\n    df[\"duration\"] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n\n    df = df[(df.duration &gt;= 1) & (df.duration &lt;= 60)]\n\n    categorical = [\"PULocationID\", \"DOLocationID\"]\n    df[categorical] = df[categorical].astype(str)\n\n    return df\n\n@task(name=\"Add Features\")\ndef add_features(df_train: pd.DataFrame, df_val: pd.DataFrame) -&gt; tuple([\n        scipy.sparse._csr.csr_matrix,\n        scipy.sparse._csr.csr_matrix,\n        np.ndarray,\n        np.ndarray,\n        sklearn.feature_extraction.DictVectorizer,\n    ]):\n    \"\"\"Add features to the model\"\"\"\n    df_train[\"PU_DO\"] = df_train[\"PULocationID\"] + \"_\" + df_train[\"DOLocationID\"]\n    df_val[\"PU_DO\"] = df_val[\"PULocationID\"] + \"_\" + df_val[\"DOLocationID\"]\n\n    categorical = [\"PU_DO\"]  #'PULocationID', 'DOLocationID']\n    numerical = [\"trip_distance\"]\n\n    dv = DictVectorizer()\n\n    train_dicts = df_train[categorical + numerical].to_dict(orient=\"records\")\n    X_train = dv.fit_transform(train_dicts)\n\n    val_dicts = df_val[categorical + numerical].to_dict(orient=\"records\")\n    X_val = dv.transform(val_dicts)\n\n    y_train = df_train[\"duration\"].values\n    y_val = df_val[\"duration\"].values\n    return X_train, X_val, y_train, y_val, dv\n\n@task(name=\"Train Model\", log_prints=True)\ndef train_best_model(\n    X_train: scipy.sparse._csr.csr_matrix,\n    X_val: scipy.sparse._csr.csr_matrix,\n    y_train: np.ndarray,\n    y_val: np.ndarray,\n    dv: sklearn.feature_extraction.DictVectorizer,\n) -&gt; None:\n    \"\"\"train a model with best hyperparams and write everything out\"\"\"\n\n    with mlflow.start_run():\n        train = xgb.DMatrix(X_train, label=y_train)\n        valid = xgb.DMatrix(X_val, label=y_val)\n\n        best_params = {\n            \"learning_rate\": 0.09585355369315604,\n            \"max_depth\": 30,\n            \"min_child_weight\": 1.060597050922164,\n            \"objective\": \"reg:linear\",\n            \"reg_alpha\": 0.018060244040060163,\n            \"reg_lambda\": 0.011658731377413597,\n            \"seed\": 42,\n        }\n\n        mlflow.log_params(best_params)\n\n        booster = xgb.train(\n            params=best_params,\n            dtrain=train,\n            num_boost_round=100,\n            evals=[(valid, \"validation\")],\n            early_stopping_rounds=20,\n        )\n\n        y_pred = booster.predict(valid)\n        rmse = mean_squared_error(y_val, y_pred, squared=False)\n        mlflow.log_metric(\"rmse\", rmse)\n\n        pathlib.Path(\"models\").mkdir(exist_ok=True)\n        with open(\"models/preprocessor.b\", \"wb\") as f_out:\n            pickle.dump(dv, f_out)\n        mlflow.log_artifact(\"models/preprocessor.b\", artifact_path=\"preprocessor\")\n\n        mlflow.xgboost.log_model(booster, artifact_path=\"models_mlflow\")\n    return None\n\n@flow(name=\"Main Flow\")\ndef main_flow():\n    \"\"\"Main flow of the program\"\"\"\n    \n    # MLflow settings\n    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n    mlflow.set_experiment(\"nyc-taxi-experiment\")\n\n    # Load\n    s3_bucket_block = S3Bucket.load(\"s3-bucket-example\")\n    s3_bucket_block.download_folder_to_path(from_folder=\"data\", to_folder=\"data\")\n\n    df_train = read_data(\"./data/green_tripdata_2021-01.parquet\")\n    df_val = read_data(\"./data/green_tripdata_2021-02.parquet\")\n\n    # Transform\n    X_train, X_val, y_train, y_val, dv = add_features(df_train, df_val)\n\n    # Train\n    train_best_model(X_train, X_val, y_train, y_val, dv)\n\nif __name__ == \"__main__\":\n    main_flow()\n\nTo keep the workflow simple, we have removed the fetch and download_and_read (Subflow) functions. We have also made some changes to how the main_flow is called. The main change is that we are using the S3Bucket block to download the data from the S3 bucket.\nTo better understand the above script you can read my previous post here.\n# Loads the configuration of the S3 Bucket from the S3Bucket Block\ns3_bucket_block = S3Bucket.load(\"s3-bucket-block\")\n\n# Downloads the data from the S3 bucket folder name \"data\" to the local folder \"data\"\ns3_bucket_block.download_folder_to_path(from_folder=\"data\", to_folder=\"data\")\nRun the above script using the command python orchestration_s3.py"
  },
  {
    "objectID": "posts/mlops/prefect-blocks.html#conclusion",
    "href": "posts/mlops/prefect-blocks.html#conclusion",
    "title": "Prefect Blocks",
    "section": "Conclusion",
    "text": "Conclusion\nPrefect Blocks are a great way to reuse code and configurations, and to share them with your team. They also help secure your credentials and other sensitive information. In this tutorial, we have seen how to create a Prefect Block for the S3 Bucket. We have also seen how to use the S3 Bucket Block in a Prefect Flow.\nThank you for reading and I hope you found this notebook helpful. Upvote if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "posts/mlops/ml-deployment-gradio-huggingface-spaces.html",
    "href": "posts/mlops/ml-deployment-gradio-huggingface-spaces.html",
    "title": "Deployment of ML models on Hugging Face Spaces",
    "section": "",
    "text": "In this post, we‚Äôll cover how to deploy your machine learning / deep learning models on HuggingFace Spaces with Gradio.\nWe‚Äôll start by creating a simple gradio app to understand how it works. Then we‚Äôll deploy our model on HuggingFace Spaces.\nLet‚Äôs get started!"
  },
  {
    "objectID": "posts/mlops/ml-deployment-gradio-huggingface-spaces.html#what-is-gradio",
    "href": "posts/mlops/ml-deployment-gradio-huggingface-spaces.html#what-is-gradio",
    "title": "Deployment of ML models on Hugging Face Spaces",
    "section": "What is Gradio?",
    "text": "What is Gradio?\nGradio is a Python library that allows you to quickly create customizable UI components around your machine learning models. It‚Äôs a great way to share your models with others and get feedback.\nIf you are familiar with Streamlit, Gradio is similar to that. You can install it using pip:\npip install gradio"
  },
  {
    "objectID": "posts/mlops/ml-deployment-gradio-huggingface-spaces.html#creating-a-gradio-app",
    "href": "posts/mlops/ml-deployment-gradio-huggingface-spaces.html#creating-a-gradio-app",
    "title": "Deployment of ML models on Hugging Face Spaces",
    "section": "Creating a Gradio App",
    "text": "Creating a Gradio App\nLet‚Äôs create a simple Gradio app to understand how it works. You can create your app in a Jupyter notebook or a Python script. I‚Äôll be using a Jupyter notebook to showcase various features of Gradio.\nLater compile the notebook to a Python script to run the app.\nLet‚Äôs start by importing the necessary libraries:\nimport gradio as gr\n\n# Use only if you are using a Jupyter notebook\n%load_ext gradio\n\nComponents\nGradio provides various pre-built components that you can use to create your app. Let‚Äôs look at some of them.\n\nMarkdown: The markdown component allows you to display text in your app.\nTextbox: The textbox component allows you to take input from the user in the form of text.\nSlider: The slider component allows you to take input from the user in the form of a number.\nDropdown: The dropdown component allows you to take input from the user in the form of a list of options. Allows you to select one options.\nRadio: The radio component allows you to take input from the user in the form of a list of options. Allows you to select only one option.\nCheckbox: The checkbox component allows you to take input from the user in the form of a boolean value.\nCheckboxGroup: The checkbox group component allows you to take input from the user in the form of a list of options. Allows you to select one/multiple options.\n\nYou can find the complete list of components here.\nBelow is an example of how to use these components.\n\nimport gradio as gr\n\n# Use only if you are using a Jupyter notebook\n%load_ext gradio\n\n\n%%blocks\n# Use `%%blocks` only if you are using Jupyter Notebook \n\n# Markdown\ngr.Markdown(\"## Gradio app\")\n\n# Textbox\ngr.Textbox(lines=1, label=\"Description\", info=\"Provide a short description\", placeholder=\"Enter your text here...\")\n\n# Slider\ngr.Slider(minimum=1, maximum=10, step=1, value=1, label=\"Number of drinks\", info=\"How many drinks do you want to order?\")\n\n# Radio\ngr.Radio(choices=[\"Takeaway\", \"Dine-in\"], label=\"Order type\", info=\"Do you want to take away or dine in?\", value=\"Takeaway\")\n\n# Dropdown\ngr.Dropdown(choices=[\"Coffee\", \"Tea\", \"Hot Chocolate\"], label=\"Type of drink\", info=\"What type of drink do you want to order?\", value=\"Coffee\")\n\n# Dropdown with multiselect\ngr.Dropdown(choices=[\"Reading\", \"Writing\", \"Coding\"], label=\"Activities\", info=\"What do you want to do while drinking?\", value=[\"Reading\", \"Coding\"], multiselect=True)\n\n# Checkbox\ngr.Checkbox(label=\"Visit\", info=\"Do you want to visit the countries this year?\", value=False)\n\n# Checkbox group\ngr.CheckboxGroup(choices=[\"Australia\", \"Canada\", \"USA\"], label=\"Countries\", info=\"Where do you want to order from?\", value=[\"Australia\", \"Canada\"])\n\n\n\n\nSimple Gradio App\nLet‚Äôs create a simple sentence generator Gradio app. Create a python script name app.py and add the following code to it:\n\n\napp.py\n\nimport gradio as gr\n\ndef sentence_generator(drinks_count, drink_type, order_type, countries, activities, visit):\n    \"\"\" Generate a sentence based on the inputs \"\"\"\n    return f\"I want to order {drinks_count} {drink_type} for {order_type}. I want to visit {' and '.join(countries)} {'this year' if visit else 'next year'}. I want to do {' and '.join(activities)}.\"\n\n# Slider\ndrinks_count = gr.Slider(minimum=1, maximum=10, step=1, value=1, label=\"Number of drinks\", info=\"How many drinks do you want to order?\")\n\n# Dropdown\ndrink_type = gr.Dropdown(choices=[\"Coffee\", \"Tea\", \"Hot Chocolate\"], label=\"Type of drink\", info=\"What type of drink do you want to order?\", value=\"Coffee\")\n\n# Radio\norder_type = gr.Radio(choices=[\"Takeaway\", \"Dine-in\"], label=\"Order type\", info=\"Do you want to take away or dine in?\", value=\"Takeaway\")\n\n# Checkbox group\ncountries = gr.CheckboxGroup(choices=[\"Australia\", \"Canada\", \"USA\"], label=\"Countries\", info=\"Where do you want to order from?\", value=[\"Australia\", \"Canada\"])\n\n# Dropdown with multiselect\nactivities = gr.Dropdown(choices=[\"Reading\", \"Writing\", \"Coding\"], label=\"Activities\", info=\"What do you want to do while drinking?\", value=[\"Reading\", \"Coding\"], multiselect=True)\n\n# Checkbox\nvisit = gr.Checkbox(label=\"Visit\", info=\"Do you want to visit the countries this year?\", value=False)\n\ndemo = gr.Interface(\n    fn=sentence_generator, \n    inputs=[drinks_count, drink_type, order_type, countries, activities, visit], \n    outputs=\"text\",\n    examples=[\n        [2, \"Coffee\", \"Takeaway\", [\"Australia\"], [\"Reading\"], False],\n        [4, \"Tea\", \"Dine-in\", [\"Australia\", \"Canada\"], [\"Reading\", \"Writing\"], True],\n        [1, \"Hot Chocolate\", \"Takeaway\", [\"Australia\", \"Canada\", \"USA\"], [\"Reading\", \"Writing\", \"Coding\"], False]\n    ]\n)\ndemo.launch()\n\nYou can run the app using the following command:\npython app.py\nNavigate to the URL shown in the output. You should see the following output.\nI have embedded the app below. You can play around with it.\n\nLet‚Äôs understand the code.\nFirst, we define a function sentence_generator that takes in the inputs and returns a sentence.\nThen we define the components that we want to use in our app. We define a slider, a dropdown, a radio, a checkbox group, a dropdown with multiselect, and a checkbox. We also define the default values for each component.\nWe declared variables for each component so that we can use the value of the component later in the code.\nGradio has two ways to create a web-based UI:\n\nInterface\nBlocks\n\n\n\nInterface\nInterface is high-level class, enables building web-based GUI in a few lines of code. You must specify three parameters:\n\nThe function to create a GUI for\nThe desired input components and\nThe desired output components.\n\nAdditional parameters can be used to control the appearance and behavior of the demo.\nMore on blocks later.\nContinuing with our code, we define the interface. We pass in the function, the inputs, the outputs, and the examples.\nYou‚Äôll notice the interface automatically created Clear, Submit and Flag buttons.\nNotice that the inputs are the variables that we defined earlier based on the components. The outputs is the type of output that we want to display. In our case, it‚Äôs text.\nExamples are great way to showcase some one-click examples that the user can try out.\nFinally, we launch the app. I encourage you to play with the app and try out different combinations.\n\n\nBlocks\nBlocks are low-level components that can be used to build a web-based GUI. They provide more flexibility and control than the interface class over:\n\nThe layout of components\nThe events that trigger the execution of functions\nData flows (e.g.¬†inputs can trigger outputs, which can trigger the next level of outputs)\n\nBlocks also offers ways to group together related demos such as with tabs.\nThe basic usage of Blocks is as follows:\n\nCreate a Blocks object, then use it as a context (with the ‚Äúwith‚Äù statement), and\nDefine layouts, components, or events within the Blocks context.\nFinally, call the launch() method to launch the demo."
  },
  {
    "objectID": "posts/mlops/ml-deployment-gradio-huggingface-spaces.html#block-layouts",
    "href": "posts/mlops/ml-deployment-gradio-huggingface-spaces.html#block-layouts",
    "title": "Deployment of ML models on Hugging Face Spaces",
    "section": "Block Layouts",
    "text": "Block Layouts\nLayouts are used to organize components in a Blocks object. There are various layouts available: Row, Column, Tab, Group, Box, and Accordion.\nWe‚Äôll go through some layout examples.\n\nRow\nThe Row layout places components in a row i.e.¬†horizontally.\n\n\napp.py\n\nimport gradio as gr\n\ndef sentence_generator(drinks_count, drink_type, order_type, countries, activities, visit):\n    \"\"\" Generate a sentence based on the inputs \"\"\"\n    return f\"I want to order {drinks_count} {drink_type} for {order_type}. I want to visit {' and '.join(countries)} {'this year' if visit else 'next year'}. I want to do {' and '.join(activities)}.\"\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        # Slider\n        drinks_count = gr.Slider(minimum=1, maximum=10, step=1, value=1, label=\"Number of drinks\", info=\"How many drinks do you want to order?\")\n\n        # Dropdown\n        drink_type = gr.Dropdown(choices=[\"Coffee\", \"Tea\", \"Hot Chocolate\"], label=\"Type of drink\", info=\"What type of drink do you want to order?\", value=\"Coffee\")\n    \n    with gr.Row():\n        # Radio\n        order_type = gr.Radio(choices=[\"Takeaway\", \"Dine-in\"], label=\"Order type\", info=\"Do you want to take away or dine in?\", value=\"Takeaway\")\n\n        # Checkbox group\n        countries = gr.CheckboxGroup(choices=[\"Australia\", \"Canada\", \"USA\"], label=\"Countries\", info=\"Where do you want to order from?\", value=[\"Australia\", \"Canada\"])\n\n        # Dropdown with multiselect\n        activities = gr.Dropdown(choices=[\"Reading\", \"Writing\", \"Coding\"], label=\"Activities\", info=\"What do you want to do while drinking?\", value=[\"Reading\", \"Coding\"], multiselect=True)\n    \n    # Checkbox\n    visit = gr.Checkbox(label=\"Visit\", info=\"Do you want to visit the countries this year?\", value=False)\n\n    # Button\n    sentence_generator_btn = gr.Button(\"Generate Sentence\")\n\n    # Output\n    output = gr.Textbox(lines=1, label=\"Generated Sentence\", info=\"Generated sentence will appear here\")\n\n    # What happens when we click on the button\n    sentence_generator_btn.click(fn=sentence_generator, inputs=[drinks_count, drink_type, order_type, countries, activities, visit], outputs=output)\n\ndemo.launch()\n\nEach component is placed in a gr.Row object appears on the same row.\n\n\n\nColumn\nThe Column layout places components in a column i.e.¬†vertically.\nLet‚Äôs combine Row and Column to see how they interact with each other.\n\n\napp.py\n\nimport gradio as gr\n\ndef sentence_generator(drinks_count, drink_type, order_type, countries, activities, visit):\n    \"\"\" Generate a sentence based on the inputs \"\"\"\n    return f\"I want to order {drinks_count} {drink_type} for {order_type}. I want to visit {' and '.join(countries)} {'this year' if visit else 'next year'}. I want to do {' and '.join(activities)}.\"\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            # Slider\n            drinks_count = gr.Slider(minimum=1, maximum=10, step=1, value=1, label=\"Number of drinks\", info=\"How many drinks do you want to order?\")\n\n            # Dropdown\n            drink_type = gr.Dropdown(choices=[\"Coffee\", \"Tea\", \"Hot Chocolate\"], label=\"Type of drink\", info=\"What type of drink do you want to order?\", value=\"Coffee\")\n        \n        with gr.Column():\n            # Radio\n            order_type = gr.Radio(choices=[\"Takeaway\", \"Dine-in\"], label=\"Order type\", info=\"Do you want to take away or dine in?\", value=\"Takeaway\")\n\n            # Checkbox group\n            countries = gr.CheckboxGroup(choices=[\"Australia\", \"Canada\", \"USA\"], label=\"Countries\", info=\"Where do you want to order from?\", value=[\"Australia\", \"Canada\"])\n\n    with gr.Row(equal_height=False):\n        # Dropdown with multiselect\n        activities = gr.Dropdown(choices=[\"Reading\", \"Writing\", \"Coding\"], label=\"Activities\", info=\"What do you want to do while drinking?\", value=[\"Reading\", \"Coding\"], multiselect=True)\n    \n        # Checkbox\n        visit = gr.Checkbox(label=\"Visit\", info=\"Do you want to visit the countries this year?\", value=False)\n    \n    # Button\n    sentence_generator_btn = gr.Button(\"Generate Sentence\")\n\n    # Output\n    output = gr.Textbox(lines=1, label=\"Generated Sentence\", info=\"Generated sentence will appear here\")\n\n    # What happens when we click on the button\n    sentence_generator_btn.click(fn=sentence_generator, inputs=[drinks_count, drink_type, order_type, countries, activities, visit], outputs=output)\n\ndemo.launch()\n\nFirst, we divided in a row into two columns. Then we placed two components that appear vertically in each column. The second row contains two components that appear horizontally. Lastly, we placed one component in the third row.\n\n\n\nTab\nThe Tab layout allows you to group components into tabs. Each tab is a gr.Tab object. Components defined within the Tab will be visible when this tab is selected tab.\n\n\napp.py\n\nimport gradio as gr\n\ndef order_generator(drinks_count, drink_type, order_type):\n    \"\"\" Generate a sentence based on the inputs \"\"\"\n    return f\"I want to order {drinks_count} {drink_type} for {order_type}\"\n\ndef travel_generator(countries, activities, visit):\n    \"\"\" Generate a sentence based on the inputs \"\"\"\n    return f\"I want to visit {' and '.join(countries)} {'this year' if visit else 'next year'}. I want to do {' and '.join(activities)}.\"\n\nwith gr.Blocks() as demo:\n    with gr.Tab(\"Order Drinks\"):\n        with gr.Row():\n            with gr.Column():\n                # Slider\n                drinks_count = gr.Slider(minimum=1, maximum=10, step=1, value=1, label=\"Number of drinks\", info=\"How many drinks do you want to order?\")\n\n                # Dropdown\n                drink_type = gr.Dropdown(choices=[\"Coffee\", \"Tea\", \"Hot Chocolate\"], label=\"Type of drink\", info=\"What type of drink do you want to order?\", value=\"Coffee\")\n            \n            with gr.Column():\n                # Radio\n                order_type = gr.Radio(choices=[\"Takeaway\", \"Dine-in\"], label=\"Order type\", info=\"Do you want to take away or dine in?\", value=\"Takeaway\")\n        \n        # Button\n        order_btn = gr.Button(\"Order\")\n        \n        # Output\n        order_output = gr.Textbox(lines=1, label=\"Generated Sentence\", info=\"Generated sentence will appear here\")\n\n    with gr.Tab(\"Travel\"):\n        with gr.Row(equal_height=False):\n            # Checkbox group\n            countries = gr.CheckboxGroup(choices=[\"Australia\", \"Canada\", \"USA\"], label=\"Countries\", info=\"Where do you want to order from?\", value=[\"Australia\", \"Canada\"])\n\n            # Dropdown with multiselect\n            activities = gr.Dropdown(choices=[\"Reading\", \"Writing\", \"Coding\"], label=\"Activities\", info=\"What do you want to do while drinking?\", value=[\"Reading\", \"Coding\"], multiselect=True)\n        \n        # Checkbox\n        visit = gr.Checkbox(label=\"Visit\", info=\"Do you want to visit the countries this year?\", value=False)\n    \n        # Button\n        travel_btn = gr.Button(\"Travel\")\n\n        # Output\n        travel_output = gr.Textbox(lines=1, label=\"Generated Sentence\", info=\"Generated sentence will appear here\")\n\n    # What happens when we click on the button\n    order_btn.click(fn=order_generator, inputs=[drinks_count, drink_type, order_type], outputs=order_output)\n    travel_btn.click(fn=travel_generator, inputs=[countries, activities, visit], outputs=travel_output)\n\ndemo.launch()\n\n\n\n\nAccordion\nThe Accordion layout allows you to toggle visibility of components in a gr.Accordion object. Components defined within the Accordion will be visible when this accordion is expanded.\n\n\napp.py\n\nimport gradio as gr\n\ndef sentence_generator(drinks_count, drink_type, order_type, countries, activities, visit):\n    \"\"\" Generate a sentence based on the inputs \"\"\"\n    return f\"I want to order {drinks_count} {drink_type} for {order_type}. I want to visit {' and '.join(countries)} {'this year' if visit else 'next year'}. I want to do {' and '.join(activities)}.\"\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            # Slider\n            drinks_count = gr.Slider(minimum=1, maximum=10, step=1, value=1, label=\"Number of drinks\", info=\"How many drinks do you want to order?\")\n\n            # Dropdown\n            drink_type = gr.Dropdown(choices=[\"Coffee\", \"Tea\", \"Hot Chocolate\"], label=\"Type of drink\", info=\"What type of drink do you want to order?\", value=\"Coffee\")\n        \n        with gr.Column():\n            # Radio\n            order_type = gr.Radio(choices=[\"Takeaway\", \"Dine-in\"], label=\"Order type\", info=\"Do you want to take away or dine in?\", value=\"Takeaway\")\n\n            # Checkbox group\n            countries = gr.CheckboxGroup(choices=[\"Australia\", \"Canada\", \"USA\"], label=\"Countries\", info=\"Where do you want to order from?\", value=[\"Australia\", \"Canada\"])\n\n    with gr.Accordion(label=\"Show elements if you want to travel\", open=False):\n        with gr.Row(equal_height=False):\n            # Dropdown with multiselect\n            activities = gr.Dropdown(choices=[\"Reading\", \"Writing\", \"Coding\"], label=\"Activities\", info=\"What do you want to do while drinking?\", value=[\"Reading\", \"Coding\"], multiselect=True)\n        \n            # Checkbox\n            visit = gr.Checkbox(label=\"Visit\", info=\"Do you want to visit the countries this year?\", value=False)\n\n    # Button\n    sentence_generator_btn = gr.Button(\"Generate Sentence\")\n\n    # Output\n    output = gr.Textbox(lines=1, label=\"Generated Sentence\", info=\"Generated sentence will appear here\")\n\n    # What happens when we click on the button\n    sentence_generator_btn.click(fn=sentence_generator, inputs=[drinks_count, drink_type, order_type, countries, activities, visit], outputs=output)\n\ndemo.launch()\n\n\nI encourage you to explore other layouts here"
  },
  {
    "objectID": "posts/mlops/ml-deployment-gradio-huggingface-spaces.html#deploy-your-ml-model-on-hugging-face-spaces",
    "href": "posts/mlops/ml-deployment-gradio-huggingface-spaces.html#deploy-your-ml-model-on-hugging-face-spaces",
    "title": "Deployment of ML models on Hugging Face Spaces",
    "section": "Deploy your ML model on Hugging Face Spaces",
    "text": "Deploy your ML model on Hugging Face Spaces\nWe‚Äôll combine the knowledge we gained in the previous sections to deploy our ML model on Hugging Face Spaces. We‚Äôll use the gradio library to create a UI for our model and then deploy it on Hugging Face Spaces.\n\nCreate a new Space\nYou can create a new Hugging Face account for free here. Once you‚Äôve created an account, follow these steps to create a new Space:\n\nGo to Hugging Face Spaces and click on Create New Space.\nGive your space a name.\nChoose Gradio as the Space SDK.\nClick on Create Space.\n\nOpen a terminal and run the git clone command to clone your space locally.\n\n\n\n\n\n\nImportant\n\n\n\nMake sure that you have git lfs installed on your system. If you don‚Äôt have it installed, you can install it by following the instructions here.\n\n\n\n\nDirectory Structure\nWe‚Äôll use a directory structure as follows:\nYou would already have the README.md and .gitattributes file in the directory. We‚Äôll add the following files to the directory.\n.\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ app.py\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îî‚îÄ‚îÄ zone_lookup.csv\n‚îú‚îÄ‚îÄ examples\n‚îÇ   ‚îú‚îÄ‚îÄ basset.jpg\n‚îÇ   ‚îú‚îÄ‚îÄ cat.jpg\n‚îÇ   ‚îú‚îÄ‚îÄ dog.jpg\n‚îÇ   ‚îî‚îÄ‚îÄ dunno.jpg\n‚îú‚îÄ‚îÄ models\n‚îÇ   ‚îú‚îÄ‚îÄ lin_reg.bin\n‚îÇ   ‚îî‚îÄ‚îÄ model.pkl\n‚îî‚îÄ‚îÄ requirements.txt\nYou can get the data, examples, and models directory from here.\nContents for the requirements.txt file:\n\n\nrequirements.txt\n\nfastai\npandas\nscikit-learn\n\n\n\nCreate the app.py file\nWe‚Äôll create the app.py file in the root directory of our space. The contents of the file are as follows:\n\n\napp.py\n\nimport pickle\nimport random\nimport pandas as pd\nimport gradio as gr\nfrom fastai.vision.all import *\n\nzone_lookup = pd.read_csv('./data/zone_lookup.csv')\n\nwith open('./models/lin_reg.bin', 'rb') as handle:\n    dv, model = pickle.load(handle)\n\ndef prepare_features(pickup, dropoff, trip_distance):\n    pickupId = zone_lookup[zone_lookup[\"borough_zone\"] == pickup].LocationID\n    dropoffId = zone_lookup[zone_lookup[\"borough_zone\"] == dropoff].LocationID\n    trip_distance = round(trip_distance, 4)\n\n    features = {}\n    features['PU_DO'] = '%s_%s' % (pickupId, dropoffId)\n    features['trip_distance'] = trip_distance\n    return features\n\ndef predict(pickup, dropoff, trip_distance):\n    features = prepare_features(pickup, dropoff, trip_distance)\n    X = dv.transform(features)\n    preds = model.predict(X)\n    duration = float(preds[0])\n    return \"The predicted duration is %.4f minutes.\" % duration\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"\"\"\n    This demo is a simple example of how to use Gradio to create a web interface for your machine learning models.\n    Models used in this demo are very simple and are not meant to perform well. The goal is to show how to use Gradio with a simple model.\n    \"\"\")\n    \n    gr.Markdown(\"Predict Taxi Duration or Classify dog vs cat using this demo\")\n\n    with gr.Tab(\"Predict Taxi Duration\"):\n        with gr.Row():\n            with gr.Column():\n                with gr.Row():\n                    pickup = gr.Dropdown(\n                        choices=list(zone_lookup[\"borough_zone\"]), \n                        label=\"Pickup Location\", \n                        info=\"The location where the passenger(s) were picked up\", \n                        value=lambda: random.choice(zone_lookup[\"borough_zone\"])\n                    )\n\n                    dropoff = gr.Dropdown(\n                        choices=list(zone_lookup[\"borough_zone\"]), \n                        label=\"Dropoff Location\", \n                        info=\"The location where the passenger(s) were dropped off\", \n                        value=lambda: random.choice(zone_lookup[\"borough_zone\"])\n                    )\n\n                trip_distance = gr.Slider(\n                    minimum=0.0, \n                    maximum=100.0,\n                    step=0.1,\n                    label=\"Trip Distance\",\n                    info=\"The trip distance in miles calculated by the taximeter\",\n                    value=lambda: random.uniform(0.0, 100.0)\n                )\n            with gr.Column():\n                output = gr.Textbox(label=\"Output Box\")\n                predict_btn = gr.Button(\"Predict\")\n\n        examples = gr.Examples([[\"Queens - Bellerose\", \"Bronx - Schuylerville/Edgewater Park\", 25], [\"Bronx - Norwood\", \"rooklyn - Sunset Park West\", 55]], inputs=[pickup, dropoff, trip_distance])\n    \n    with gr.Tab(\"Classify Dog vs Cat\"):\n        def is_cat(x): return x[0].isupper()\n\n        learn = load_learner('./models/model.pkl')\n\n        categories = ('Dog', 'Cat')\n\n        def classify_image(img):\n            pred, idx, probs = learn.predict(img)\n            return dict(zip(categories, map(float,probs)))\n\n        with gr.Row():\n            image = gr.inputs.Image(shape=(192, 192))\n            label = gr.outputs.Label()\n\n        examples = gr.Examples(['./examples/dog.jpg', './examples/cat.jpg', './examples/dunno.jpg', './examples/basset.jpg'], inputs=[image])\n        \n        classify_btn = gr.Button(\"Predict\")\n\n    predict_btn.click(fn=predict, inputs=[pickup, dropoff, trip_distance], outputs=output, api_name=\"predict-duration\")       \n    classify_btn.click(fn=classify_image, inputs=image, outputs=label, api_name=\"classify-dog-breed\")\n\ndemo.launch()\n\n\n\nPush the code to your space\nOnce you‚Äôve created the app.py file and the directory structure, you can push the code to your space using the following commands:\ngit add .\ngit commit -m \"Initial commit\"\ngit push\nYou‚Äôll be asked to enter your huggingface.co username and password. Once you‚Äôve entered the credentials, the code will be pushed to your space.\nIn future if you want to add huge files to your space, make sure to track them using git lfs before pushing them to your space.\ngit lfs track &lt;FILE_NAME&gt;\ngit lfs install\n\ngit add .\ngit commit -m \"Add model files\"\ngit push\nNavigate to your space and choose the App tab. You‚Äôll see the following screen:"
  },
  {
    "objectID": "posts/mlops/ml-deployment-gradio-huggingface-spaces.html#conclusion",
    "href": "posts/mlops/ml-deployment-gradio-huggingface-spaces.html#conclusion",
    "title": "Deployment of ML models on Hugging Face Spaces",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we saw how to create a simple web app using Gradio. We also saw how to deploy the app to Hugging Face Spaces.\nI hope you enjoyed this article. If you have any questions, feel free to reach out to me on Twitter or Email.\nüëè Upvote if you liked it, üí¨ comment if you loved it. Hope to see you guys in the next one. ‚úåÔ∏è Peace!"
  },
  {
    "objectID": "posts/ml/machine-learning-expert-path.html",
    "href": "posts/ml/machine-learning-expert-path.html",
    "title": "Path to become a Machine Learning Expert",
    "section": "",
    "text": "Path to becoming a Machine Learning (ML) Expert made easy. There are a lot of resources out there that can be overwhelming at the start. But don‚Äôt worry this learning path would provide structure and lay the foundational knowledge to begin a career in ML."
  },
  {
    "objectID": "posts/ml/machine-learning-expert-path.html#learn-the-basics-of-descriptive-statistics-inferential-statistics-and-math",
    "href": "posts/ml/machine-learning-expert-path.html#learn-the-basics-of-descriptive-statistics-inferential-statistics-and-math",
    "title": "Path to become a Machine Learning Expert",
    "section": "1. Learn the basics of Descriptive Statistics, Inferential Statistics and Math",
    "text": "1. Learn the basics of Descriptive Statistics, Inferential Statistics and Math\nUnderstanding the math used in ML can help in building the foundation strong. Udacity offers courses on descriptive statistics and inferential statistics. These courses are free and use excel to teach the concepts.\nAlong with statistics and probabilities, concepts on linear algebra, multivariate calculus, optimization functions and many more form the building blocks for ML. There is an awesome youtube channel that makes these concepts very easy to learn. 3Brown1Blue focuses on teaching mathematics using a distinct visual perspective.\nMore resources:\n\nComputational Linear Algebra for Coders\nProf.¬†Gilbert Strang‚Äôs Linear Algebra book/course\nMatrix Cookbook by Kaare Brandt Petersen & Michael Syskind Pedersen\nThink Stats (Exploratory Data Analysis in Python) by Allen Downey\nConvex Optimization by Stephen Boyd and Lieven Vandenberghe\nEssentials of Metaheuristics by Sean Luke"
  },
  {
    "objectID": "posts/ml/machine-learning-expert-path.html#learn-the-basics-of-python-and-its-packages",
    "href": "posts/ml/machine-learning-expert-path.html#learn-the-basics-of-python-and-its-packages",
    "title": "Path to become a Machine Learning Expert",
    "section": "2. Learn the basics of Python and it‚Äôs packages",
    "text": "2. Learn the basics of Python and it‚Äôs packages\nFirst, let‚Äôs install Python. The easiest way to do this is by installing Anaconda. All the packages that are required come along with Anaconda.\nYou can start from learning the basics of Python i.e.¬†data structures, functions, class, etc. and it‚Äôs libraries. I started learning about python in my college days, I read the book Learn Python the Hard Way. A very good book for beginners. Introduction to Python Programming by Udacity is a free course that covers the basics of Python. Introduction to Python is another free course by Analytics Vidhya. Another free course by Google is Google‚Äôs Python Class.\nNext, learn about how to use Regular Expression (also called regex) in Python. It will come in use for data cleaning, especially if you are working with text data. Learn regular expressions through Google class. A very good beginner tutorial for learning regular expression in python on Analytics Vidhya. Cheatsheet for Regex.\nNow comes the fun part of learning the various libraries in Python. Numpy, Pandas, Matplotlib, Seaborn, and Sklearn are the packages heavily used in ML.\n\nNumpy provides a high-performance multidimensional array and basic tools to compute with and manipulate these arrays. Numpy quickstart tutorial is a good place to start. This will form a good foundation for this to come. Practice numpy by solving 100 numpy exercises to solve.\nPandas is used for data manipulation and analysis. The most used package in Python is Pandas. Intro to pandas data structure provides a detailed tutorial on pandas. A short course by Kaggle on pandas.\nMatplotlib is a visualization library in python. In the matplotlib tutorial, you will learn the basics of Python data visualization, the anatomy of a Matplotlib plot, and much more. Official documentation of matplotlib is one of the best ways to learn the library.\nSeaborn is another visualization library built on top of matplotlib. Kaggle short course on data visualization provides a good start point to learn the library."
  },
  {
    "objectID": "posts/ml/machine-learning-expert-path.html#data-explorationcleaningpreparation",
    "href": "posts/ml/machine-learning-expert-path.html#data-explorationcleaningpreparation",
    "title": "Path to become a Machine Learning Expert",
    "section": "3. Data Exploration/Cleaning/Preparation",
    "text": "3. Data Exploration/Cleaning/Preparation\nReal-world data is unstructured, contains missing values, outliers, typos, etc. This step is one of the most important steps for a data analyst to perform because how good the model will perform will depend on the quality of the data.\nLearn different stages of data explorations:\n\nVariable Identification, Univariate and Multivariate analysis\nMissing values treatment\nOutlier treatment\nFeature Engineering\n\nAdditional resources:\n\nYou can also refer to the data exploration guide.\nBook on Python for Data Analysis by Wes McKinney"
  },
  {
    "objectID": "posts/ml/machine-learning-expert-path.html#introduction-to-machine-learning",
    "href": "posts/ml/machine-learning-expert-path.html#introduction-to-machine-learning",
    "title": "Path to become a Machine Learning Expert",
    "section": "4. Introduction to Machine Learning",
    "text": "4. Introduction to Machine Learning\nNow it‚Äôs time to enter the belly of the beast. There are various resources to learn ML and I would suggest the following courses:\n\nMachine Learning by Stanford (Coursera) The Machine Learning course by Andrew Ng is one of the best courses out there and covers all the basic algorithms. Also, it introduces all the advanced topics in a very simple manner which is easy to understand. However, this course is taught in Octave rather than the popular languages like R/Python. Also, this course is NOT free but you can apply for financial aid.\nMachine Learning A-Z‚Ñ¢: Hands-On Python & R In Data Science (Udemy) Good course for beginners. Explore complex topics such as natural language processing (NLP), reinforcement learning (RL), deep learning (DL) among many others. Tons of practice exercise and quizzes. This course is NOT free but comparatively not expensive.\nComprehensive learning path for Data Science (Analytics Vidhya) This course covers every topic right from the beginning. Installing Python, data cleaning and preparation, Machine learning concepts, deep learning, and NLP. This course is free and does not come with any certification.\n\nBooks:\n\nThe Hundred Page Machine Learning Book\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition\n\nList of best books for machine learning.\nAfter learning about the various techniques in ML the next natural thing to do is apply those techniques. What better place than Kaggle. It is one of the most popular websites among data science enthusiasts. Below two problem statement can be a good starting problem statement to begin with.\n\nTitanic: Machine Learning from Disaster\nHouse Prices: Advanced Regression Techniques"
  },
  {
    "objectID": "posts/ml/machine-learning-expert-path.html#deep-learning",
    "href": "posts/ml/machine-learning-expert-path.html#deep-learning",
    "title": "Path to become a Machine Learning Expert",
    "section": "5. Deep Learning",
    "text": "5. Deep Learning\nUsing the idea to mimic a human brain has been around since the 1900s. There were various algorithms and techniques developed for the same but due to the lack of computing power, it was difficult to run those algorithms.\nDue to the improvements in the hardware and the introduction to using GPUs to compute caught the attention of people passionate about working on neural net-based models. Today, state of the art results can be obtained using deep neural networks.\nCourses from deeplearning.ai on Coursera are one of the most popular and fantastic courses on deep learning.\n\nNeural Networks and Deep Learning\nDeep Learning Specialization\n\nBoth the courses are paid but financial aid is available for both of them.\nAdditional Resources:\n\nDeep Learning Summer School, Montreal 2015\nDeep Learning for Perception, Virginia Tech, Electrical, and Computer Engineering\nCS231N 2017\nA blog that explains concepts on Convolutional Neural Nets (CNN)\n(Book) Deep Learning ‚Äì Methods and Applications\n(Youtube Channel) DeepLearning.TV\nDeep Learning book from MIT\nNeural Networks and Deep Learning online Book\nComprehensive resources on deeplearning.net"
  },
  {
    "objectID": "posts/ml/machine-learning-expert-path.html#natural-language-processing",
    "href": "posts/ml/machine-learning-expert-path.html#natural-language-processing",
    "title": "Path to become a Machine Learning Expert",
    "section": "6. Natural Language Processing",
    "text": "6. Natural Language Processing\nNatural language processing (NLP) is a branch of Artificial Intelligence which aims to bridge the gap between how a computer and human communicate with each other. The two major handles used for communication are speech and written i.e.¬†text. If you are unfamiliar with what NLP is, this blog could help in understanding what NLP is.\nCourses:\n\n(Youtube) Natural Language Processing by University of Michigan\nSpeech and Language Processing\nStanford CS224N: NLP with Deep Learning Winter 2019 ‚Äì Stanford\nLecture Collection on Natural Language Processing with Deep Learning (Winter 2017) ‚Äì Stanford\nCS224d: Deep Learning for Natural Language Processing ‚Äì Stanford\nNatural Language Processing Specialization offered by deeplearning.ai on Coursera (Intermediate level)\nNatural Language Processing offered by National Research University Higher School of Economics on Coursera (Advanced level course)\n\nMachine Learning in itself is a huge domain and the only way to master it is to explore and practice. I cannot stress more on practice because without practice is like trying to play the guitar without any strings.\nPopular blogs to follow:\n\nAnalytics Vidhya\nMachine Learning Mastery\nTowards Data Science\nKDnuggets\n\nAdditional Resources:\n\nA Complete Python Tutorial to Learn Data Science from Scratch\nA Comprehensive Learning Path for Deep Learning in 2019 on Analytics Vidhya\nLearning Path to Master Computer Vision in 2020 on Analytics Vidhya\nA Comprehensive Learning Path to Understand and Master NLP in 2020 on Analytics Vidhya\nA Comprehensive Guide to Understand and Implement Text Classification in Python on Analytics Vidhya\nA comprehensive Learning path to becoming a data scientist in 2020 free course on Analytics Vidhya\n\nI wish you all the best on your journey to becoming a machine learning expert.\nShare if you like it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html",
    "href": "posts/nlp/text-preprocessing.html",
    "title": "Text Preprocessing",
    "section": "",
    "text": "In any machine learning task, cleaning and pre-processing of the data is a very important step. The better we can represent our data, the better the model training and prediction can be expected.\nSpecially in the domain of Natural Language Processing (NLP) the data is unstructured. It become crucial to clean and properly format it based on the task at hand. There are various pre-processing steps that can be performed but not necessary to perform all. These steps should be applied based on the problem statement.\nExample: Sentiment analysis on twitter data can required to remove hashtags, emoticons, etc. but this may not be the case if we are doing the same analysis on customer feedback data.\nHere we are using the twitter_sample dataset from the nltk library.\nCode\n# Import libraries and load the data\nimport numpy as np\nimport pandas as pd\nimport re\nimport nltk\nimport spacy\nimport string\nimport demoji\nimport contractions\nimport unidecode\nfrom num2words import num2words\nfrom nltk.corpus import twitter_samples\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom bs4 import BeautifulSoup\nfrom spellchecker import SpellChecker\nfrom ekphrasis.classes.preprocessor import TextPreProcessor\nfrom ekphrasis.classes.tokenizer import SocialTokenizer\nfrom ekphrasis.dicts.emoticons import emoticons\n\npd.options.display.max_columns = None\npd.options.display.max_rows = None\npd.options.display.max_colwidth = None\n\nnltk.download('twitter_samples')  # Download the dataset\n\n# We are going to use the Negative and Positive Tweets file which each contains 5000 tweets.\nfor name in twitter_samples.fileids():\n    print(f' - {name}')\n\n\n - negative_tweets.json\n - positive_tweets.json\n - tweets.20150430-223406.json\n\n\n[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /Users/wizard/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\nCode\n# Load the negative tweets file and assign label as 0 for negative\nnegative_tweets = twitter_samples.strings(\"negative_tweets.json\")\ndf_neg = pd.DataFrame(negative_tweets, columns=['text'])\ndf_neg['label'] = 0\n\n# Load the positive tweets file and assign label as 1 for positive\npositive_tweets = twitter_samples.strings(\"positive_tweets.json\")\ndf_pos = pd.DataFrame(positive_tweets, columns=['text'])\ndf_pos['label'] = 1\n\ndf = pd.concat([df_pos, df_neg])  # Concatenate both the files\n# Shuffle the data to mix negative and positive tweets\ndf = df.sample(frac=1).reset_index(drop=True)\nprint(f'Shape of the whole data is: {df.shape[0]} rows and {df.shape[1]} columns')\n\nShape of the whole data is: 10000 rows and 2 columns\n# Look at the head of the dataframe\ndf.head()\n\n\n\n\n\n\n\n\ntext\nlabel\n\n\n\n\n0\nUsername Changed! :D\n1\n\n\n1\n@kimtaaeyeonss unnieeee!!!:)\n1\n\n\n2\n@amyewest Thanks! I hope you've got a good book to keep you company. :-)\n1\n\n\n3\n:) where are you situated? @Hijay09\n1\n\n\n4\n@egaroo You're welcome, I'm glad you liked it :)\n1"
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#lower-casing",
    "href": "posts/nlp/text-preprocessing.html#lower-casing",
    "title": "Text Preprocessing",
    "section": "Lower Casing",
    "text": "Lower Casing\nLowercasing is a common text preprocessing technique. It helps to transform all the text in same case.  Examples ‚ÄòThe‚Äô, ‚Äòthe‚Äô, ‚ÄòThE‚Äô -&gt; ‚Äòthe‚Äô\nThis is also useful to find all the duplicates since words in different cases are treated as separate words and becomes difficult for us to remove redundant words in all different case combination.\nThis may not be helpful when we do tasks like Part of Speech tagging (where proper casing gives some information about Nouns and so on) and Sentiment Analysis (where upper casing refers to anger and so on)\n\ndf.text = df.text.str.lower()\ndf.head(2)\n\n\n\n\n\n\n\n\ntext\nlabel\n\n\n\n\n0\nusername changed! :d\n1\n\n\n1\n@kimtaaeyeonss unnieeee!!!:)\n1"
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#remove-redundant-features",
    "href": "posts/nlp/text-preprocessing.html#remove-redundant-features",
    "title": "Text Preprocessing",
    "section": "Remove Redundant Features",
    "text": "Remove Redundant Features\n\n\n\n\n\n\nNote\n\n\n\nNote: How you define redundant features varies based on the problem statement.\n\n\n\nURL‚Äôs\nURL stands for Uniform Resource Locator. If present in a text, it represents the location of another website.\nIf we are performing any websites backlink analysis, in that case URL‚Äôs are useful to keep. Otherwise, they don‚Äôt provide any information. So we can remove them from our text.\n\ndf.text = df.text.str.replace(r'https?://\\S+|www\\.\\S+', '', regex=True)\ndf.head()\n\n\n\n\n\n\n\n\ntext\nlabel\n\n\n\n\n0\nusername changed! :d\n1\n\n\n1\n@kimtaaeyeonss unnieeee!!!:)\n1\n\n\n2\n@amyewest thanks! i hope you've got a good book to keep you company. :-)\n1\n\n\n3\n:) where are you situated? @hijay09\n1\n\n\n4\n@egaroo you're welcome, i'm glad you liked it :)\n1\n\n\n\n\n\n\n\n\n\nE-mail\nE-mail id‚Äôs are common in customer feedback data and they do not provide any useful information. So we remove them from the text.\nTwitter data that we are using does not contain any email id‚Äôs. Hence, please find the code snipper with an dummy example to remove e-mail id‚Äôs.\n\ntext = 'I have being trying to contact xyz via email to xyz@abc.co.in but there is no response.'\nre.sub(r'\\S+@\\S+', '', text)\n\n'I have being trying to contact xyz via email to  but there is no response.'\n\n\n\n\nDate\nDates can be represented in various formats and can be difficult at times to remove them. They are unlikely to contain any useful information for predicting the labels.\nBelow I have used dummy text to showcase the following task.\n\ntext = \"Today is 22/12/2020 and after two days on 24-12-2020 our vacation starts until 25th.09.2021\"\n\n# 1. Remove date formats like: dd/mm/yy(yy), dd-mm-yy(yy), dd(st|nd|rd).mm/yy(yy)\nre.sub(r'\\d{1,2}(st|nd|rd|th)?[-./]\\d{1,2}[-./]\\d{2,4}', '', text)\n\n'Today is  and after two days on  our vacation starts until '\n\n\n\ntext = \"Today is 11th of January, 2021 when I am writing this post. I hope to post this by February 15th or max to max by 20 may 21 or 20th-December-21\"\n\n# 2. Remove date formats like: 20 apr 21, April 15th, 11th of April, 2021\npattern = re.compile(\n    r'(\\d{1,2})?(st|nd|rd|th)?[-./,]?\\s?(of)?\\s?([J|j]an(uary)?|[F|f]eb(ruary)?|[Mm]ar(ch)?|[Aa]pr(il)?|[Mm]ay|[Jj]un(e)?|[Jj]ul(y)?|[Aa]ug(ust)?|[Ss]ep(tember)?|[Oo]ct(ober)?|[Nn]ov(ember)?|[Dd]ec(ember)?)\\s?(\\d{1,2})?(st|nd|rd|th)?\\s?[-./,]?\\s?(\\d{2,4})?')\npattern.sub(r'', text)\n\n'Today is  when I am writing this post. I hope to post this byor max to max by or '\n\n\nThere are various formats in which dates are represented and the above regex can be customized in many ways. Above, ‚Äúbyor‚Äù got combined cause we are trying multiple format in single regex pattern. You can customize the above expression accordingly to your need.\n\n\nHTML Tags\nIf we are extracting data from various websites, it is possible that the data also contains HTML tags. These tags does not provide any information and should be removed. These tags can be removed using regex or by using BeautifulSoup library.\n\n# Dummy text\ntext = \"\"\"\n&lt;title&gt;Below is a dummy html code.&lt;/title&gt;\n&lt;body&gt;\n    &lt;p&gt;All the html opening and closing brackets should be remove.&lt;/p&gt;\n    &lt;a href=\"https://www.abc.com\"&gt;Company Site&lt;/a&gt;\n&lt;/body&gt;\n\"\"\"\n\n\n# Using regex to remove html tags\npattern = re.compile('&lt;.*?&gt;')\npattern.sub('', text)\n\n'\\nBelow is a dummy html code.\\n\\n    All the html opening and closing brackets should be remove.\\n    Company Site\\n\\n'\n\n\n\n# Using Beautiful Soup\ndef remove_html(text):\n    clean_text = BeautifulSoup(text).get_text()\n    return clean_text\n\n\nremove_html(text)\n\n'\\nBelow is a dummy html code.\\n\\nAll the html opening and closing brackets should be remove.\\nCompany Site\\n\\n'\n\n\n\n\nEmojis\nAs more and more people have started using social media emoji‚Äôs play a very crucial role. Emoji‚Äôs are used to express emotions that are universally understood.\nIn some analysis such as sentiment analysis emoji‚Äôs can be useful. We can convert them to words or create some new features based on them. For some analysis we need to remove them. Find the below code snippet used to remove the emoji‚Äôs.\n\n\nCode\n# Reference: https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               u\"\\U0001f926-\\U0001f937\"\n                               u\"\\U00010000-\\U0010ffff\"\n                               u\"\\u2640-\\u2642\"\n                               u\"\\u2600-\\u2B55\"\n                               u\"\\u200d\"\n                               u\"\\u23cf\"\n                               u\"\\u23e9\"\n                               u\"\\u231a\"\n                               u\"\\ufe0f\"  # dingbats\n                               u\"\\u3030\"\n                               \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\n\ntext = \"game is on üî•üî•. HilariousüòÇ\"\nremove_emoji(text)\n\n'game is on . Hilarious'\n\n\n\n# Remove emoji's from text\ndf.text = df.text.apply(lambda x: remove_emoji(x))\n\n\n\nEmoticons\nEmoji‚Äôs and Emoticons are different. Yes!! Emoticons are used to express facial expressions using keyboard characters such as letters, numbers, and pucntuation marks. Where emjoi‚Äôs are small images.\nThanks to Neel Shah for curating a dictionary of emoticons and their description. We shall use this dictionary and remove the emoticons from our text.\n\n\nCode\nEMOTICONS = {\n    u\":‚Äë\\)\": \"Happy face or smiley\",\n    u\":\\)\": \"Happy face or smiley\",\n    u\":-\\]\": \"Happy face or smiley\",\n    u\":\\]\": \"Happy face or smiley\",\n    u\":-3\": \"Happy face smiley\",\n    u\":3\": \"Happy face smiley\",\n    u\":-&gt;\": \"Happy face smiley\",\n    u\":&gt;\": \"Happy face smiley\",\n    u\"8-\\)\": \"Happy face smiley\",\n    u\":o\\)\": \"Happy face smiley\",\n    u\":-\\}\": \"Happy face smiley\",\n    u\":\\}\": \"Happy face smiley\",\n    u\":-\\)\": \"Happy face smiley\",\n    u\":c\\)\": \"Happy face smiley\",\n    u\":\\^\\)\": \"Happy face smiley\",\n    u\"=\\]\": \"Happy face smiley\",\n    u\"=\\)\": \"Happy face smiley\",\n    u\":‚ÄëD\": \"Laughing, big grin or laugh with glasses\",\n    u\":D\": \"Laughing, big grin or laugh with glasses\",\n    u\"8‚ÄëD\": \"Laughing, big grin or laugh with glasses\",\n    u\"8D\": \"Laughing, big grin or laugh with glasses\",\n    u\"X‚ÄëD\": \"Laughing, big grin or laugh with glasses\",\n    u\"XD\": \"Laughing, big grin or laugh with glasses\",\n    u\"=D\": \"Laughing, big grin or laugh with glasses\",\n    u\"=3\": \"Laughing, big grin or laugh with glasses\",\n    u\"B\\^D\": \"Laughing, big grin or laugh with glasses\",\n    u\":-\\)\\)\": \"Very happy\",\n    u\":‚Äë\\(\": \"Frown, sad, andry or pouting\",\n    u\":-\\(\": \"Frown, sad, andry or pouting\",\n    u\":\\(\": \"Frown, sad, andry or pouting\",\n    u\":‚Äëc\": \"Frown, sad, andry or pouting\",\n    u\":c\": \"Frown, sad, andry or pouting\",\n    u\":‚Äë&lt;\": \"Frown, sad, andry or pouting\",\n    u\":&lt;\": \"Frown, sad, andry or pouting\",\n    u\":‚Äë\\[\": \"Frown, sad, andry or pouting\",\n    u\":\\[\": \"Frown, sad, andry or pouting\",\n    u\":-\\|\\|\": \"Frown, sad, andry or pouting\",\n    u\"&gt;:\\[\": \"Frown, sad, andry or pouting\",\n    u\":\\{\": \"Frown, sad, andry or pouting\",\n    u\":@\": \"Frown, sad, andry or pouting\",\n    u\"&gt;:\\(\": \"Frown, sad, andry or pouting\",\n    u\":'‚Äë\\(\": \"Crying\",\n    u\":'\\(\": \"Crying\",\n    u\":'‚Äë\\)\": \"Tears of happiness\",\n    u\":'\\)\": \"Tears of happiness\",\n    u\"D‚Äë':\": \"Horror\",\n    u\"D:&lt;\": \"Disgust\",\n    u\"D:\": \"Sadness\",\n    u\"D8\": \"Great dismay\",\n    u\"D;\": \"Great dismay\",\n    u\"D=\": \"Great dismay\",\n    u\"DX\": \"Great dismay\",\n    u\":‚ÄëO\": \"Surprise\",\n    u\":O\": \"Surprise\",\n    u\":‚Äëo\": \"Surprise\",\n    u\":o\": \"Surprise\",\n    u\":-0\": \"Shock\",\n    u\"8‚Äë0\": \"Yawn\",\n    u\"&gt;:O\": \"Yawn\",\n    u\":-\\*\": \"Kiss\",\n    u\":\\*\": \"Kiss\",\n    u\":X\": \"Kiss\",\n    u\";‚Äë\\)\": \"Wink or smirk\",\n    u\";\\)\": \"Wink or smirk\",\n    u\"\\*-\\)\": \"Wink or smirk\",\n    u\"\\*\\)\": \"Wink or smirk\",\n    u\";‚Äë\\]\": \"Wink or smirk\",\n    u\";\\]\": \"Wink or smirk\",\n    u\";\\^\\)\": \"Wink or smirk\",\n    u\":‚Äë,\": \"Wink or smirk\",\n    u\";D\": \"Wink or smirk\",\n    u\":‚ÄëP\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":P\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"X‚ÄëP\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"XP\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":‚Äë√û\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":√û\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":b\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"d:\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"=p\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"&gt;:P\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":‚Äë/\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":/\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":-[.]\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"&gt;:[(\\\\\\)]\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"&gt;:/\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":[(\\\\\\)]\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=/\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=[(\\\\\\)]\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":L\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=L\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":S\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":‚Äë\\|\": \"Straight face\",\n    u\":\\|\": \"Straight face\",\n    u\":$\": \"Embarrassed or blushing\",\n    u\":‚Äëx\": \"Sealed lips or wearing braces or tongue-tied\",\n    u\":x\": \"Sealed lips or wearing braces or tongue-tied\",\n    u\":‚Äë#\": \"Sealed lips or wearing braces or tongue-tied\",\n    u\":#\": \"Sealed lips or wearing braces or tongue-tied\",\n    u\":‚Äë&\": \"Sealed lips or wearing braces or tongue-tied\",\n    u\":&\": \"Sealed lips or wearing braces or tongue-tied\",\n    u\"O:‚Äë\\)\": \"Angel, saint or innocent\",\n    u\"O:\\)\": \"Angel, saint or innocent\",\n    u\"0:‚Äë3\": \"Angel, saint or innocent\",\n    u\"0:3\": \"Angel, saint or innocent\",\n    u\"0:‚Äë\\)\": \"Angel, saint or innocent\",\n    u\"0:\\)\": \"Angel, saint or innocent\",\n    u\":‚Äëb\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"0;\\^\\)\": \"Angel, saint or innocent\",\n    u\"&gt;:‚Äë\\)\": \"Evil or devilish\",\n    u\"&gt;:\\)\": \"Evil or devilish\",\n    u\"\\}:‚Äë\\)\": \"Evil or devilish\",\n    u\"\\}:\\)\": \"Evil or devilish\",\n    u\"3:‚Äë\\)\": \"Evil or devilish\",\n    u\"3:\\)\": \"Evil or devilish\",\n    u\"&gt;;\\)\": \"Evil or devilish\",\n    u\"\\|;‚Äë\\)\": \"Cool\",\n    u\"\\|‚ÄëO\": \"Bored\",\n    u\":‚ÄëJ\": \"Tongue-in-cheek\",\n    u\"#‚Äë\\)\": \"Party all night\",\n    u\"%‚Äë\\)\": \"Drunk or confused\",\n    u\"%\\)\": \"Drunk or confused\",\n    u\":-###..\": \"Being sick\",\n    u\":###..\": \"Being sick\",\n    u\"&lt;:‚Äë\\|\": \"Dump\",\n    u\"\\(&gt;_&lt;\\)\": \"Troubled\",\n    u\"\\(&gt;_&lt;\\)&gt;\": \"Troubled\",\n    u\"\\(';'\\)\": \"Baby\",\n    u\"\\(\\^\\^&gt;``\": \"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(\\^_\\^;\\)\": \"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-;\\)\": \"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(~_~;\\) \\(„Éª\\.„Éª;\\)\": \"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-\\)zzz\": \"Sleeping\",\n    u\"\\(\\^_-\\)\": \"Wink\",\n    u\"\\(\\(\\+_\\+\\)\\)\": \"Confused\",\n    u\"\\(\\+o\\+\\)\": \"Confused\",\n    u\"\\(o\\|o\\)\": \"Ultraman\",\n    u\"\\^_\\^\": \"Joyful\",\n    u\"\\(\\^_\\^\\)/\": \"Joyful\",\n    u\"\\(\\^O\\^\\)Ôºè\": \"Joyful\",\n    u\"\\(\\^o\\^\\)Ôºè\": \"Joyful\",\n    u\"\\(__\\)\": \"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"_\\(\\._\\.\\)_\": \"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"&lt;\\(_ _\\)&gt;\": \"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"&lt;m\\(__\\)m&gt;\": \"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(__\\)m\": \"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(_ _\\)m\": \"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"\\('_'\\)\": \"Sad or Crying\",\n    u\"\\(/_;\\)\": \"Sad or Crying\",\n    u\"\\(T_T\\) \\(;_;\\)\": \"Sad or Crying\",\n    u\"\\(;_;\": \"Sad of Crying\",\n    u\"\\(;_:\\)\": \"Sad or Crying\",\n    u\"\\(;O;\\)\": \"Sad or Crying\",\n    u\"\\(:_;\\)\": \"Sad or Crying\",\n    u\"\\(ToT\\)\": \"Sad or Crying\",\n    u\";_;\": \"Sad or Crying\",\n    u\";-;\": \"Sad or Crying\",\n    u\";n;\": \"Sad or Crying\",\n    u\";;\": \"Sad or Crying\",\n    u\"Q\\.Q\": \"Sad or Crying\",\n    u\"T\\.T\": \"Sad or Crying\",\n    u\"QQ\": \"Sad or Crying\",\n    u\"Q_Q\": \"Sad or Crying\",\n    u\"\\(-\\.-\\)\": \"Shame\",\n    u\"\\(-_-\\)\": \"Shame\",\n    u\"\\(‰∏Ä‰∏Ä\\)\": \"Shame\",\n    u\"\\(Ôºõ‰∏Ä_‰∏Ä\\)\": \"Shame\",\n    u\"\\(=_=\\)\": \"Tired\",\n    u\"\\(=\\^\\¬∑\\^=\\)\": \"cat\",\n    u\"\\(=\\^\\¬∑\\¬∑\\^=\\)\": \"cat\",\n    u\"=_\\^= \": \"cat\",\n    u\"\\(\\.\\.\\)\": \"Looking down\",\n    u\"\\(\\._\\.\\)\": \"Looking down\",\n    u\"\\^m\\^\": \"Giggling with hand covering mouth\",\n    u\"\\(\\„Éª\\„Éª?\": \"Confusion\",\n    u\"\\(?_?\\)\": \"Confusion\",\n    u\"&gt;\\^_\\^&lt;\": \"Normal Laugh\",\n    u\"&lt;\\^!\\^&gt;\": \"Normal Laugh\",\n    u\"\\^/\\^\": \"Normal Laugh\",\n    u\"\\Ôºà\\*\\^_\\^\\*Ôºâ\": \"Normal Laugh\",\n    u\"\\(\\^&lt;\\^\\) \\(\\^\\.\\^\\)\": \"Normal Laugh\",\n    u\"\\(^\\^\\)\": \"Normal Laugh\",\n    u\"\\(\\^\\.\\^\\)\": \"Normal Laugh\",\n    u\"\\(\\^_\\^\\.\\)\": \"Normal Laugh\",\n    u\"\\(\\^_\\^\\)\": \"Normal Laugh\",\n    u\"\\(\\^\\^\\)\": \"Normal Laugh\",\n    u\"\\(\\^J\\^\\)\": \"Normal Laugh\",\n    u\"\\(\\*\\^\\.\\^\\*\\)\": \"Normal Laugh\",\n    u\"\\(\\^‚Äî\\^\\Ôºâ\": \"Normal Laugh\",\n    u\"\\(#\\^\\.\\^#\\)\": \"Normal Laugh\",\n    u\"\\Ôºà\\^‚Äî\\^\\Ôºâ\": \"Waving\",\n    u\"\\(;_;\\)/~~~\": \"Waving\",\n    u\"\\(\\^\\.\\^\\)/~~~\": \"Waving\",\n    u\"\\(-_-\\)/~~~ \\($\\¬∑\\¬∑\\)/~~~\": \"Waving\",\n    u\"\\(T_T\\)/~~~\": \"Waving\",\n    u\"\\(ToT\\)/~~~\": \"Waving\",\n    u\"\\(\\*\\^0\\^\\*\\)\": \"Excited\",\n    u\"\\(\\*_\\*\\)\": \"Amazed\",\n    u\"\\(\\*_\\*;\": \"Amazed\",\n    u\"\\(\\+_\\+\\) \\(@_@\\)\": \"Amazed\",\n    u\"\\(\\*\\^\\^\\)v\": \"Laughing,Cheerful\",\n    u\"\\(\\^_\\^\\)v\": \"Laughing,Cheerful\",\n    u\"\\(\\(d[-_-]b\\)\\)\": \"Headphones,Listening to music\",\n    u'\\(-\"-\\)': \"Worried\",\n    u\"\\(„Éº„Éº;\\)\": \"Worried\",\n    u\"\\(\\^0_0\\^\\)\": \"Eyeglasses\",\n    u\"\\(\\ÔºæÔΩñ\\Ôºæ\\)\": \"Happy\",\n    u\"\\(\\ÔºæÔΩï\\Ôºæ\\)\": \"Happy\",\n    u\"\\(\\^\\)o\\(\\^\\)\": \"Happy\",\n    u\"\\(\\^O\\^\\)\": \"Happy\",\n    u\"\\(\\^o\\^\\)\": \"Happy\",\n    u\"\\)\\^o\\^\\(\": \"Happy\",\n    u\":O o_O\": \"Surprised\",\n    u\"o_0\": \"Surprised\",\n    u\"o\\.O\": \"Surpised\",\n    u\"\\(o\\.o\\)\": \"Surprised\",\n    u\"oO\": \"Surprised\",\n    u\"\\(\\*Ôø£mÔø£\\)\": \"Dissatisfied\",\n    u\"\\(‚ÄòA`\\)\": \"Snubbed or Deflated\"\n}\n\n\n\ndef remove_emoticons(text):\n    emoticons_pattern = re.compile(\n        u'(' + u'|'.join(emo for emo in EMOTICONS) + u')')\n    return emoticons_pattern.sub(r'', text)\n\n\nremove_emoticons(\"Hello :-&gt;\")\n\n'Hello '\n\n\n\n# Remove emoticons from text\ndf.text = df.text.apply(lambda x: remove_emoticons(x))\n\n\n\nHashtags and Mentions\nWe are habituated to use hashtags and mentions in our tweet either to indicate the context or bring attention to an individual. Hashtags can be used to extract features, to see what‚Äôs trending and in various other applications.\nSince, we don‚Äôt require them we‚Äôll remove them.\n\ndef remove_tags_mentions(text):\n    pattern = re.compile(r'(@\\S+|#\\S+)')\n    return pattern.sub('', text)\n\n\ntext = \"live @flippinginja on #younow - jonah and jareddddd\"\nremove_tags_mentions(text)\n\n'live  on  - jonah and jareddddd'\n\n\n\n# Remove hashtags and mentions\ndf.text = df.text.apply(lambda x: remove_tags_mentions(x))\n\n\n\nPunctuations\nPunctuations are character other than alphaters and digits. These include [!‚Äú#$%&'()*+,-./:;&lt;=&gt;?@\\^_`{|}~]\nIt is better remove or convert emoticons before removing the punctuations, since if we do the other we around we might loose the emoticons from the text. Another example, if the text contains $10.50 then we‚Äôll remove the .(dot) and the value will loose it‚Äôs meaning.\n\nPUNCTUATIONS = string.punctuation\n\n\ndef remove_punctuation(text):\n    return text.translate(str.maketrans('', '', PUNCTUATIONS))\n\n\ndf.text = df[\"text\"].apply(lambda text: remove_punctuation(text))\n\n\n\n\n\n\n\n\ntext\nlabel\n\n\n\n\n0\nusername changed d\n1\n\n\n1\nunnieeee\n1\n\n\n2\nthanks i hope youve got a good book to keep you company\n1\n\n\n3\nwhere are you situated\n1\n\n\n4\nyoure welcome im glad you liked it\n1\n\n\n\n\n\n\n\n\n\nStopwords\nStopwords are commonly occuring words in any language. Such as, in english these words are ‚Äòthe‚Äô, ‚Äòa‚Äô, ‚Äòan‚Äô, & many more. They are in most cases not useful and should be removed.\nThere are certain tasks in which these words are useful such as Part-of-Speech(POS) tagging, language translation. Stopwords are compiled for many languages, for english language we can use the list from the nltk package.\n\nSTOPWORDS = set(stopwords.words('english'))\n\n\ndef remove_stopwords(text):\n    return ' '.join([word for word in text.split() if word not in STOPWORDS])\n\n\n# Remove stopwords\ndf.text = df.text.apply(lambda text: remove_stopwords(text))\ndf.head()\n\n\n\n\n\n\n\n\ntext\nlabel\n\n\n\n\n0\nusername changed\n1\n\n\n1\nunnieeee\n1\n\n\n2\nthanks hope youve got good book keep company\n1\n\n\n3\nsituated\n1\n\n\n4\nyoure welcome im glad liked\n1\n\n\n\n\n\n\n\n\n\nNumbers\nWe may remove numbers if they are not useful in our analysis. But analysis in the financial domain, numbers are very useful.\n\ndf.text = df.text.str.replace(r'\\d+', '', regex=True)\n\n\n\nExtra whitespaces\nAfter usually after preprocessing the text there might be extra whitespaces that might be created after transforming, removing various characters. Also, there is a need to remove all the new line, tab characters as well from our text.\n\ndef remove_whitespaces(text):\n    return \" \".join(text.split())\n\n\ntext = \"  Whitespaces in the beginning are removed  \\t as well \\n  as in between  the text   \"\n\nclean_text = \" \".join(text.split())\nclean_text\n\n'Whitespaces in the beginning are removed as well as in between the text'\n\n\n\ndf.text = df.text.apply(lambda x: remove_whitespaces(x))\n\n\n\nFrequent words\nPreviously we have removed stopwords which are common in any language. If we are working in any domain, we can also remove the common words used in that domain which don‚Äôt provide us with much information.\n\ndef freq_words(text):\n    tokens = word_tokenize(text)\n    FrequentWords = []\n\n    for word in tokens:\n        counter[word] += 1\n\n    for (word, word_count) in counter.most_common(10):\n        FrequentWords.append(word)\n    return FrequentWords\n\n\ndef remove_fw(text, FrequentWords):\n    tokens = word_tokenize(text)\n    without_fw = []\n    for word in tokens:\n        if word not in FrequentWords:\n            without_fw.append(word)\n\n    without_fw = ' '.join(without_fw)\n    return without_fw\n\n\ncounter = Counter()\n\n\ntext = \"\"\"\nNatural Language Processing is the technology used to aid computers to understand the human‚Äôs natural language. It‚Äôs not an easy task teaching machines to understand how we communicate. Leand Romaf, an experienced software engineer who is passionate at teaching people how artificial intelligence systems work, says that ‚Äúin recent years, there have been significant breakthroughs in empowering computers to understand language just as we do.‚Äù This article will give a simple introduction to Natural Language Processing and how it can be achieved. Natural Language Processing, usually shortened as NLP, is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable. Most NLP techniques rely on machine learning to derive meaning from human languages.\n\"\"\"\n\n\nFrequentWords = freq_words(text)\nprint(FrequentWords)\n\n[',', 'to', '.', 'is', 'the', 'understand', 'Natural', 'Language', 'Processing', 'computers']\n\n\n\nfw_result = remove_fw(text, FrequentWords)\nfw_result\n\n'technology used aid human ‚Äô s natural language It ‚Äô s not an easy task teaching machines how we communicate Leand Romaf an experienced software engineer who passionate at teaching people how artificial intelligence systems work says that ‚Äú in recent years there have been significant breakthroughs in empowering language just as we do. ‚Äù This article will give a simple introduction and how it can be achieved usually shortened as NLP a branch of artificial intelligence that deals with interaction between and humans using natural language The ultimate objective of NLP read decipher and make sense of human languages in a manner that valuable Most NLP techniques rely on machine learning derive meaning from human languages'\n\n\n\n\nRare words\nRare words are similar to frequent words. We can remove them because they are so less that they cannot add any value to the purpose.\n\ndef rare_words(text):\n    # tokenization\n    tokens = word_tokenize(text)\n    for word in tokens:\n        counter[word] = +1\n\n    RareWords = []\n    number_rare_words = 10\n    # take top 10 frequent words\n    frequentWords = counter.most_common()\n    for (word, word_count) in frequentWords[:-number_rare_words:-1]:\n        RareWords.append(word)\n\n    return RareWords\n\n\ndef remove_rw(text, RareWords):\n    tokens = word_tokenize(text)\n    without_rw = []\n    for word in tokens:\n        if word not in RareWords:\n            without_rw.append(word)\n\n    without_rw = ' '.join(without_fw)\n    return without_rw\n\n\ncounter = Counter()\n\n\ntext = \"\"\"\nNatural Language Processing is the technology used to aid computers to understand the human‚Äôs natural language. It‚Äôs not an easy task teaching machines to understand how we communicate. Leand Romaf, an experienced software engineer who is passionate at teaching people how artificial intelligence systems work, says that ‚Äúin recent years, there have been significant breakthroughs in empowering computers to understand language just as we do.‚Äù This article will give a simple introduction to Natural Language Processing and how it can be achieved. Natural Language Processing, usually shortened as NLP, is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable. Most NLP techniques rely on machine learning to derive meaning from human languages.\n\"\"\"\n\n\nRareWords = rare_words(text)\nRareWords\n\n['from',\n 'meaning',\n 'derive',\n 'learning',\n 'machine',\n 'on',\n 'rely',\n 'techniques',\n 'Most']\n\n\n\nrw_result = remove_fw(text, RareWords)\nrw_result\n\n'Natural Language Processing is the technology used to aid computers to understand the human ‚Äô s natural language . It ‚Äô s not an easy task teaching machines to understand how we communicate . Leand Romaf , an experienced software engineer who is passionate at teaching people how artificial intelligence systems work , says that ‚Äú in recent years , there have been significant breakthroughs in empowering computers to understand language just as we do. ‚Äù This article will give a simple introduction to Natural Language Processing and how it can be achieved . Natural Language Processing , usually shortened as NLP , is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language . The ultimate objective of NLP is to read , decipher , understand , and make sense of the human languages in a manner that is valuable . NLP to human languages .'"
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#conversion-of-emoji-to-words",
    "href": "posts/nlp/text-preprocessing.html#conversion-of-emoji-to-words",
    "title": "Text Preprocessing",
    "section": "Conversion of Emoji to Words",
    "text": "Conversion of Emoji to Words\nTo remove or not is done based on the purpose of the application. Example if we are building a sentiment analysis system emoji‚Äôs can be useful.\n‚ÄúThe movie was üî•‚Äù or ‚ÄúThe movie was üí©‚Äù\nIf we remove the emoji‚Äôs the meaning of the sentence changes completely. In these cases we can convert emoji‚Äôs to words.\ndemoji requires an initial data download from the Unicode Consortium‚Äôs emoji code repository.\nOn first use of the package, call download_codes(). This will store the Unicode hex-notated symbols at ~/.demoji/codes.json for future use.\nRead more about demoji on pypi.org\n\ndemoji.download_codes()\n\nDownloading emoji data ...\n... OK (Got response in 1.35 seconds)\nWriting emoji data to C:\\Users\\sagar\\.demoji\\codes.json ...\n... OK\n\n\n\ndef emoji_to_words(text):\n    return demoji.replace_with_desc(text, sep=\"__\")\n\n\ntext = \"game is on üî• üö£üèº\"\nemoji_to_words(text)\n\n'game is on __fire__ __person rowing boat: medium-light skin tone__'"
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#conversion-of-emoticons-to-words",
    "href": "posts/nlp/text-preprocessing.html#conversion-of-emoticons-to-words",
    "title": "Text Preprocessing",
    "section": "Conversion of Emoticons to Words",
    "text": "Conversion of Emoticons to Words\nAs we did for emoji‚Äôs, we convert emoticons to words for the same purpose.\n\ndef emoticons_to_words(text):\n    for emot in EMOTICONS:\n        text = re.sub(\n            u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\", \"\").replace(\":\", \"\").split()), text)\n    return text\n\n\ntext = \"Hey there!! :-)\"\nemoticons_to_words(text)\n\n'Hey there!! Happy_face_smiley'"
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#converting-numbers-to-words",
    "href": "posts/nlp/text-preprocessing.html#converting-numbers-to-words",
    "title": "Text Preprocessing",
    "section": "Converting Numbers to Words",
    "text": "Converting Numbers to Words\nIf our analysis require us to use information based on the numbers in the text, we can convert them to words.\nRead more about num2words on github\n\ndef nums_to_words(text):\n    new_text = []\n    for word in text.split():\n        if word.isdigit():\n            new_text.append(num2words(word))\n        else:\n            new_text.append(word)\n    return \" \".join(new_text)\n\n\ntext = \"I ran this track 30 times\"\nnums_to_words(text)\n\n'I ran this track thirty times'"
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#chat-words-conversion",
    "href": "posts/nlp/text-preprocessing.html#chat-words-conversion",
    "title": "Text Preprocessing",
    "section": "Chat words Conversion",
    "text": "Chat words Conversion\nThe more we use social media, we have become lazy to type the whole phrase or word. Due to which slang words came into existance such as ‚Äúomg‚Äù which represents ‚ÄúOh my god‚Äù. Such slang words don‚Äôt provide much information and if we need to use them we have to convert them.\nThank you: GitHub repo for the list of slang words\n\n\nCode\nchat_words = \"\"\"\nAFAIK=As Far As I Know\nAFK=Away From Keyboard\nASAP=As Soon As Possible\nATK=At The Keyboard\nATM=At The Moment\nA3=Anytime, Anywhere, Anyplace\nBAK=Back At Keyboard\nBBL=Be Back Later\nBBS=Be Back Soon\nBFN=Bye For Now\nB4N=Bye For Now\nBRB=Be Right Back\nBRT=Be Right There\nBTW=By The Way\nB4=Before\nB4N=Bye For Now\nCU=See You\nCUL8R=See You Later\nCYA=See You\nFAQ=Frequently Asked Questions\nFC=Fingers Crossed\nFWIW=For What It's Worth\nFYI=For Your Information\nGAL=Get A Life\nGG=Good Game\nGN=Good Night\nGMTA=Great Minds Think Alike\nGR8=Great!\nG9=Genius\nIC=I See\nICQ=I Seek you (also a chat program)\nILU=ILU: I Love You\nIMHO=In My Honest/Humble Opinion\nIMO=In My Opinion\nIOW=In Other Words\nIRL=In Real Life\nKISS=Keep It Simple, Stupid\nLDR=Long Distance Relationship\nLMAO=Laugh My A.. Off\nLOL=Laughing Out Loud\nLTNS=Long Time No See\nL8R=Later\nMTE=My Thoughts Exactly\nM8=Mate\nNRN=No Reply Necessary\nOIC=Oh I See\nPITA=Pain In The A..\nPRT=Party\nPRW=Parents Are Watching\nQPSA?=Que Pasa?\nROFL=Rolling On The Floor Laughing\nROFLOL=Rolling On The Floor Laughing Out Loud\nROTFLMAO=Rolling On The Floor Laughing My A.. Off\nSK8=Skate\nSTATS=Your sex and age\nASL=Age, Sex, Location\nTHX=Thank You\nTTFN=Ta-Ta For Now!\nTTYL=Talk To You Later\nU=You\nU2=You Too\nU4E=Yours For Ever\nWB=Welcome Back\nWTF=What The F...\nWTG=Way To Go!\nWUF=Where Are You From?\nW8=Wait...\n7K=Sick:-D Laugher\nOMG=Oh my god\"\"\"\n\n\n\nchat_words_dict = dict()\nchat_words_set = set()\n\n\ndef cw_conversion(text):\n    new_text = []\n    for word in text.split():\n        if word.upper() in chat_words_set:\n            new_text.append(chat_words_dict[word.upper()])\n        else:\n            new_text.append(word)\n    return \" \".join(new_text)\n\n\nfor line in chat_words.split('\\n'):\n    if line != '':\n        cw, cw_expanded = line.split('=')[0], line.split('=')[1]\n\n        chat_words_set.add(cw)\n        chat_words_dict[cw] = cw_expanded\n\n\ntext = \"omg that's awesome.\"\ncw_conversion(text)\n\n\"Oh my god that's awesome.\""
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#expanding-contractions",
    "href": "posts/nlp/text-preprocessing.html#expanding-contractions",
    "title": "Text Preprocessing",
    "section": "Expanding Contractions",
    "text": "Expanding Contractions\nContractions are words or combinations of words created by dropping a few letters and replacing those letters by an apostrophe.\nExample:\n\ndon‚Äôt: do not\nwe‚Äôll: we will\n\nOur nlp model don‚Äôt understand these contractions i.e.¬†they don‚Äôt understand that ‚Äúdon‚Äôt‚Äù and ‚Äúdo not‚Äù are the same thing. If our problem statement requires them then we can expand them or else leave it as it is.\n\ndef expand_contractions(text):\n    expanded_text = []\n    for line in text:\n        expanded_text.append(contractions.fix(line))\n    return expanded_text\n\n\ntext = [\"I'll be there within 15 minutes.\",\n        \"It's awesome to meet your new friends.\"]\nexpand_contractions(text)\n\n['I will be there within 15 minutes.',\n 'it is awesome to meet your new friends.']"
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#stemming",
    "href": "posts/nlp/text-preprocessing.html#stemming",
    "title": "Text Preprocessing",
    "section": "Stemming",
    "text": "Stemming\nIn stemming we reduce the word to it‚Äôs base or root form by removing the suffix characters from the word. It is one of the technique to normalize text.\nStemming for root word ‚Äúlike‚Äù include:\n\n‚Äúlikes‚Äù\n‚Äúliked‚Äù\n‚Äúlikely‚Äù\n‚Äúliking‚Äù\n\nStemmed word doesn‚Äôt always match the words in our dictionary such as:\n\nconsole -&gt; consol\ncompany -&gt; compani\nwelcome -&gt; welcom\n\nDue to which stemming is not performed in all nlp tasks.\nThere are various algorithms used for stemming but the most widely used is PorterStemmer. In this post we have used the PorterStemmer as well.\n\nstemmer = PorterStemmer()\n\n\ndef stem_words(text):\n    return ' '.join([stemmer.stem(word) for word in text.split()])\n\n\ndf['text_stemmed'] = df.text.apply(lambda text: stem_words(text))\ndf[['text', 'text_stemmed']].head()\n\n\n\n\n\n\n\n\ntext\ntext_stemmed\n\n\n\n\n0\nusername changed\nusernam chang\n\n\n1\nunnieeee\nunnieee\n\n\n2\nthanks hope youve got good book keep company\nthank hope youv got good book keep compani\n\n\n3\nsituated\nsituat\n\n\n4\nyoure welcome im glad liked\nyour welcom im glad like\n\n\n\n\n\n\n\nPorterStemmer can be used only for english. If we are working with other than english then we can use SnowballStemmer.\n\nSnowballStemmer.languages\n\n('arabic',\n 'danish',\n 'dutch',\n 'english',\n 'finnish',\n 'french',\n 'german',\n 'hungarian',\n 'italian',\n 'norwegian',\n 'porter',\n 'portuguese',\n 'romanian',\n 'russian',\n 'spanish',\n 'swedish')"
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#lemmatization",
    "href": "posts/nlp/text-preprocessing.html#lemmatization",
    "title": "Text Preprocessing",
    "section": "Lemmatization",
    "text": "Lemmatization\nLemmatization tried to perform the similar task as that of stemming i.e.¬†trying to reduce the inflection words to it‚Äôs base form. But lemmatization does it by using a different approach.\nLemmatizations takes into consideration of the morphological analysis of the word. It tries to reduce to words to it‚Äôs dictionary form which is known as lemma.\n\nlemmatizer = WordNetLemmatizer()\n\n\ndef text_lemmatize(text):\n    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n\n\ndf['text_lemmatized'] = df.text.apply(lambda text: text_lemmatize(text))\ndf[['text', 'text_stemmed', 'text_lemmatized']].head()\n\n\n\n\n\n\n\n\ntext\ntext_stemmed\ntext_lemmatized\n\n\n\n\n0\nusername changed\nusernam chang\nusername changed\n\n\n1\nunnieeee\nunnieee\nunnieeee\n\n\n2\nthanks hope youve got good book keep company\nthank hope youv got good book keep compani\nthanks hope youve got good book keep company\n\n\n3\nsituated\nsituat\nsituated\n\n\n4\nyoure welcome im glad liked\nyour welcom im glad like\nyoure welcome im glad liked\n\n\n\n\n\n\n\nDifference between Stemming and Lemmatization:\n\n\n\n\n\n\n\nStemming\nLemmatization\n\n\n\n\nFast compared to lemmatization\nSlow compared to stemming\n\n\nReduces the word to it‚Äôs base form by removing the suffix\nUses lexical knowledge to get the base form of the word\n\n\nDoes not always provide meaning or dictionary form of the original word\nResulting words are always meaningful and dictionary words"
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#spelling-correction",
    "href": "posts/nlp/text-preprocessing.html#spelling-correction",
    "title": "Text Preprocessing",
    "section": "Spelling Correction",
    "text": "Spelling Correction\nWe as human always make mistake. Normally incorrect spelling in text are know as typos.\nSince the NLP model doesn‚Äôt know the difference between a correct and an incorrect word. For the model ‚Äúthanks‚Äù and ‚Äúthnks‚Äù are two different words. Therefore, spelling correction is an important step to bring the incorrect words in the correct format.\n\nspell = SpellChecker()\n\n\ndef correct_spelling(text):\n    correct_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            correct_text.append(spell.correction(word))\n        else:\n            correct_text.append(word)\n    return \" \".join(correct_text)\n\n\ntext = \"Hi, hwo are you doin? I'm good thnks for asking\"\ncorrect_spelling(text)\n\n\"Hi, how are you doing I'm good thanks for asking\"\n\n\n\ntext = \"hw are you doin? I'm god thnks\"\ncorrect_spelling(text)\n\n\"he are you doing I'm god thanks\""
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#convert-accented-characters-to-ascii-characters",
    "href": "posts/nlp/text-preprocessing.html#convert-accented-characters-to-ascii-characters",
    "title": "Text Preprocessing",
    "section": "Convert accented characters to ASCII characters",
    "text": "Convert accented characters to ASCII characters\nAccent marks (also referred to as diacritics or diacriticals) usually appear above a character when we press the character for a long time. These need to be remove cause the model cannot distinguish between ‚Äúd√®√®p‚Äù and ‚Äúdeep‚Äù. It will consider them as two different words.\n\ndef accented_to_ascii(text):\n    return unidecode.unidecode(text)\n\n\ntext = \"This is an example text with accented characters like d√®√®p l√®arning √°nd c√∂mputer v√≠s√≠√∂n etc.\"\naccented_to_ascii(text)\n\n'This is an example text with accented characters like deep learning and computer vision etc.'"
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#ekphrasis",
    "href": "posts/nlp/text-preprocessing.html#ekphrasis",
    "title": "Text Preprocessing",
    "section": "Ekphrasis",
    "text": "Ekphrasis\nCollection of lightweight text tools, geared towards text from social networks, such as Twitter or Facebook, for tokenization, word normalization, word segmentation (for splitting hashtags) and spell correction, using word statistics from 2 big corpora (english Wikipedia, twitter - 330mil english tweets).\nekphrasis was developed as part of the text processing pipeline for DataStories team‚Äôs submission for SemEval-2017 Task 4 (English), Sentiment Analysis in Twitter (source).\n\n# Referred from: https://github.com/cbaziotis/ekphrasis\n\ntext_processor = TextPreProcessor(\n    # terms that will be normalized\n    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n               'time', 'url', 'date', 'number'],\n\n    # terms that will be annotated\n    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n              'emphasis', 'censored'},\n    fix_html=True,  # fix HTML tokens\n\n    # corpus from which the word statistics are going to be used\n    # for word segmentation\n    segmenter=\"twitter\",\n\n    # corpus from which the word statistics are going to be used\n    # for spell correction\n    corrector=\"twitter\",\n\n    unpack_hashtags=True,  # perform word segmentation on hashtags\n    unpack_contractions=True,  # Unpack contractions (can't -&gt; can not)\n    spell_correct_elong=False,  # spell correction for elongated words\n\n    # select a tokenizer. You can use SocialTokenizer, or pass your own\n    # the tokenizer, should take as input a string and return a list of tokens\n    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n\n    # list of dictionaries, for replacing tokens extracted from the text,\n    # with other expressions. You can pass more than one dictionaries.\n    dicts=[emoticons]\n)\n\nReading twitter - 1grams ...\nReading twitter - 2grams ...\nReading twitter - 1grams ...\n\n\n\n# Clean text based on function defined above\ndf['clean_tweets'] = [\n    \" \".join(text_processor.pre_process_doc(tweet)) for tweet in df.text]\n\n\n# Display the clean text\ndf.head()\n\n\n\n\n\n\n\n\ntext\nlabel\nclean_tweets\n\n\n\n\n0\n@ManiMint_ you're welcome :)\n1\n&lt;user&gt; you are welcome &lt;happy&gt;\n\n\n1\nExpired and I used BIS money now I'm broke ):(...\n0\nexpired and i used &lt;allcaps&gt; bis &lt;/allcaps&gt; mo...\n\n\n2\nThank you :) https://t.co/DuVcLseonQ\n1\nthank you &lt;happy&gt; &lt;url&gt;\n\n\n3\n@DeMoorSophie Hii, can you follow me, please? ...\n1\n&lt;user&gt; hii , can you follow me , please ? i ' ...\n\n\n4\n@hellasugg @MyNamesChai and sacconejoly's (@Jo...\n0\n&lt;user&gt; &lt;user&gt; and sacconejoly ' s ( &lt;user&gt; and..."
  },
  {
    "objectID": "posts/nlp/text-preprocessing.html#conclusion",
    "href": "posts/nlp/text-preprocessing.html#conclusion",
    "title": "Text Preprocessing",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, most of the text pre-processing techniques are explanied. I‚Äôll update this post as I learn more techniques to pre-process text.\nShare if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     Github\n  \n  \n    \n     Email\n  \n  \n    \n     Resume\n  \n\n      \n\n    \n    \n  \n\n\nI‚Äôm a research scientist at University at Buffalo where I focus on identifying racist language in online hate speech. I am fortunate to be advised by Professor Kenneth Joseph.\nMy expertise lies in conducting research and development and creating software solutions using Natural Language Processing (NLP), Applied Machine Learning (ML), and Information Access. I have a strong interest in understanding entire ML lifecycle end-to-end to evaluate the business requirements; run experiments; build data and machine learning (ML) models; evaluate performance and business impact; deploy the models; setup long-term monitoring.\nI recently completed my Master‚Äôs degree in Computer Science from the University at Buffalo, The State University of New York in December 2022. During that period, one of my most exciting and rewarding project I worked upon involved various shared tasks from Social Media Mining for Health Applications (SMM4H) 2020 & 2021 to tackle problems in the health care domain. You can read more about this project here, along with my report.\nBefore moving to the USA, I worked remotely as a Data Scientist at FedEx Express. My responsibility included End-to-End Research, Design, Developement, Deployment of NLP, and classical ML classification tasks . I worked on projects such as identifying customer sentiments towards our services, classifying customer tickets to relevant departments for efficient resolution, building Extract-Transform-Load (ETL) pipelines, and developing dashboards to improve communication, productivity, and save time and money.\nIn 2018, I got the opportunity to work as a Assistant System Engineer at Tata Consultancy Services. I worked on developing automated test scripts to simulate testing process in web apps, model customer/software system interaction in ARD for automatic tests scripts generation.\nDuring my undergraduate studies at Ramrao Adik Institute of Technology (RAIT), I majored in Computer Science and actively participated in cultural events and competitions. I also served as the Administrator of Social Wing RAIT, a non-profit organization dedicated to creating a better society. In this role, I managed various events focused on making a positive impact, such as tree planting, teaching at orphanages, donation drives, and clean-up efforts.\nOne of our most successful events was a marathon fundraiser, with all proceeds going towards providing education for underprivileged children in our community for an entire academic year. Thanks to the overwhelming support of our community, we were able to provide education for more than 20 children, making a tangible and meaningful difference in their lives.\nOutside of work, I have a keen interest in learning about our galaxy, black holes, and physics. I also enjoy exploring different cuisines, watching football, and playing Dota 2. Connecting with others is something I truly enjoy, so if you share similar interests or would like to connect, please feel free to reach out. I look forward to hearing from you! üôå\n\nMy Journey\n\n\n‚ÄúStay hungry. Stay foolish.‚Äù - Steve Job\n\n\n\n\n\n\n\n2021 - 2023\n\n\n\n\n\nUniversity at Buffalo, The State University of New York\n\n\nMaster of Science, Computer Science\n\n\n\n\n\n\n2014 - 2018\n\n\n\n\n\nUniversity of Mumbai\n\n\nBachelors of Engineering, Computer Engineering\n\n\n\n\n\n\n\n\n\n\n\n2020 - 2021\n\n\n\n\n\nData Scientist\n\n\nFedEx Express\n\n\n\n\n\n\n2018 - 2019\n\n\n\n\n\nAssistant System Engineer\n\n\nTata Consultancy Services\n\n\n\n\n\n\n\n\n\nAchievements\n\n\n\nSecured second place in the H2O World India Hackathon held in April 2023."
  },
  {
    "objectID": "pages/nlp.html",
    "href": "pages/nlp.html",
    "title": "Natural Language Processing",
    "section": "",
    "text": "This page contains the list of posts related to natural language processing. It makes it easier to follow and read topics in a structured format:\n\nWhat is Natural Language Processing\nText Preprocessing"
  }
]